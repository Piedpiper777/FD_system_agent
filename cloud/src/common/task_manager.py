"""
äº‘ç«¯è®­ç»ƒä»»åŠ¡ç®¡ç†ç³»ç»Ÿ
æ”¯æŒå¼‚æ­¥è®­ç»ƒä»»åŠ¡çš„æäº¤ã€æŸ¥è¯¢å’Œç®¡ç†
"""

import uuid
import threading
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class TrainingStatus(Enum):
    """è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
    QUEUED = "queued"  # å·²æäº¤
    RUNNING = "running"  # è¿è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # å¤±è´¥
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ


@dataclass
class TrainingTask:
    """è®­ç»ƒä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    module: str
    model_type: str
    output_path: str
    input_dim: int
    data_path: str = None  # å¯é€‰ï¼ŒæŸäº›æ¨¡å¼å¯èƒ½ä¸éœ€è¦
    dataset_mode: str = 'one'  # æ–°å¢ï¼šæ•°æ®é›†æ¨¡å¼
    _raw_config: dict = None  # ä¿å­˜å®Œæ•´çš„åŸå§‹é…ç½®ï¼ˆåŒ…æ‹¬train_files, test_filesç­‰ï¼‰
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 0.001
    validation_split: float = 0.2
    train_ratio: float = None
    val_ratio: float = None
    test_ratio: float = None
    val_ratio_from_train: float = None
    sequence_length: int = 50
    prediction_horizon: int = 1
    hidden_units: int = 128
    num_layers: int = 2
    dropout: float = 0.1
    activation: str = 'tanh'
    bidirectional: bool = False
    preprocess_method: str = None
    status: str = 'queued'
    created_at: str = None
    updated_at: str = None
    progress: int = 0
    message: str = ""
    logs: str = ""
    error: str = ""
    model_save_path: str = ""  # æ–°å¢ï¼šæ¨¡å‹ä¿å­˜è·¯å¾„
    edge_host: str = None  # æ–°å¢ï¼šè¾¹ç¼˜ç«¯ä¸»æœº
    edge_port: int = None  # æ–°å¢ï¼šè¾¹ç¼˜ç«¯ç«¯å£
    dataset_file: str = None  # æ–°å¢ï¼šæ•°æ®é›†æ–‡ä»¶
    train_file: str = None  # æ–°å¢ï¼šè®­ç»ƒæ–‡ä»¶
    val_file: str = None  # æ–°å¢ï¼šéªŒè¯æ–‡ä»¶
    test_file: str = None  # æ–°å¢ï¼šæµ‹è¯•æ–‡ä»¶
    scaler_path: str = ""
    threshold_path: str = ""
    threshold_value: float = None
    threshold_metadata: dict = None
    # è®­ç»ƒè¿›åº¦è·Ÿè¸ª
    current_epoch: int = 0
    completed_epochs: int = 0
    # å½“å‰è®­ç»ƒè½®æ¬¡çš„æŸå¤±å€¼ï¼ˆç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰
    current_train_loss: float = None
    current_val_loss: float = None
    # æœ€ç»ˆæŸå¤±å€¼ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
    final_train_loss: float = None
    final_val_loss: float = None
    # é˜ˆå€¼è®¡ç®—å‚æ•°
    threshold_method: str = 'percentile'  # percentile, 3sigma, contamination
    percentile: float = 95.0
    residual_metric: str = 'rmse'  # rmse, l1, max

    def __post_init__(self):
        if not self.created_at:
            self.created_at = datetime.now().isoformat()
        if not self.updated_at:
            self.updated_at = datetime.now().isoformat()

    @property
    def config(self):
        """è¿”å›è®­ç»ƒé…ç½®å­—å…¸"""
        # å¦‚æœæœ‰åŸå§‹é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨åŸå§‹é…ç½®ï¼ˆåŒ…å«train_files, test_filesç­‰å®Œæ•´ä¿¡æ¯ï¼‰
        if self._raw_config is not None:
            return self._raw_config
        
        # å¦åˆ™è¿”å›ä»å­—æ®µæ„å»ºçš„é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        return {
            'module': self.module,
            'model_type': self.model_type,
            'data_path': self.data_path,
            'output_path': self.output_path,
            'input_dim': self.input_dim,
            'dataset_mode': self.dataset_mode,
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate,
            'validation_split': self.validation_split,
            'train_ratio': self.train_ratio,
            'val_ratio': self.val_ratio,
            'test_ratio': self.test_ratio,
            'val_ratio_from_train': self.val_ratio_from_train,
            'sequence_length': self.sequence_length,
            'prediction_horizon': self.prediction_horizon,
            'hidden_units': self.hidden_units,
            'num_layers': self.num_layers,
            'dropout': self.dropout,
            'activation': self.activation,
            'bidirectional': self.bidirectional,
            'preprocess_method': self.preprocess_method,
            'edge_host': self.edge_host,
            'edge_port': self.edge_port,
            'dataset_file': self.dataset_file,
            'train_file': self.train_file,
            'val_file': self.val_file,
            'test_file': self.test_file,
            'threshold_method': self.threshold_method,
            'percentile': self.percentile,
            'residual_metric': self.residual_metric,
        }

    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'task_id': self.task_id,
            'module': self.module,
            'model_type': self.model_type,
            'status': self.status,
            'progress': self.progress,
            'message': self.message,
            'model_save_path': self.model_save_path,  # åŒ…å«æ¨¡å‹ä¿å­˜è·¯å¾„
            'current_epoch': self.current_epoch,
            'completed_epochs': self.completed_epochs,
            'current_train_loss': self.current_train_loss,
            'current_val_loss': self.current_val_loss,
            'final_train_loss': self.final_train_loss,
            'final_val_loss': self.final_val_loss,
            'scaler_path': self.scaler_path,
            'threshold_path': self.threshold_path,
            'threshold_value': self.threshold_value,
            'threshold_metadata': self.threshold_metadata or {},
            'created_at': self.created_at,
            'updated_at': self.updated_at,
            'config': {
                'module': self.module,
                'model_type': self.model_type,
                'data_path': self.data_path,
                'output_path': self.output_path,
                'input_dim': self.input_dim,
                'dataset_mode': self.dataset_mode,
                'epochs': self.epochs,
                'batch_size': self.batch_size,
                'learning_rate': self.learning_rate,
                'validation_split': self.validation_split,
                'train_ratio': self.train_ratio,
                'val_ratio': self.val_ratio,
                'test_ratio': self.test_ratio,
                'val_ratio_from_train': self.val_ratio_from_train,
                'sequence_length': self.sequence_length,
                'prediction_horizon': self.prediction_horizon,
                'hidden_units': self.hidden_units,
                'num_layers': self.num_layers,
                'dropout': self.dropout,
                'activation': self.activation,
                'bidirectional': self.bidirectional,
                'preprocess_method': self.preprocess_method,
                'edge_host': self.edge_host,  # æ–°å¢
                'edge_port': self.edge_port,  # æ–°å¢
                'dataset_file': self.dataset_file,  # æ–°å¢
                'train_file': self.train_file,  # æ–°å¢
                'val_file': self.val_file,  # æ–°å¢
                'test_file': self.test_file,  # æ–°å¢
                'threshold_method': self.threshold_method,
                'percentile': self.percentile,
                'residual_metric': self.residual_metric,
            }
        }


class TrainingTaskManager:
    """è®­ç»ƒä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨"""
        self.tasks: Dict[str, TrainingTask] = {}
        self.task_threads: Dict[str, threading.Thread] = {}
        self.lock = threading.Lock()
        self._task_counter = 0  # ç”¨äºé¿å…åŒä¸€æ¯«ç§’å†…çš„é‡å¤
        self._last_ad_task_timestamp: Optional[str] = None

    def _generate_task_id(self, module: Optional[str] = None) -> str:
        """ç”ŸæˆåŸºäºæ—¶é—´çš„ä»»åŠ¡ID
        
        æ ¼å¼: YYYYMMDD_HHMMSS_XXX
        ç¤ºä¾‹: 20251119_143052_001
        
        Returns:
            str: å”¯ä¸€çš„ä»»åŠ¡ID
        """
        if module == 'anomaly_detection':
            while True:
                time_part = datetime.now().strftime("%Y%m%d_%H%M%S")
                with self.lock:
                    if time_part != self._last_ad_task_timestamp and time_part not in self.tasks:
                        self._last_ad_task_timestamp = time_part
                        return time_part
                time.sleep(0.2)

        with self.lock:
            self._task_counter = (self._task_counter + 1) % 1000
            now = datetime.now()
            time_part = now.strftime("%Y%m%d_%H%M%S")
            counter_part = f"{self._task_counter:03d}"
            return f"{time_part}_{counter_part}"

    def create_task(self, config: dict) -> TrainingTask:
        """åˆ›å»ºè®­ç»ƒä»»åŠ¡
        
        Args:
            config (dict): è®­ç»ƒé…ç½®
        
        Returns:
            TrainingTask: æ–°åˆ›å»ºçš„è®­ç»ƒä»»åŠ¡
        """
        module_name = config.get('module', 'anomaly_detection')
        task_id = self._generate_task_id(module_name)
        
        # å¤„ç†æ•°æ®åˆ’åˆ†å‚æ•° - å…¼å®¹ä¸åŒçš„å‚æ•°æ ¼å¼
        validation_split = 0.2  # é»˜è®¤å€¼
        
        # å¯¹äºLSTMé¢„æµ‹å™¨ï¼Œä½¿ç”¨val_ratioä½œä¸ºvalidation_split
        if config.get('model_type') == 'lstm_predictor':
            if 'val_ratio' in config:
                validation_split = float(config['val_ratio'])
            elif 'validation_split' in config:
                validation_split = float(config['validation_split'])
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„validation_splitå‚æ•°
            validation_split = float(config.get('validation_split', 0.2))
        
        def _to_int(value, default):
            try:
                if value is None or value == '':
                    return default
                return int(value)
            except (TypeError, ValueError):
                return default

        def _to_float(value, default):
            try:
                if value is None or value == '':
                    return default
                return float(value)
            except (TypeError, ValueError):
                return default

        def _to_bool(value, default=False):
            if value is None:
                return default
            if isinstance(value, bool):
                return value
            if isinstance(value, str):
                return value.strip().lower() in ('true', '1', 'yes', 'y')
            return bool(value)

        sequence_length = _to_int(config.get('sequence_length', config.get('seq_len')), 50)
        prediction_horizon = _to_int(config.get('prediction_horizon', config.get('pred_len')), 1)
        hidden_units = _to_int(config.get('hidden_units', config.get('hidden_dim')), 128)
        num_layers = _to_int(config.get('num_layers'), 2)
        dropout = _to_float(config.get('dropout'), 0.1)
        activation = config.get('activation', 'tanh')
        bidirectional = _to_bool(config.get('bidirectional'), False)

        train_ratio = _to_float(config.get('train_ratio'), None)
        val_ratio = _to_float(config.get('val_ratio'), None)
        test_ratio = _to_float(config.get('test_ratio'), None)
        val_ratio_from_train = _to_float(config.get('val_ratio_from_train'), None)

        preprocess_method = config.get('preprocess_method')

        # å¤„ç†é˜ˆå€¼è®¡ç®—å‚æ•°
        threshold_method = config.get('threshold_method', 'percentile')
        percentile = _to_float(config.get('percentile'), 95.0)
        residual_metric = config.get('residual_metric', 'rmse')

        task = TrainingTask(
            task_id=task_id,
            module=module_name,
            model_type=config.get('model_type', 'lstm'),
            data_path=config.get('data_path') or config.get('dataset_file'),  # å°è¯•å¤šç§å­—æ®µå
            output_path=config.get('output_path', f'output/model_{task_id}.ckpt'),
            input_dim=_to_int(config.get('input_dim'), 10),
            dataset_mode=config.get('dataset_mode', 'one'),
            epochs=_to_int(config.get('epochs'), 100),
            batch_size=_to_int(config.get('batch_size'), 32),
            learning_rate=_to_float(config.get('learning_rate'), 0.001),
            validation_split=validation_split,
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            val_ratio_from_train=val_ratio_from_train,
            sequence_length=sequence_length,
            prediction_horizon=prediction_horizon,
            hidden_units=hidden_units,
            num_layers=num_layers,
            dropout=dropout,
            activation=activation,
            bidirectional=bidirectional,
            preprocess_method=preprocess_method,
            edge_host=config.get('edge_host'),  # æ–°å¢
            edge_port=int(config.get('edge_port', 5000)) if config.get('edge_port') else None,  # æ–°å¢
            dataset_file=config.get('dataset_file'),  # æ–°å¢
            train_file=config.get('train_file'),  # æ–°å¢
            val_file=config.get('val_file'),  # æ–°å¢
            test_file=config.get('test_file'),  # æ–°å¢
            threshold_method=threshold_method,
            percentile=percentile,
            residual_metric=residual_metric,
        )
        
        # è°ƒè¯•ï¼šæ‰“å°æ¥æ”¶åˆ°çš„é…ç½®å’Œåˆ›å»ºçš„ä»»åŠ¡å‚æ•°
        print(f"ğŸ” ä»»åŠ¡åˆ›å»ºè°ƒè¯•ä¿¡æ¯:")
        print(f"   æ¥æ”¶åˆ°çš„config: {config}")
        print(f"   åˆ›å»ºçš„ä»»åŠ¡epochs: {task.epochs}")
        print(f"   åˆ›å»ºçš„ä»»åŠ¡batch_size: {task.batch_size}")
        print(f"   åˆ›å»ºçš„ä»»åŠ¡learning_rate: {task.learning_rate}")

        # ä¿å­˜å®Œæ•´çš„åŸå§‹é…ç½®ï¼ˆåŒ…æ‹¬train_files, test_filesç­‰ï¼‰
        task._raw_config = config.copy()

        with self.lock:
            self.tasks[task_id] = task

        logger.info(f"Created training task: {task_id}")
        return task

    def get_task(self, task_id: str) -> Optional[TrainingTask]:
        """è·å–è®­ç»ƒä»»åŠ¡
        
        Args:
            task_id (str): ä»»åŠ¡ID
        
        Returns:
            TrainingTask: è®­ç»ƒä»»åŠ¡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        with self.lock:
            return self.tasks.get(task_id)

    def list_tasks(self, status: Optional[str] = None) -> List[TrainingTask]:
        """åˆ—å‡ºæ‰€æœ‰è®­ç»ƒä»»åŠ¡
        
        Args:
            status (str, optional): ç­›é€‰çŠ¶æ€
        
        Returns:
            List[TrainingTask]: ä»»åŠ¡åˆ—è¡¨
        """
        with self.lock:
            if status:
                return [t for t in self.tasks.values() if t.status == status]
            return list(self.tasks.values())

    def update_task_status(self, task_id: str, status: str, message: str = "", progress: int = None, current_epoch: int = None, train_loss: float = None, val_loss: float = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id (str): ä»»åŠ¡ID
            status (str): æ–°çŠ¶æ€
            message (str): çŠ¶æ€æ¶ˆæ¯
            progress (int): è¿›åº¦ç™¾åˆ†æ¯”
            current_epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡
            train_loss (float): å½“å‰è®­ç»ƒæŸå¤±å€¼
            val_loss (float): å½“å‰éªŒè¯æŸå¤±å€¼
        """
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.status = status
                task.updated_at = datetime.now().isoformat()
                if message:
                    task.message = message
                if progress is not None:
                    task.progress = progress
                if current_epoch is not None:
                    task.current_epoch = current_epoch
                    # å¦‚æœçŠ¶æ€ä¸ºå®Œæˆï¼Œæ›´æ–°completed_epochs
                    if status in ['completed', 'threshold_completed']:
                        task.completed_epochs = current_epoch
                if train_loss is not None:
                    task.current_train_loss = train_loss
                if val_loss is not None:
                    task.current_val_loss = val_loss
                logger.info(f"Updated task {task_id}: status={status}, progress={progress}%, epoch={current_epoch}, train_loss={train_loss}, val_loss={val_loss}")

    def update_final_losses(self, task_id: str, train_loss: float = None, val_loss: float = None):
        """æ›´æ–°ä»»åŠ¡çš„æœ€ç»ˆæŸå¤±å€¼
        
        Args:
            task_id (str): ä»»åŠ¡ID
            train_loss (float): æœ€ç»ˆè®­ç»ƒæŸå¤±
            val_loss (float): æœ€ç»ˆéªŒè¯æŸå¤±
        """
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                if train_loss is not None:
                    task.final_train_loss = train_loss
                if val_loss is not None:
                    task.final_val_loss = val_loss
                task.updated_at = datetime.now().isoformat()
                logger.info(f"Updated final losses for task {task_id}: train_loss={train_loss}, val_loss={val_loss}")

    def update_model_save_path(self, task_id: str, model_path: str):
        """æ›´æ–°æ¨¡å‹ä¿å­˜è·¯å¾„
        
        Args:
            task_id (str): ä»»åŠ¡ID
            model_path (str): æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.model_save_path = model_path
                task.updated_at = datetime.now().isoformat()
                logger.info(f"Updated model save path for task {task_id}: {model_path}")

    def update_scaler_path(self, task_id: str, scaler_path: str):
        """è®°å½•æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶è·¯å¾„"""
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.scaler_path = scaler_path
                task.updated_at = datetime.now().isoformat()
                logger.info(f"Updated scaler path for task {task_id}: {scaler_path}")

    def update_threshold_info(self, task_id: str, threshold_path: str, threshold_value: float, metadata: Optional[dict] = None):
        """è®°å½•é˜ˆå€¼æ–‡ä»¶åŠä¿¡æ¯"""
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.threshold_path = threshold_path
                task.threshold_value = threshold_value
                task.threshold_metadata = metadata or {}
                task.updated_at = datetime.now().isoformat()
                logger.info(
                    f"Updated threshold info for task {task_id}: value={threshold_value}, path={threshold_path}"
                )

    def update_task_logs(self, task_id: str, logs: str):
        """æ›´æ–°ä»»åŠ¡æ—¥å¿—
        
        Args:
            task_id (str): ä»»åŠ¡ID
            logs (str): æ—¥å¿—å†…å®¹
        """
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.logs = logs

    def add_log(self, task_id: str, log_message: str):
        """æ·»åŠ æ—¥å¿—æ¶ˆæ¯
        
        Args:
            task_id (str): ä»»åŠ¡ID
            log_message (str): æ—¥å¿—æ¶ˆæ¯
        """
        # ç›´æ¥ä½¿ç”¨æ—¥å¿—ç³»ç»Ÿè®°å½•
        logger.info(f"[{task_id}] {log_message}")
        
        # åŒæ—¶ç´¯ç§¯åˆ°ä»»åŠ¡å¯¹è±¡çš„logså­—æ®µï¼Œæ·»åŠ æ—¶é—´æˆ³
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                # æ·»åŠ æ—¶é—´æˆ³åˆ°æ—¥å¿—æ¶ˆæ¯ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS,mmmï¼‰
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]  # ä¿ç•™æ¯«ç§’
                log_with_timestamp = f"{timestamp} - {log_message}"
                if task.logs:
                    task.logs += f"\n{log_with_timestamp}"
                else:
                    task.logs = log_with_timestamp

    def fail_task(self, task_id: str, error: str):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤±è´¥
        
        Args:
            task_id (str): ä»»åŠ¡ID
            error (str): é”™è¯¯ä¿¡æ¯
        """
        with self.lock:
            task = self.tasks.get(task_id)
            if task:
                task.status = 'failed'
                task.error = error
                task.updated_at = datetime.now().isoformat()
                logger.error(f"Task {task_id} failed: {error}")

    def start_training(self, task_id: str, training_function):
        """å¯åŠ¨å¼‚æ­¥è®­ç»ƒä»»åŠ¡
        
        Args:
            task_id (str): ä»»åŠ¡ID
            training_function (callable): è®­ç»ƒå‡½æ•°
        """
        task = self.get_task(task_id)
        if not task:
            logger.error(f"Task {task_id} not found")
            return

        # æ ‡è®°ä¸ºè¿è¡Œä¸­
        self.update_task_status(task_id, 'running', "å¼€å§‹è®­ç»ƒ...")

        # åˆ›å»ºåå°çº¿ç¨‹æ‰§è¡Œè®­ç»ƒ
        thread = threading.Thread(
            target=self._execute_training,
            args=(task_id, training_function),
            daemon=True
        )
        thread.start()
        self.task_threads[task_id] = thread

    def _execute_training(self, task_id: str, training_function):
        """æ‰§è¡Œè®­ç»ƒçš„å†…éƒ¨å‡½æ•°ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        
        Args:
            task_id (str): ä»»åŠ¡ID
            training_function (callable): è®­ç»ƒå‡½æ•°
        """
        try:
            task = self.get_task(task_id)
            if not task:
                return

            # æ‰§è¡Œè®­ç»ƒå‡½æ•°ï¼Œä¼ é€’task_idè€Œä¸æ˜¯taskå¯¹è±¡
            result = training_function(task_id)

            # æ£€æŸ¥è®­ç»ƒå‡½æ•°æ˜¯å¦è¿”å›äº†ç»“æœ
            if result is None:
                # å¦‚æœæ²¡æœ‰è¿”å›ç»“æœï¼Œæ£€æŸ¥ä»»åŠ¡å½“å‰çŠ¶æ€
                current_task = self.get_task(task_id)
                if current_task and current_task.status == 'completed':
                    logger.info(f"Task {task_id} completed successfully (no return result)")
                else:
                    logger.warning(f"Task {task_id} finished without result and status is: {current_task.status if current_task else 'not found'}")
            elif result.get('success'):
                # åªæœ‰åœ¨è®­ç»ƒå‡½æ•°æ˜ç¡®è¿”å›æˆåŠŸç»“æœæ—¶æ‰æ›´æ–°çŠ¶æ€ï¼ˆé¿å…é‡å¤æ›´æ–°ï¼‰
                logger.info(f"Task {task_id} completed successfully with result")
            else:
                # å¤„ç†æ˜ç¡®çš„å¤±è´¥ç»“æœ
                error_msg = result.get('error', 'è®­ç»ƒå¤±è´¥')
                self.fail_task(task_id, error_msg)

        except Exception as e:
            error_msg = f"è®­ç»ƒå¼‚å¸¸: {str(e)}"
            self.fail_task(task_id, error_msg)
            logger.exception(f"Error in training task {task_id}")

    def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆè®­ç»ƒä»»åŠ¡
        
        Args:
            task_id (str): ä»»åŠ¡ID
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        task = self.get_task(task_id)
        if not task:
            return False

        if task.status in ['completed', 'failed']:
            return False

        self.update_task_status(task_id, 'cancelled', "ä»»åŠ¡å·²å–æ¶ˆ")
        return True


# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
task_manager = TrainingTaskManager()


def get_task_manager() -> TrainingTaskManager:
    """è·å–å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
    
    Returns:
        TrainingTaskManager: ä»»åŠ¡ç®¡ç†å™¨
    """
    return task_manager
