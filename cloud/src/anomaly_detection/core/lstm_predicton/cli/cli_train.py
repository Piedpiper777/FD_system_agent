#!/usr/bin/env python3
"""
LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - å‘½ä»¤è¡Œè®­ç»ƒè„šæœ¬ (Server Machine Dataset)

ä¸“é—¨ç”¨äºè®­ç»ƒServer Machine Datasetçš„LSTMå¼‚å¸¸æ£€æµ‹æ¨¡å‹

æ•°æ®é›†æ ¼å¼ï¼š
- è®­ç»ƒæ•°æ®ï¼štimestamp,col_0,col_1,...,col_37 (38ä¸ªç‰¹å¾åˆ—)
- æµ‹è¯•æ•°æ®ï¼štimestamp,col_0,col_1,...,col_37,label (38ä¸ªç‰¹å¾åˆ— + 1ä¸ªæ ‡ç­¾åˆ—)

ä½¿ç”¨æ–¹æ³•ï¼š
python cli_train.py --data_path /path/to/machine-1-1_train.csv --model_path models/machine_model.ckpt
"""

import argparse
import sys
import json
from pathlib import Path
from typing import Optional, List, Tuple
import numpy as np
import mindspore as ms

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.parent.parent.parent.parent.parent
sys.path.insert(0, str(current_dir))

from src.anomaly_detection.core.lstm_predicton.data_processor import DataProcessor, TimeSeriesData
from src.anomaly_detection.core.lstm_predicton.model_builder import ModelBuilder
from src.anomaly_detection.core.lstm_predicton.trainer import Trainer
from src.anomaly_detection.core.lstm_predicton.threshold_calculator import ThresholdCalculator


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - Server Machine Datasetè®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ•°æ®é›†ä¿¡æ¯ï¼š
  - è®­ç»ƒæ•°æ®æ ¼å¼ï¼štimestamp,col_0,col_1,...,col_37 (38ä¸ªç‰¹å¾åˆ—)
  - æµ‹è¯•æ•°æ®æ ¼å¼ï¼štimestamp,col_0,col_1,...,col_37,label (38ä¸ªç‰¹å¾åˆ— + æ ‡ç­¾åˆ—)
  - æ•°æ®å·²é¢„å¤„ç†ä¸º0-1èŒƒå›´

ä½¿ç”¨ç¤ºä¾‹ï¼š
  python cli_train.py --data_path data/machine-1-1_train.csv --model_path models/machine_model.ckpt

  python cli_train.py --data_path data/machine-1-1_train.csv --model_path models/machine_model.ckpt \\
                      --sequence_length 100 --epochs 50 --batch_size 64
        """
    )

    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument('--data_path', type=str, required=True,
                       help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼)')
    parser.add_argument('--timestamp_column', type=str, default='timestamp',
                       help='æ—¶é—´æˆ³åˆ—å (é»˜è®¤: timestamp)')

    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--sequence_length', type=int, default=50,
                       help='åºåˆ—é•¿åº¦ (é»˜è®¤: 50)')
    parser.add_argument('--prediction_horizon', type=int, default=1,
                       help='é¢„æµ‹æ­¥é•¿ (é»˜è®¤: 1)')
    parser.add_argument('--hidden_size', type=int, default=128,
                       help='LSTMéšè—å•å…ƒæ•° (é»˜è®¤: 128)')
    parser.add_argument('--num_layers', type=int, default=2,
                       help='LSTMå±‚æ•° (é»˜è®¤: 2)')

    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤§å° (é»˜è®¤: 32)')
    parser.add_argument('--epochs', type=int, default=30,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 30)')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.001)')
    parser.add_argument('--patience', type=int, default=10,
                       help='æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 10)')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)')

    # è¾“å‡ºç›¸å…³å‚æ•°
    parser.add_argument('--model_path', type=str, default=None,
                       help='æ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: models/anomaly_detection/lstm/lstm_anomaly_model_YYYYMMDD_HHMMSS.ckpt)')
    parser.add_argument('--threshold_path', type=str, default=None,
                       help='é˜ˆå€¼ä¿å­˜è·¯å¾„ (é»˜è®¤: model_pathåŒç›®å½•çš„threshold.json)')
    parser.add_argument('--scaler_path', type=str, default=None,
                       help='æ ‡å‡†åŒ–å‚æ•°ä¿å­˜è·¯å¾„ (é»˜è®¤: model_pathåŒç›®å½•çš„scaler.npz)')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--device_target', type=str, default='CPU',
                       choices=['CPU', 'GPU', 'Ascend'],
                       help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: CPU)')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    parser.add_argument('--verbose', action='store_true',
                       help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')

    return parser.parse_args()


def setup_mindspore_context(device_target: str, seed: int):
    """è®¾ç½®MindSporeä¸Šä¸‹æ–‡"""
    ms.set_context(mode=ms.PYNATIVE_MODE)
    ms.set_device(device_target)
    ms.set_seed(seed)
    print(f"ğŸ”§ MindSporeä¸Šä¸‹æ–‡è®¾ç½®å®Œæˆ: è®¾å¤‡={device_target}, ç§å­={seed}")


def load_and_preprocess_data(args) -> Tuple[TimeSeriesData, TimeSeriesData, DataProcessor]:
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print("ğŸ“Š å¼€å§‹åŠ è½½å’Œé¢„å¤„ç†Server Machineæ•°æ®...")

    # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
    processor = DataProcessor(
        sequence_length=args.sequence_length,
        prediction_horizon=args.prediction_horizon,
        normalize=False  # æ•°æ®å·²ç»æ ‡å‡†åŒ–
    )

    # åŠ è½½æ•°æ®å¹¶å¤„ç†æµæ°´çº¿
    try:
        train_data, val_data = processor.process_pipeline(
            args.data_path,
            train_ratio=args.train_ratio
        )

        print("âœ… æ•°æ®å¤„ç†å®Œæˆ")
        print(f"  - è®­ç»ƒé›†: {len(train_data.sequences)} åºåˆ—")
        print(f"  - éªŒè¯é›†: {len(val_data.sequences)} åºåˆ—")
        print(f"  - ç‰¹å¾æ•°: {train_data.sequences.shape[2]}")
        print(f"  - åºåˆ—é•¿åº¦: {args.sequence_length}")
        print(f"  - æ•°æ®èŒƒå›´: [{train_data.sequences.min():.3f}, {train_data.sequences.max():.3f}]")

        return train_data, val_data, processor

    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)


def create_and_train_model(train_data: TimeSeriesData, val_data: TimeSeriesData,
                          processor: DataProcessor, args) -> Trainer:
    """åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹"""
    print("\nğŸ¤– å¼€å§‹åˆ›å»ºå’Œè®­ç»ƒLSTMæ¨¡å‹...")

    # åˆ›å»ºæ¨¡å‹
    input_shape = (args.sequence_length, train_data.sequences.shape[2])
    model = ModelBuilder.create_model(
        'lstm_predictor',
        input_shape=input_shape,
        hidden_units=args.hidden_size,
        num_layers=args.num_layers
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        learning_rate=args.learning_rate
    )

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = ms.dataset.NumpySlicesDataset(
        {'sequences': train_data.sequences, 'targets': train_data.targets},
        shuffle=True
    ).batch(args.batch_size)

    val_dataset = ms.dataset.NumpySlicesDataset(
        {'sequences': val_data.sequences, 'targets': val_data.targets},
        shuffle=False
    ).batch(args.batch_size)

    # è®­ç»ƒæ¨¡å‹
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {args.epochs}è½®, æ‰¹å¤§å°={args.batch_size}, å­¦ä¹ ç‡={args.learning_rate}")
    trained_model = trainer.train(
        train_loader=train_dataset,
        num_epochs=args.epochs,
        val_loader=val_dataset,
        patience=args.patience
    )

    # è·å–è®­ç»ƒæŒ‡æ ‡
    metrics = trainer.get_training_metrics()
    print("âœ… è®­ç»ƒå®Œæˆ")
    print(f"  - å®é™…è®­ç»ƒè½®æ•°: {metrics['epochs_trained']}")
    print(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {metrics.get('final_train_loss', 'N/A')}")
    print(f"  - æœ€ç»ˆéªŒè¯æŸå¤±: {metrics.get('final_val_loss', 'N/A')}")

    return trainer


def calculate_and_save_threshold(trainer: Trainer, train_data: TimeSeriesData,
                               threshold_path: str):
    """è®¡ç®—å¹¶ä¿å­˜å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
    print("\nğŸ¯ å¼€å§‹è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼...")

    # ä»è®­ç»ƒæ•°æ®ä¸­é‡‡æ ·é¢„æµ‹ç»“æœæ¥è®¡ç®—é˜ˆå€¼
    model = trainer.model

    # ä¸ºäº†è®¡ç®—é˜ˆå€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å‰Nä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹
    n_samples = min(2000, len(train_data.sequences))  # æœ€å¤šä½¿ç”¨2000ä¸ªæ ·æœ¬
    sample_sequences = train_data.sequences[:n_samples]
    sample_targets = train_data.targets[:n_samples]

    predictions = []
    for i in range(0, len(sample_sequences), 32):  # æ‰¹å¤„ç†é¢„æµ‹
        batch_seq = sample_sequences[i:i+32]
        batch_tensor = ms.Tensor(batch_seq, ms.float32)
        batch_pred = model(batch_tensor)
        predictions.extend(batch_pred.asnumpy())

    predictions = np.array(predictions)
    actuals = sample_targets

    # åˆ›å»ºé˜ˆå€¼è®¡ç®—å™¨
    threshold_calculator = ThresholdCalculator(residual_method='l2_norm')

    # è®¡ç®—é˜ˆå€¼
    threshold = threshold_calculator.fit_threshold(
        predictions, actuals,
        method='percentile', percentile=95.0
    )

    # ä¿å­˜é˜ˆå€¼
    threshold_calculator.save_threshold(threshold_path)
    print(f"âœ… é˜ˆå€¼è®¡ç®—å®Œæˆ: {threshold:.6f}")
    print(f"ğŸ’¾ é˜ˆå€¼å·²ä¿å­˜: {threshold_path}")

    return threshold_calculator


def save_scaler_params(processor: DataProcessor, scaler_path: str):
    """ä¿å­˜æ ‡å‡†åŒ–å‚æ•°"""
    scaler_params = processor.get_scaler_params()
    processor.save_scaler_params(scaler_path)
    print(f"ğŸ’¾ æ ‡å‡†åŒ–å‚æ•°å·²ä¿å­˜: {scaler_path}")


def save_model(trainer: Trainer, model_path: str):
    """ä¿å­˜æ¨¡å‹"""
    model_path = Path(model_path)
    model_path.parent.mkdir(parents=True, exist_ok=True)

    ms.save_checkpoint(trainer.model, str(model_path))
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")


def save_training_config(args, config_path: str):
    """ä¿å­˜è®­ç»ƒé…ç½®"""
    config = {
        'data_path': args.data_path,
        'timestamp_column': args.timestamp_column,
        'sequence_length': args.sequence_length,
        'prediction_horizon': args.prediction_horizon,
        'hidden_size': args.hidden_size,
        'num_layers': args.num_layers,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'learning_rate': args.learning_rate,
        'patience': args.patience,
        'train_ratio': args.train_ratio,
        'device_target': args.device_target,
        'seed': args.seed,
        'dataset_info': {
            'name': 'Server Machine Dataset',
            'features': 38,
            'description': 'æœåŠ¡å™¨æœºå™¨å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ï¼Œå·²é¢„å¤„ç†'
        }
    }

    config_path = Path(config_path)
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - Server Machine Datasetè®­ç»ƒè„šæœ¬")
    print("=" * 70)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # è®¾ç½®è¾“å‡ºè·¯å¾„ - ä¼˜åŒ–ç‰ˆæœ¬
    if args.model_path is None:
        # é»˜è®¤ä¿å­˜åˆ° anomaly_detection/lstm ç›®å½•
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir = Path("models/anomaly_detection/lstm")
        model_dir.mkdir(parents=True, exist_ok=True)
        model_path = model_dir / f"lstm_anomaly_model_{timestamp}.ckpt"
    else:
        model_path = Path(args.model_path)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šå®Œæ•´è·¯å¾„ï¼Œè‡ªåŠ¨åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç›®å½•ç»“æ„
    if not args.threshold_path and not args.scaler_path:
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        model_path.parent.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ¨¡å‹æ–‡ä»¶åï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šå…·ä½“æ–‡ä»¶åï¼‰
        if model_path.name == model_path.parent.name or not model_path.suffix:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = f"lstm_anomaly_model_{timestamp}.ckpt"
            model_path = model_path.parent / model_name

        # è®¾ç½®ç›¸å…³æ–‡ä»¶è·¯å¾„
        base_name = model_path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
        if args.threshold_path is None:
            args.threshold_path = str(model_path.parent / f"{base_name}_threshold.json")
        if args.scaler_path is None:
            args.scaler_path = str(model_path.parent / f"{base_name}_scaler.npz")

    config_path = str(model_path.parent / f"{model_path.stem}_config.json")

    print(f"ğŸ“ è¾“å‡ºç›®å½•: {model_path.parent}")
    print(f"ğŸ“„ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“„ é˜ˆå€¼è·¯å¾„: {args.threshold_path}")
    print(f"ğŸ“„ æ ‡å‡†åŒ–å‚æ•°è·¯å¾„: {args.scaler_path}")
    print()

    try:
        # è®¾ç½®MindSpore
        setup_mindspore_context(args.device_target, args.seed)

        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        train_data, val_data, processor = load_and_preprocess_data(args)

        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        trainer = create_and_train_model(train_data, val_data, processor, args)

        # ä¿å­˜æ¨¡å‹
        save_model(trainer, str(model_path))

        # è®¡ç®—å¹¶ä¿å­˜é˜ˆå€¼
        threshold_calculator = calculate_and_save_threshold(
            trainer, train_data, args.threshold_path
        )

        # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
        save_scaler_params(processor, args.scaler_path)

        # ä¿å­˜è®­ç»ƒé…ç½®
        save_training_config(args, config_path)

        print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print("=" * 70)
        print("ğŸ“Š è®­ç»ƒæ€»ç»“:")
        print(f"  - è®­ç»ƒæ•°æ®: {args.data_path}")
        print(f"  - æ•°æ®é›†: Server Machine Dataset (38ä¸ªç‰¹å¾)")
        print(f"  - æ¨¡å‹ä¿å­˜: {model_path}")
        print(f"  - é˜ˆå€¼ä¿å­˜: {args.threshold_path}")
        print(f"  - æ ‡å‡†åŒ–å‚æ•°: {args.scaler_path}")
        print(f"  - è®­ç»ƒé…ç½®: {config_path}")
        print("\nğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼")
        print(f"   python cli_evaluate.py --model_path {model_path} --test_data_path your_test_data.csv")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()