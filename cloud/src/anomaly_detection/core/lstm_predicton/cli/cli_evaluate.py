#!/usr/bin/env python3
"""
LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - Server Machine Datasetè¯„ä¼°è„šæœ¬
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„LSTMæ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Tuple, Optional, Dict

import mindspore as ms
import numpy as np
from sklearn.metrics import classification_report

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import sys
from pathlib import Path

# ä»cliç›®å½•å‘ä¸Šæ‰¾åˆ°cloudç›®å½•: cli -> lstm_predicton -> core -> anomaly_detection -> src -> cloud
cloud_dir = Path(__file__).parent.parent.parent.parent.parent.parent
sys.path.insert(0, str(cloud_dir))

from src.anomaly_detection.core.lstm_predicton.data_processor import DataProcessor, TimeSeriesData
from src.anomaly_detection.core.lstm_predicton.model_builder import ModelBuilder
from src.anomaly_detection.core.lstm_predicton.evaluator import Evaluator
from src.anomaly_detection.core.lstm_predicton.threshold_calculator import ThresholdCalculator


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - æ¨¡å‹è¯„ä¼°è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python cli_evaluate.py --model_path models/test_machine_model.ckpt --test_data_path data/test.csv
  python cli_evaluate.py --model_path models/model.ckpt --test_data_path data/test.csv --threshold_path models/threshold.json --scaler_path models/scaler.npz --output_dir results/
        """
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument(
        '--model_path',
        type=str,
        default=None,
        help='è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.ckpt)ï¼Œé»˜è®¤åœ¨ models/anomaly_detection/lstm/ ç›®å½•ä¸­æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹'
    )

    parser.add_argument(
        '--test_data_path',
        type=str,
        required=True,
        help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ (.csv)'
    )

    # å¯é€‰å‚æ•°
    parser.add_argument(
        '--threshold_path',
        type=str,
        default=None,
        help='å¼‚å¸¸æ£€æµ‹é˜ˆå€¼æ–‡ä»¶è·¯å¾„ (.json)ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ä»æ¨¡å‹ç›®å½•æŸ¥æ‰¾'
    )

    parser.add_argument(
        '--scaler_path',
        type=str,
        default=None,
        help='æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶è·¯å¾„ (.npz)ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ä»æ¨¡å‹ç›®å½•æŸ¥æ‰¾'
    )

    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: model_path/evaluations)ï¼Œä½¿ç”¨"auto"è‡ªåŠ¨åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç›®å½•'
    )

    parser.add_argument(
        '--sequence_length',
        type=int,
        default=50,
        help='åºåˆ—é•¿åº¦ (é»˜è®¤: 50)'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=32,
        help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 32)'
    )

    parser.add_argument(
        '--timestamp_column',
        type=str,
        default='timestamp',
        help='æ—¶é—´æˆ³åˆ—å (é»˜è®¤: timestamp)'
    )

    parser.add_argument(
        '--label_column',
        type=str,
        default='label',
        help='æ ‡ç­¾åˆ—åï¼Œç”¨äºçœŸå®å¼‚å¸¸æ ‡ç­¾ (é»˜è®¤: label)'
    )

    parser.add_argument(
        '--device_target',
        type=str,
        default='CPU',
        choices=['CPU', 'GPU', 'Ascend'],
        help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: CPU)'
    )

    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­ (é»˜è®¤: 42)'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='å¯ç”¨è¯¦ç»†è¾“å‡º'
    )

    return parser.parse_args()


def setup_mindspore_context(device_target: str = 'CPU', seed: int = 42):
    """è®¾ç½®MindSporeä¸Šä¸‹æ–‡"""
    ms.set_context(mode=ms.GRAPH_MODE)
    ms.set_device(device_target)
    ms.set_seed(seed)
    print(f"ğŸ”§ MindSporeä¸Šä¸‹æ–‡è®¾ç½®å®Œæˆ: è®¾å¤‡={device_target}, ç§å­={seed}")


def load_model_and_components(model_path: str, threshold_path: Optional[str] = None,
                            scaler_path: Optional[str] = None) -> Tuple[ms.nn.Cell, ThresholdCalculator, DataProcessor]:
    """
    åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        threshold_path: é˜ˆå€¼æ–‡ä»¶è·¯å¾„
        scaler_path: æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶è·¯å¾„

    Returns:
        æ¨¡å‹ã€é˜ˆå€¼è®¡ç®—å™¨ã€æ•°æ®å¤„ç†å™¨
    """
    model_path = Path(model_path)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œè‡ªåŠ¨ä»æ¨¡å‹ç›®å½•æŸ¥æ‰¾
    if threshold_path is None:
        threshold_path = model_path.parent / "threshold.json"
    else:
        threshold_path = Path(threshold_path)

    if scaler_path is None:
        scaler_path = model_path.parent / "scaler.npz"
    else:
        scaler_path = Path(scaler_path)

    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹å’Œç»„ä»¶...")
    print(f"  - æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  - é˜ˆå€¼è·¯å¾„: {threshold_path}")
    print(f"  - æ ‡å‡†åŒ–å‚æ•°è·¯å¾„: {scaler_path}")

    # åŠ è½½æ¨¡å‹
    if not model_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    # é¦–å…ˆåˆ›å»ºæ¨¡å‹ç»“æ„ï¼ˆéœ€è¦ä»é…ç½®ä¸­è·å–å‚æ•°ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
    model_builder = ModelBuilder()
    model = model_builder.build_lstm_predictor(
        input_shape=(50, 38),  # åŸºäºè®­ç»ƒé…ç½®
        hidden_units=128,
        num_layers=2
    )

    # åŠ è½½æƒé‡
    param_dict = ms.load_checkpoint(str(model_path))
    ms.load_param_into_net(model, param_dict)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")

    # åŠ è½½é˜ˆå€¼è®¡ç®—å™¨
    if not threshold_path.exists():
        print(f"âš ï¸ é˜ˆå€¼æ–‡ä»¶ä¸å­˜åœ¨: {threshold_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤é˜ˆå€¼")
        threshold_calculator = ThresholdCalculator(residual_method='l2_norm')
        threshold_calculator.threshold = 0.1  # é»˜è®¤é˜ˆå€¼
    else:
        threshold_calculator = ThresholdCalculator(residual_method='l2_norm')
        threshold_calculator.load_threshold(str(threshold_path))
        print(f"âœ… é˜ˆå€¼åŠ è½½å®Œæˆ: {threshold_path} (é˜ˆå€¼: {threshold_calculator.threshold:.6f})")

    # åŠ è½½æ•°æ®å¤„ç†å™¨
    processor = DataProcessor(
        sequence_length=50,
        prediction_horizon=1,
        normalize=False  # æµ‹è¯•æ—¶ä¸è¿›è¡Œæ ‡å‡†åŒ–
    )

    if scaler_path.exists():
        processor.load_scaler_params(str(scaler_path))
        print(f"âœ… æ ‡å‡†åŒ–å‚æ•°åŠ è½½å®Œæˆ: {scaler_path}")
    else:
        print(f"âš ï¸ æ ‡å‡†åŒ–å‚æ•°æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}ï¼Œå°†ä½¿ç”¨åŸå§‹æ•°æ®")

    return model, threshold_calculator, processor


def load_and_preprocess_test_data(data_path: str, processor: DataProcessor,
                                timestamp_column: str = 'timestamp',
                                label_column: str = 'label') -> Tuple[TimeSeriesData, np.ndarray]:
    """
    åŠ è½½å’Œé¢„å¤„ç†æµ‹è¯•æ•°æ®

    Args:
        data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        processor: æ•°æ®å¤„ç†å™¨
        timestamp_column: æ—¶é—´æˆ³åˆ—å
        label_column: æ ‡ç­¾åˆ—å

    Returns:
        å¤„ç†åçš„æ•°æ®å’ŒçœŸå®æ ‡ç­¾
    """
    print(f"ğŸ“Š å¼€å§‹åŠ è½½å’Œé¢„å¤„ç†æµ‹è¯•æ•°æ®...")
    print(f"  - æ•°æ®è·¯å¾„: {data_path}")
    print(f"  - æ—¶é—´æˆ³åˆ—: {timestamp_column}")
    print(f"  - æ ‡ç­¾åˆ—: {label_column}")

    # åŠ è½½æ•°æ®
    data_path = Path(data_path)
    if not data_path.exists():
        raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

    # åŠ è½½CSVæ•°æ®
    df = processor.load_data(str(data_path), timestamp_column=timestamp_column, label_column=label_column)

    # ä»processorä¸­è·å–è¯†åˆ«çš„åˆ—ä¿¡æ¯
    timestamp_col = processor.timestamp_column
    feature_cols = processor.feature_names
    label_col = processor.label_column

    print(f"ğŸ” åˆ—è¯†åˆ«å®Œæˆ:")
    print(f"  - æ—¶é—´æˆ³åˆ—: {timestamp_col}")
    print(f"  - ç‰¹å¾åˆ—: {feature_cols}")
    print(f"  - æ ‡ç­¾åˆ—: {label_col}")

    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    features = df[feature_cols].values
    true_labels = df[label_col].values if label_col else np.zeros(len(df))

    print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {data_path}")
    print(f"  - æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"  - ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"  - å¼‚å¸¸æ ·æœ¬æ•°: {np.sum(true_labels)} / {len(true_labels)} ({np.sum(true_labels)/len(true_labels)*100:.2f}%)")

    # åˆ›å»ºåºåˆ—æ•°æ®
    time_series_data = processor.create_sequences(features)
    sequences = time_series_data.sequences
    targets = time_series_data.targets
    print(f"ğŸ”„ åºåˆ—åˆ›å»ºå®Œæˆ")
    print(f"  - ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
    print(f"  - åºåˆ—å½¢çŠ¶: {sequences.shape}")

    # åˆ›å»ºTimeSeriesDataå¯¹è±¡
    test_data = time_series_data

    return test_data, true_labels[len(true_labels) - len(sequences):]  # å¯¹é½åºåˆ—é•¿åº¦


def perform_inference(model: ms.nn.Cell, test_data: TimeSeriesData,
                     batch_size: int = 32) -> np.ndarray:
    """
    æ‰§è¡Œæ¨ç†

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_data: æµ‹è¯•æ•°æ®
        batch_size: æ‰¹å¤„ç†å¤§å°

    Returns:
        é¢„æµ‹ç»“æœ
    """
    print(f"ğŸ”® å¼€å§‹æ¨¡å‹æ¨ç†...")

    model.set_train(False)
    predictions = []

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(test_data.sequences), batch_size):
        batch_sequences = test_data.sequences[i:i+batch_size]
        batch_tensor = ms.Tensor(batch_sequences, ms.float32)

        batch_pred = model(batch_tensor)
        predictions.extend(batch_pred.asnumpy())

    predictions = np.array(predictions)
    print(f"âœ… æ¨ç†å®Œæˆ: {len(predictions)} ä¸ªé¢„æµ‹ç»“æœ")

    return predictions


def calculate_anomaly_scores(predictions: np.ndarray, actuals: np.ndarray,
                           method: str = 'l2_norm') -> np.ndarray:
    """
    è®¡ç®—å¼‚å¸¸åˆ†æ•°

    Args:
        predictions: é¢„æµ‹å€¼
        actuals: å®é™…å€¼
        method: è®¡ç®—æ–¹æ³•

    Returns:
        å¼‚å¸¸åˆ†æ•°
    """
    if method == 'l2_norm':
        # L2èŒƒæ•°ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
        scores = np.linalg.norm(predictions - actuals, axis=1)
    elif method == 'l1_norm':
        # L1èŒƒæ•°ï¼ˆæ›¼å“ˆé¡¿è·ç¦»ï¼‰
        scores = np.sum(np.abs(predictions - actuals), axis=1)
    elif method == 'mse':
        # å‡æ–¹è¯¯å·®
        scores = np.mean((predictions - actuals) ** 2, axis=1)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¼‚å¸¸åˆ†æ•°è®¡ç®—æ–¹æ³•: {method}")

    return scores


def evaluate_model(predictions: np.ndarray, actuals: np.ndarray, true_labels: np.ndarray,
                  threshold: float, anomaly_scores: np.ndarray) -> Dict:
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½

    Args:
        predictions: é¢„æµ‹å€¼
        actuals: å®é™…å€¼
        true_labels: çœŸå®æ ‡ç­¾
        threshold: å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
        anomaly_scores: å¼‚å¸¸åˆ†æ•°

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator()

    # è¯„ä¼°é¢„æµ‹æ€§èƒ½
    metrics = evaluator.evaluate(
        predictions=predictions,
        actuals=actuals,
        true_labels=true_labels,
        anomaly_scores=anomaly_scores
    )

    # åŸºäºé˜ˆå€¼çš„å¼‚å¸¸æ£€æµ‹
    pred_anomalies = (anomaly_scores > threshold).astype(int)

    # è®¡ç®—å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

    detection_accuracy = accuracy_score(true_labels, pred_anomalies)
    detection_precision = precision_score(true_labels, pred_anomalies, zero_division=0)
    detection_recall = recall_score(true_labels, pred_anomalies, zero_division=0)
    detection_f1 = f1_score(true_labels, pred_anomalies, zero_division=0)

    # æ‰©å±•è¯„ä¼°ç»“æœ
    evaluation_results = {
        'prediction_metrics': metrics,
        'anomaly_detection': {
            'threshold': float(threshold),
            'accuracy': float(detection_accuracy),
            'precision': float(detection_precision),
            'recall': float(detection_recall),
            'f1_score': float(detection_f1),
            'predicted_anomalies': int(np.sum(pred_anomalies)),
            'true_anomalies': int(np.sum(true_labels))
        },
        'anomaly_scores': {
            'mean': float(np.mean(anomaly_scores)),
            'std': float(np.std(anomaly_scores)),
            'min': float(np.min(anomaly_scores)),
            'max': float(np.max(anomaly_scores)),
            'percentiles': {
                '50': float(np.percentile(anomaly_scores, 50)),
                '75': float(np.percentile(anomaly_scores, 75)),
                '90': float(np.percentile(anomaly_scores, 90)),
                '95': float(np.percentile(anomaly_scores, 95)),
                '99': float(np.percentile(anomaly_scores, 99))
            }
        }
    }

    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"  - é¢„æµ‹å‡†ç¡®ç‡: {metrics.get('accuracy', 0):.4f}")
    print(f"  - å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡: {detection_accuracy:.4f}")
    print(f"  - å¼‚å¸¸æ£€æµ‹ç²¾ç¡®ç‡: {detection_precision:.4f}")
    print(f"  - å¼‚å¸¸æ£€æµ‹å¬å›ç‡: {detection_recall:.4f}")
    print(f"  - å¼‚å¸¸æ£€æµ‹F1åˆ†æ•°: {detection_f1:.4f}")

    return evaluation_results


def save_evaluation_results(results: Dict, output_dir: str, model_name: str):
    """
    ä¿å­˜è¯„ä¼°ç»“æœ

    Args:
        results: è¯„ä¼°ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        model_name: æ¨¡å‹åç§°
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = output_dir / f"{model_name}_evaluation_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")

    # ä¿å­˜æ€§èƒ½æ‘˜è¦
    summary_path = output_dir / f"{model_name}_evaluation_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("LSTMå¼‚å¸¸æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n\n")

        # é¢„æµ‹æ€§èƒ½
        pred_metrics = results['prediction_metrics']
        f.write("é¢„æµ‹æ€§èƒ½æŒ‡æ ‡:\n")
        f.write(f"  æ ·æœ¬æ€»æ•°: {pred_metrics['total_samples']}\n")
        f.write(f"  å¼‚å¸¸æ ·æœ¬æ•°: {pred_metrics['n_anomalies']}\n")
        f.write(f"  å¼‚å¸¸æ¯”ä¾‹: {pred_metrics['anomaly_ratio']:.4f}\n")
        f.write(f"  å‡†ç¡®ç‡: {pred_metrics['accuracy']:.4f}\n")
        f.write(f"  ç²¾ç¡®ç‡: {pred_metrics['precision']:.4f}\n")
        f.write(f"  å¬å›ç‡: {pred_metrics['recall']:.4f}\n")
        f.write(f"  F1åˆ†æ•°: {pred_metrics['f1_score']:.4f}\n")
        if pred_metrics['auc']:
            f.write(f"  AUC: {pred_metrics['auc']:.4f}\n")
        f.write("\n")

        # å¼‚å¸¸æ£€æµ‹æ€§èƒ½
        det_metrics = results['anomaly_detection']
        f.write("å¼‚å¸¸æ£€æµ‹æ€§èƒ½æŒ‡æ ‡:\n")
        f.write(f"  æ£€æµ‹é˜ˆå€¼: {det_metrics['threshold']:.6f}\n")
        f.write(f"  å‡†ç¡®ç‡: {det_metrics['accuracy']:.4f}\n")
        f.write(f"  ç²¾ç¡®ç‡: {det_metrics['precision']:.4f}\n")
        f.write(f"  å¬å›ç‡: {det_metrics['recall']:.4f}\n")
        f.write(f"  F1åˆ†æ•°: {det_metrics['f1_score']:.4f}\n")
        f.write(f"  é¢„æµ‹å¼‚å¸¸æ•°: {det_metrics['predicted_anomalies']}\n")
        f.write(f"  çœŸå®å¼‚å¸¸æ•°: {det_metrics['true_anomalies']}\n")
        f.write("\n")

        # å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡
        score_stats = results['anomaly_scores']
        f.write("å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡:\n")
        f.write(f"  å‡å€¼: {score_stats['mean']:.6f}\n")
        f.write(f"  æ ‡å‡†å·®: {score_stats['std']:.6f}\n")
        f.write(f"  æœ€å°å€¼: {score_stats['min']:.6f}\n")
        f.write(f"  æœ€å¤§å€¼: {score_stats['max']:.6f}\n")
        f.write("  åˆ†ä½æ•°:\n")
        for p, v in score_stats['percentiles'].items():
            f.write(f"    {p}%: {v:.6f}\n")

    print(f"ğŸ’¾ è¯„ä¼°æ‘˜è¦å·²ä¿å­˜: {summary_path}")


def print_evaluation_summary(results: Dict):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    print("\n" + "=" * 70)
    print("ğŸ¯ LSTMå¼‚å¸¸æ£€æµ‹æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 70)

    # é¢„æµ‹æ€§èƒ½
    pred_metrics = results['prediction_metrics']
    print("\nğŸ“ˆ é¢„æµ‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ ·æœ¬æ€»æ•°: {pred_metrics['total_samples']}")
    print(f"  å¼‚å¸¸æ ·æœ¬æ•°: {pred_metrics['n_anomalies']} ({pred_metrics['anomaly_ratio']:.2f}%)")
    print(f"  å‡†ç¡®ç‡: {pred_metrics['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {pred_metrics['precision']:.4f}")
    print(f"  å¬å›ç‡: {pred_metrics['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {pred_metrics['f1_score']:.4f}")
    if pred_metrics['auc']:
        print(f"  AUC: {pred_metrics['auc']:.4f}")

    # å¼‚å¸¸æ£€æµ‹æ€§èƒ½
    det_metrics = results['anomaly_detection']
    print("\nğŸš¨ å¼‚å¸¸æ£€æµ‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ£€æµ‹é˜ˆå€¼: {det_metrics['threshold']:.6f}")
    print(f"  å‡†ç¡®ç‡: {det_metrics['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {det_metrics['precision']:.4f}")
    print(f"  å¬å›ç‡: {det_metrics['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {det_metrics['f1_score']:.4f}")
    print(f"  é¢„æµ‹å¼‚å¸¸æ•°: {det_metrics['predicted_anomalies']}")
    print(f"  çœŸå®å¼‚å¸¸æ•°: {det_metrics['true_anomalies']}")

    # å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡
    score_stats = results['anomaly_scores']
    print("\nğŸ“Š å¼‚å¸¸åˆ†æ•°ç»Ÿè®¡:")
    print(f"  å‡å€¼: {score_stats['mean']:.6f} Â± {score_stats['std']:.6f}")
    print(f"  èŒƒå›´: [{score_stats['min']:.6f}, {score_stats['max']:.6f}]")
    print("  åˆ†ä½æ•°:")
    for p, v in score_stats['percentiles'].items():
        print(f"    {p}%: {v:.6f}")

    print("\n" + "=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– LSTMå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - Server Machine Datasetè¯„ä¼°è„šæœ¬")
    print("=" * 70)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„LSTMæ¨¡å‹
    if args.model_path is None:
        lstm_dir = Path("models/anomaly_detection/lstm")
        if lstm_dir.exists():
            # æŸ¥æ‰¾æ‰€æœ‰.ckptæ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            ckpt_files = list(lstm_dir.glob("*.ckpt"))
            if ckpt_files:
                # é€‰æ‹©æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
                latest_model = max(ckpt_files, key=lambda x: x.stat().st_mtime)
                args.model_path = str(latest_model)
                print(f"ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ¨¡å‹: {args.model_path}")
            else:
                print("âŒ åœ¨ models/anomaly_detection/lstm/ ç›®å½•ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
                sys.exit(1)
        else:
            print("âŒ models/anomaly_detection/lstm/ ç›®å½•ä¸å­˜åœ¨")
            sys.exit(1)

    # è®¾ç½®è¾“å‡ºç›®å½• - ä¼˜åŒ–ç‰ˆæœ¬
    model_path = Path(args.model_path)
    model_name = model_path.stem

    # ä¼˜åŒ–è¾“å‡ºç›®å½•é€»è¾‘
    if args.output_dir is None:
        # é»˜è®¤åœ¨æ¨¡å‹ç›®å½•ä¸‹åˆ›å»ºevaluationså­ç›®å½•
        args.output_dir = str(model_path.parent / "evaluations")
    elif args.output_dir == "auto":
        # è‡ªåŠ¨åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¯„ä¼°ç›®å½•
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        eval_dir_name = f"evaluation_{model_name}_{timestamp}"
        args.output_dir = str(model_path.parent / "evaluations" / eval_dir_name)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“„ æ¨¡å‹åç§°: {model_name}")
    print(f"ğŸ“„ æ¨¡å‹è·¯å¾„: {model_path}")
    print()

    try:
        # è®¾ç½®MindSpore
        setup_mindspore_context(args.device_target, args.seed)

        # åŠ è½½æ¨¡å‹å’Œç»„ä»¶
        model, threshold_calculator, processor = load_model_and_components(
            args.model_path, args.threshold_path, args.scaler_path
        )

        # åŠ è½½å’Œé¢„å¤„ç†æµ‹è¯•æ•°æ®
        test_data, true_labels = load_and_preprocess_test_data(
            args.test_data_path, processor, args.timestamp_column, args.label_column
        )

        # æ‰§è¡Œæ¨ç†
        predictions = perform_inference(model, test_data, args.batch_size)

        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
        anomaly_scores = calculate_anomaly_scores(
            predictions, test_data.targets, method='l2_norm'
        )

        # è¯„ä¼°æ¨¡å‹æ€§èƒ½
        evaluation_results = evaluate_model(
            predictions, test_data.targets, true_labels,
            threshold_calculator.threshold, anomaly_scores
        )

        # ä¿å­˜è¯„ä¼°ç»“æœ
        save_evaluation_results(evaluation_results, args.output_dir, model_name)

        # æ‰“å°è¯„ä¼°æ‘˜è¦
        print_evaluation_summary(evaluation_results)

        print("\nğŸ‰ è¯„ä¼°æµç¨‹å®Œæˆï¼")
        print("=" * 70)
        print("ğŸ“Š è¯„ä¼°æ€»ç»“:")
        print(f"  - æµ‹è¯•æ•°æ®: {args.test_data_path}")
        print(f"  - æ¨¡å‹æ–‡ä»¶: {args.model_path}")
        print(f"  - è¯„ä¼°ç»“æœç›®å½•: {output_dir}")
        print(f"  - è¯¦ç»†ç»“æœ: {output_dir}/{model_name}_evaluation_results.json")
        print(f"  - æ€§èƒ½æ‘˜è¦: {output_dir}/{model_name}_evaluation_summary.txt")
        print("\nğŸš€ è¯„ä¼°å®Œæˆï¼Œå¯ä»¥æŸ¥çœ‹è¯¦ç»†ç»“æœï¼")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()