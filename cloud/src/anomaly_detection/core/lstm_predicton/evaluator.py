"""
LSTMé¢„æµ‹å¼‚å¸¸æ£€æµ‹æ¨¡å— - è¯„ä¼°å™¨
è´Ÿè´£æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œç»“æœå¯è§†åŒ–
"""

import numpy as np
from typing import Optional, Dict, Any, Union
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt


class Evaluator:
    """
    æ¨¡å‹è¯„ä¼°å™¨

    ä¸“é—¨ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š
    - è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
    - ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    - å¯è§†åŒ–è¯„ä¼°ç»“æœ
    """

    def __init__(self):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        """
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.evaluation_results = {
            'predictions': [],
            'actuals': [],
            'true_labels': []
        }

        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {}

        print(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")

    def evaluate(self, predictions: np.ndarray, actuals: np.ndarray,
                 true_labels: np.ndarray, anomaly_scores: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            predictions: æ¨¡å‹é¢„æµ‹å€¼
            actuals: å®é™…å€¼
            true_labels: çœŸå®å¼‚å¸¸æ ‡ç­¾
            anomaly_scores: å¼‚å¸¸åˆ†æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºAUCè®¡ç®—ï¼‰

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        # å­˜å‚¨è¯„ä¼°ç»“æœ
        self.evaluation_results['predictions'].extend(predictions.tolist())
        self.evaluation_results['actuals'].extend(actuals.tolist())
        self.evaluation_results['true_labels'].extend(true_labels.tolist())

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        self.metrics = self._compute_metrics(true_labels, anomaly_scores)

        return self.metrics.copy()

    def _compute_metrics(self, true_labels: np.ndarray,
                        anomaly_scores: Optional[np.ndarray] = None) -> Dict[str, Any]:
        """
        è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡

        Args:
            true_labels: çœŸå®å¼‚å¸¸æ ‡ç­¾
            anomaly_scores: å¼‚å¸¸åˆ†æ•°

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(true_labels)
        n_anomalies = np.sum(true_labels)
        anomaly_ratio = n_anomalies / total_samples if total_samples > 0 else 0

        # å¦‚æœæ²¡æœ‰å¼‚å¸¸åˆ†æ•°ï¼Œå‡è®¾é¢„æµ‹æ ‡ç­¾å°±æ˜¯çœŸå®æ ‡ç­¾ï¼ˆç”¨äºåŸºæœ¬è¯„ä¼°ï¼‰
        if anomaly_scores is None:
            pred_labels = true_labels  # è‡ªè¯„ä¼°æ¨¡å¼
            auc = None
        else:
            # ä½¿ç”¨å¼‚å¸¸åˆ†æ•°ä½œä¸ºé¢„æµ‹æ ‡ç­¾ï¼ˆ>0.5ä¸ºå¼‚å¸¸ï¼‰
            pred_labels = (anomaly_scores > 0.5).astype(int)

            # è®¡ç®—AUC
            auc = None
            try:
                if len(np.unique(true_labels)) > 1:
                    auc = roc_auc_score(true_labels, anomaly_scores)
            except:
                pass

        # åŸºç¡€åˆ†ç±»æŒ‡æ ‡
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels, pred_labels, average='binary', zero_division=0
        )

        # æ··æ·†çŸ©é˜µ
        tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()

        # å¼‚å¸¸æ£€æµ‹ç‰¹æœ‰æŒ‡æ ‡
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0

        # å·¥ä¸šåº”ç”¨æŒ‡æ ‡
        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0

        metrics = {
            'total_samples': int(total_samples),
            'n_anomalies': int(n_anomalies),
            'anomaly_ratio': float(anomaly_ratio),
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'specificity': float(specificity),
            'sensitivity': float(sensitivity),
            'false_positive_rate': float(false_positive_rate),
            'false_negative_rate': float(false_negative_rate),
            'auc': float(auc) if auc is not None else None,
            'confusion_matrix': {
                'tp': int(tp), 'fp': int(fp),
                'tn': int(tn), 'fn': int(fn)
            }
        }

        return metrics

    def plot_evaluation_results(self, save_path: Optional[str] = None,
                               show_plot: bool = True):
        """
        å¯è§†åŒ–è¯„ä¼°ç»“æœ

        Args:
            save_path: ä¿å­˜è·¯å¾„
            show_plot: æ˜¯å¦æ˜¾ç¤ºå›¾åƒ
        """
        if not self.evaluation_results['predictions']:
            print("âš ï¸ æ²¡æœ‰è¯„ä¼°ç»“æœå¯ä¾›å¯è§†åŒ–")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        predictions = np.array(self.evaluation_results['predictions'])
        actuals = np.array(self.evaluation_results['actuals'])
        true_labels = np.array(self.evaluation_results['true_labels'])

        # è®¡ç®—é¢„æµ‹è¯¯å·®
        errors = np.abs(predictions - actuals)
        if errors.ndim > 1:
            errors = np.mean(errors, axis=1)  # å¤šç‰¹å¾å–å¹³å‡

        axes[0, 0].hist(errors[true_labels == 0], alpha=0.7, label='æ­£å¸¸', bins=50, color='blue')
        axes[0, 0].hist(errors[true_labels == 1], alpha=0.7, label='å¼‚å¸¸', bins=50, color='red')
        axes[0, 0].set_xlabel('é¢„æµ‹è¯¯å·®')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. é¢„æµ‹å€¼vså®é™…å€¼æ•£ç‚¹å›¾
        if predictions.ndim > 1:
            pred_flat = predictions.flatten()
            actual_flat = actuals.flatten()
        else:
            pred_flat = predictions
            actual_flat = actuals

        axes[0, 1].scatter(actual_flat[true_labels == 0], pred_flat[true_labels == 0],
                          alpha=0.6, label='æ­£å¸¸', color='blue', s=20)
        axes[0, 1].scatter(actual_flat[true_labels == 1], pred_flat[true_labels == 1],
                          alpha=0.6, label='å¼‚å¸¸', color='red', s=20)
        # æ·»åŠ å¯¹è§’çº¿
        min_val = min(np.min(actual_flat), np.min(pred_flat))
        max_val = max(np.max(actual_flat), np.max(pred_flat))
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, label='ç†æƒ³é¢„æµ‹')
        axes[0, 1].set_xlabel('å®é™…å€¼')
        axes[0, 1].set_ylabel('é¢„æµ‹å€¼')
        axes[0, 1].set_title('é¢„æµ‹å€¼ vs å®é™…å€¼')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. æ®‹å·®åˆ†æï¼ˆå¦‚æœæœ‰å¼‚å¸¸åˆ†æ•°ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæš‚æ—¶ç”¨é¢„æµ‹è¯¯å·®ä½œä¸ºæ®‹å·®
        axes[1, 0].scatter(range(len(errors)), errors, alpha=0.6,
                          c=true_labels, cmap='coolwarm', s=20)
        axes[1, 0].set_xlabel('æ ·æœ¬ç´¢å¼•')
        axes[1, 0].set_ylabel('æ®‹å·®')
        axes[1, 0].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[1, 0].grid(True, alpha=0.3)

        # 4. æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
        axes[1, 1].axis('off')
        metrics_text = f"""
        æ€§èƒ½æŒ‡æ ‡æ‘˜è¦:

        æ ·æœ¬æ€»æ•°: {self.metrics.get('total_samples', 0)}
        å¼‚å¸¸æ ·æœ¬æ•°: {self.metrics.get('n_anomalies', 0)}
        å¼‚å¸¸æ¯”ä¾‹: {self.metrics.get('anomaly_ratio', 0):.3f}

        å‡†ç¡®ç‡: {self.metrics.get('accuracy', 0):.3f}
        ç²¾ç¡®ç‡: {self.metrics.get('precision', 0):.3f}
        å¬å›ç‡: {self.metrics.get('recall', 0):.3f}
        F1åˆ†æ•°: {self.metrics.get('f1_score', 0):.3f}
        AUC: {self.metrics.get('auc', 'N/A')}

        ç‰¹å¼‚æ€§: {self.metrics.get('specificity', 0):.3f}
        çµæ•åº¦: {self.metrics.get('sensitivity', 0):.3f}
        å‡æ­£ç‡: {self.metrics.get('false_positive_rate', 0):.3f}
        å‡è´Ÿç‡: {self.metrics.get('false_negative_rate', 0):.3f}
        """

        axes[1, 1].text(0.1, 0.95, metrics_text, transform=axes[1, 1].transAxes,
                       fontsize=10, verticalalignment='top', fontfamily='monospace')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ è¯„ä¼°ç»“æœå›¾è¡¨å·²ä¿å­˜: {save_path}")

        if show_plot:
            plt.show()
        else:
            plt.close()

    def get_evaluation_report(self) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š

        Returns:
            è¯„ä¼°æŠ¥å‘Šå­—å…¸
        """
        report = {
            'metrics': self.metrics.copy(),
            'evaluation_summary': {
                'total_samples': len(self.evaluation_results['predictions']),
                'total_anomalies': sum(self.evaluation_results['true_labels'])
            }
        }
        return report

    def reset_evaluation(self):
        """
        é‡ç½®è¯„ä¼°ç»“æœ
        """
        self.evaluation_results = {
            'predictions': [],
            'actuals': [],
            'true_labels': []
        }
        self.metrics = {}


def get_default_evaluator_config(evaluator_type: str = 'evaluator') -> Dict[str, Any]:
    """
    è·å–é»˜è®¤è¯„ä¼°å™¨é…ç½®

    Args:
        evaluator_type: è¯„ä¼°å™¨ç±»å‹

    Returns:
        é»˜è®¤é…ç½®å­—å…¸
    """
    base_config = {
        'evaluator_type': evaluator_type
    }

    return base_config


def create_evaluator_from_config(config: Dict[str, Any]) -> Evaluator:
    """
    ä»é…ç½®åˆ›å»ºè¯„ä¼°å™¨

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        è¯„ä¼°å™¨å®ä¾‹
    """
    evaluator_type = config.get('evaluator_type', 'evaluator')

    if evaluator_type == 'evaluator':
        return Evaluator()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°å™¨ç±»å‹: {evaluator_type}. æ”¯æŒçš„ç±»å‹: ['evaluator']")