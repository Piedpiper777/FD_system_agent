"""
LSTMé¢„æµ‹å¼‚å¸¸æ£€æµ‹æ¨¡å— - æ•°æ®å¤„ç†å™¨
è´Ÿè´£æ•°æ®çš„åŠ è½½ã€æ¸…æ´—ã€é¢„å¤„ç†å’Œæ—¶åºçª—å£åˆ’åˆ†
"""

import numpy as np
import pandas as pd
from typing import Optional, Tuple, List, Dict, Any, Union
from pathlib import Path
from sklearn.preprocessing import StandardScaler


class TimeSeriesData:
    """æ—¶åºæ•°æ®å®¹å™¨"""
    def __init__(self, sequences: np.ndarray, targets: np.ndarray,
                 timestamps: Optional[np.ndarray] = None):
        self.sequences = sequences  # (n_samples, seq_len, n_features)
        self.targets = targets      # (n_samples, n_features)
        self.timestamps = timestamps


class DataProcessor:
    """
    æ—¶åºæ•°æ®å¤„ç†å™¨

    ä¸“é—¨é’ˆå¯¹å·¥ä¸šå¼‚å¸¸æ£€æµ‹ä»»åŠ¡è®¾è®¡ï¼š
    - æ•°æ®åŠ è½½å’Œæ¸…æ´—
    - æ ‡å‡†åŒ–é¢„å¤„ç†
    - æ»‘åŠ¨çª—å£åºåˆ—åˆ›å»º
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†
    """

    def __init__(self, sequence_length: int = 50, prediction_horizon: int = 1,
                 stride: int = 1, normalize: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            sequence_length: åºåˆ—é•¿åº¦ï¼ˆå†å²çª—å£å¤§å°ï¼‰
            prediction_horizon: é¢„æµ‹æ­¥é•¿
            stride: æ»‘åŠ¨çª—å£æ­¥é•¿
            normalize: æ˜¯å¦æ ‡å‡†åŒ–
        """
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.stride = stride
        self.normalize = normalize

        # é¢„å¤„ç†ç»„ä»¶
        self.scaler = StandardScaler() if normalize else None
        self.feature_names: List[str] = []
        self.target_names: List[str] = []

        # å¤„ç†åçš„æ•°æ®
        self.train_data: Optional[np.ndarray] = None
        self.val_data: Optional[np.ndarray] = None
        self.test_data: Optional[np.ndarray] = None

        print(f"âœ… æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  - åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"  - é¢„æµ‹æ­¥é•¿: {prediction_horizon}")
        print(f"  - æ ‡å‡†åŒ–: {'å¯ç”¨' if normalize else 'ç¦ç”¨'}")

    def load_data(self, file_path: Union[str, Path],
                  feature_columns: Optional[List[str]] = None,
                  target_columns: Optional[List[str]] = None,
                  timestamp_column: Optional[str] = None,
                  label_column: Optional[str] = None) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®

        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            target_columns: ç›®æ ‡åˆ—ååˆ—è¡¨
            timestamp_column: æ—¶é—´æˆ³åˆ—å
            label_column: æ ‡ç­¾åˆ—åï¼ˆç”¨äºæµ‹è¯•æ•°æ®ï¼‰

        Returns:
            åŠ è½½çš„DataFrame
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # åŠ è½½æ•°æ®
        if file_path.suffix.lower() == '.csv':
            data = pd.read_csv(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")

        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: {file_path}")
        print(f"  - æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"  - åˆ—å: {list(data.columns)}")

        # è‡ªåŠ¨è¯†åˆ«æ—¶é—´æˆ³åˆ—
        if timestamp_column is None:
            timestamp_candidates = ['timestamp', 'time', 'date', 'datetime']
            for col in data.columns:
                if col.lower() in timestamp_candidates or 'time' in col.lower():
                    timestamp_column = col
                    break
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¶é—´æˆ³
            if timestamp_column is None and len(data.columns) > 0:
                timestamp_column = data.columns[0]

        # å­˜å‚¨æ—¶é—´æˆ³åˆ—å
        self.timestamp_column = timestamp_column

        # è®¾ç½®ç‰¹å¾å’Œç›®æ ‡åˆ—
        if feature_columns is None:
            # è‡ªåŠ¨è¯†åˆ«ç‰¹å¾åˆ—ï¼šæ’é™¤æ—¶é—´æˆ³åˆ—å’Œæ ‡ç­¾åˆ—
            exclude_cols = set()
            if timestamp_column and timestamp_column in data.columns:
                exclude_cols.add(timestamp_column)
            if label_column and label_column in data.columns:
                exclude_cols.add(label_column)

            # é€‰æ‹©æ•°å€¼åˆ—ä½œä¸ºç‰¹å¾ï¼Œæ’é™¤æŒ‡å®šçš„åˆ—
            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
            self.feature_names = [col for col in numeric_cols if col not in exclude_cols]

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å€¼ç‰¹å¾åˆ—ï¼Œå°è¯•ä½¿ç”¨æ‰€æœ‰éæ’é™¤åˆ—
            if not self.feature_names:
                all_cols = [col for col in data.columns if col not in exclude_cols]
                self.feature_names = all_cols
                print(f"âš ï¸ æœªæ‰¾åˆ°æ•°å€¼ç‰¹å¾åˆ—ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨åˆ—: {self.feature_names}")
        else:
            self.feature_names = feature_columns

        if target_columns is None:
            self.target_names = self.feature_names
        else:
            self.target_names = target_columns

        # å­˜å‚¨æ ‡ç­¾åˆ—ï¼ˆå¦‚æœæœ‰ï¼‰
        self.label_column = label_column

        print(f"ğŸ” åˆ—è¯†åˆ«å®Œæˆ:")
        print(f"  - æ—¶é—´æˆ³åˆ—: {timestamp_column}")
        print(f"  - ç‰¹å¾åˆ—: {self.feature_names}")
        if label_column:
            print(f"  - æ ‡ç­¾åˆ—: {label_column}")

        return data

    def preprocess_data(self, raw_data: pd.DataFrame) -> np.ndarray:
        """
        æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†

        Args:
            raw_data: åŸå§‹æ•°æ®

        Returns:
            å¤„ç†åçš„æ•°å€¼æ•°ç»„
        """
        # é€‰æ‹©ç‰¹å¾åˆ—
        if not all(col in raw_data.columns for col in self.feature_names):
            missing_cols = [col for col in self.feature_names if col not in raw_data.columns]
            raise ValueError(f"æ•°æ®ä¸­ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")

        data = raw_data[self.feature_names].copy()

        # å¤„ç†ç¼ºå¤±å€¼
        if data.isnull().any().any():
            print(f"âš ï¸ å‘ç°ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‰å‘å¡«å……å¤„ç†")
            data = data.fillna(method='ffill').fillna(method='bfill')

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        processed_data = data.values.astype(np.float32)

        # æ ‡å‡†åŒ–
        if self.normalize and self.scaler is not None:
            processed_data = self.scaler.fit_transform(processed_data)

        print(f"ğŸ”§ æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"  - ç‰¹å¾åˆ—: {self.feature_names}")
        print(f"  - æ•°æ®å½¢çŠ¶: {processed_data.shape}")
        if self.normalize:
            print(f"  - æ ‡å‡†åŒ–å‚æ•°: mean={self.scaler.mean_}, std={self.scaler.scale_}")

        return processed_data

    def create_sequences(self, data: np.ndarray) -> TimeSeriesData:
        """
        åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—

        Args:
            data: è¾“å…¥æ•°æ® (n_samples, n_features)

        Returns:
            TimeSeriesDataå¯¹è±¡
        """
        n_samples, n_features = data.shape
        sequences = []
        targets = []

        for i in range(0, n_samples - self.sequence_length - self.prediction_horizon + 1, self.stride):
            # è¾“å…¥åºåˆ—
            seq_end = i + self.sequence_length
            sequence = data[i:seq_end]  # (sequence_length, n_features)

            # ç›®æ ‡å€¼
            target_idx = seq_end + self.prediction_horizon - 1
            target = data[target_idx]  # (n_features,)

            sequences.append(sequence)
            targets.append(target)

        sequences = np.array(sequences)  # (n_sequences, seq_len, n_features)
        targets = np.array(targets)      # (n_sequences, n_features)

        print(f"ğŸ”„ åºåˆ—åˆ›å»ºå®Œæˆ")
        print(f"  - åŸå§‹æ ·æœ¬æ•°: {n_samples}")
        print(f"  - ç”Ÿæˆåºåˆ—æ•°: {len(sequences)}")
        print(f"  - åºåˆ—å½¢çŠ¶: {sequences.shape}")

        return TimeSeriesData(sequences, targets)

    def load_test_data(self, file_path: Union[str, Path],
                      feature_columns: Optional[List[str]] = None,
                      label_column: Optional[str] = None) -> Tuple[pd.DataFrame, Optional[np.ndarray]]:
        """
        åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰

        Args:
            file_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            label_column: æ ‡ç­¾åˆ—å

        Returns:
            (æµ‹è¯•æ•°æ®DataFrame, æ ‡ç­¾æ•°ç»„)
        """
        # åŠ è½½æ•°æ®
        test_data = self.load_data(file_path, feature_columns=feature_columns, label_column=label_column)

        # æå–æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        labels = None
        if label_column and label_column in test_data.columns:
            labels = test_data[label_column].values
            print(f"ğŸ·ï¸ æå–æ ‡ç­¾å®Œæˆ: {label_column}, å½¢çŠ¶: {labels.shape}")
            print(f"  - æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(labels):.3f}")

        return test_data, labels

    def process_test_pipeline(self, file_path: Union[str, Path],
                             feature_columns: Optional[List[str]] = None,
                             label_column: Optional[str] = None) -> Tuple[TimeSeriesData, Optional[np.ndarray]]:
        """
        æµ‹è¯•æ•°æ®å¤„ç†æµæ°´çº¿

        Args:
            file_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            label_column: æ ‡ç­¾åˆ—å

        Returns:
            (æµ‹è¯•åºåˆ—æ•°æ®, æ ‡ç­¾æ•°ç»„)
        """
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        test_data, labels = self.load_test_data(file_path, feature_columns, label_column)

        # 2. é¢„å¤„ç†ï¼ˆæ³¨æ„ï¼šæµ‹è¯•æ•°æ®åº”è¯¥ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ ‡å‡†åŒ–å‚æ•°ï¼‰
        processed_data = self.preprocess_data(test_data)

        # 3. åˆ›å»ºåºåˆ—
        test_sequences = self.create_sequences(processed_data)

        return test_sequences, labels

    def split_dataset(self, data: np.ndarray, train_ratio: float = 0.8) -> Tuple[np.ndarray, np.ndarray]:
        """
        æŒ‰æ—¶åºåˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†

        Args:
            data: è¾“å…¥æ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹

        Returns:
            (train_data, val_data)
        """
        n_samples = len(data)
        train_end = int(n_samples * train_ratio)

        # ğŸ”§ ç¡®ä¿åˆ†å‰²åçš„æ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œè‡³å°‘èƒ½å½¢æˆä¸€ä¸ªbatch
        min_samples = 32  # æœ€å°batch_size
        if train_end < min_samples:
            train_end = min_samples
        if n_samples - train_end < min_samples:
            train_end = n_samples - min_samples

        train_data = data[:train_end]
        val_data = data[train_end:]

        print(f"âœ‚ï¸ æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
        print(f"  - æ€»æ ·æœ¬æ•°: {n_samples}")
        print(f"  - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"  - éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        print(f"  - train_ratio: {train_ratio}")

        return train_data, val_data

    def process_pipeline(self, file_path: Union[str, Path],
                        train_ratio: float = 0.8,
                        feature_columns: Optional[List[str]] = None,
                        label_column: Optional[str] = None) -> Tuple[TimeSeriesData, TimeSeriesData]:
        """
        å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿

        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            label_column: æ ‡ç­¾åˆ—åï¼ˆç”¨äºæµ‹è¯•æ•°æ®ï¼‰

        Returns:
            (train_data, val_data) TimeSeriesDataå¯¹è±¡
        """
        # 1. åŠ è½½æ•°æ®
        raw_data = self.load_data(file_path, feature_columns=feature_columns, label_column=label_column)

        # 2. é¢„å¤„ç†
        processed_data = self.preprocess_data(raw_data)

        # 3. åˆ›å»ºåºåˆ—
        all_sequences = self.create_sequences(processed_data)

        # 4. åˆ’åˆ†æ•°æ®é›†
        train_sequences, val_sequences = self.split_dataset(
            all_sequences.sequences, train_ratio
        )
        train_targets, val_targets = self.split_dataset(
            all_sequences.targets, train_ratio
        )

        # 5. åˆ›å»ºTimeSeriesDataå¯¹è±¡
        train_data = TimeSeriesData(train_sequences, train_targets)
        val_data = TimeSeriesData(val_sequences, val_targets)

        return train_data, val_data

    def inverse_transform(self, data: np.ndarray) -> np.ndarray:
        """
        åæ ‡å‡†åŒ–æ•°æ®

        Args:
            data: æ ‡å‡†åŒ–åçš„æ•°æ®

        Returns:
            åŸå§‹å°ºåº¦çš„æ•°æ®
        """
        if self.scaler is None:
            return data
        return self.scaler.inverse_transform(data)

    def get_scaler_params(self) -> Dict[str, np.ndarray]:
        """
        è·å–æ ‡å‡†åŒ–å‚æ•°ï¼ˆç”¨äºåœ¨çº¿æ¨ç†ï¼‰

        Returns:
            æ ‡å‡†åŒ–å‚æ•°å­—å…¸
        """
        if self.scaler is None:
            return {}
        return {
            'mean': self.scaler.mean_,
            'scale': self.scaler.scale_,
            'feature_names': self.feature_names,
            'target_names': self.target_names
        }

    def save_scaler_params(self, file_path: str):
        """ä¿å­˜æ ‡å‡†åŒ–å‚æ•°"""
        params = self.get_scaler_params()
        np.savez(file_path, **params)
        print(f"ğŸ’¾ æ ‡å‡†åŒ–å‚æ•°å·²ä¿å­˜: {file_path}")

    @classmethod
    def load_scaler_params(cls, file_path: str) -> Dict[str, np.ndarray]:
        """åŠ è½½æ ‡å‡†åŒ–å‚æ•°"""
        data = np.load(file_path, allow_pickle=True)
        params = {key: data[key] for key in data.files}
        print(f"ğŸ“‚ æ ‡å‡†åŒ–å‚æ•°å·²åŠ è½½: {file_path}")
        return params