"""
LSTMé¢„æµ‹å¼‚å¸¸æ£€æµ‹æ¨¡å— - è®­ç»ƒå™¨
è´Ÿè´£æ¨¡å‹è®­ç»ƒã€è¶…å‚æ•°è°ƒä¼˜å’Œè®­ç»ƒè¿‡ç¨‹ç›‘æ§
"""

import mindspore as ms
import mindspore.ops as ops
from mindspore.nn import MSELoss, Adam
import mindspore.numpy as np
from typing import Optional, Any, Dict, Union
from pathlib import Path


class Trainer:
    """
    LSTMæ¨¡å‹è®­ç»ƒå™¨

    ä¸“æ³¨äºæ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ‰§è¡Œè®­ç»ƒå¾ªç¯
    - è®°å½•è®­ç»ƒæŒ‡æ ‡
    - æ”¯æŒæ—©åœæœºåˆ¶
    - æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
    """

    def __init__(self, model: ms.nn.Cell, learning_rate: float = 0.001,
                 weight_decay: float = 1e-4, clip_grad_norm: float = 5.0):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            clip_grad_norm: æ¢¯åº¦è£å‰ªèŒƒæ•°
        """
        self.model = model
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.clip_grad_norm = clip_grad_norm

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = Adam(
            params=self.model.trainable_params(),
            learning_rate=learning_rate,
            weight_decay=weight_decay
        )

        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_metrics = {
            'train_losses': [],
            'val_losses': [],
            'epochs_trained': 0
        }

        # æŸå¤±å‡½æ•°
        self.criterion = MSELoss()

        # æ¢¯åº¦å‡½æ•°
        self.grad_fn = ms.value_and_grad(
            self.forward_fn, None, self.optimizer.parameters, has_aux=False
        )

        print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  - å­¦ä¹ ç‡: {learning_rate}")
        print(f"  - æƒé‡è¡°å‡: {weight_decay}")

    def process_gradients(self, grads):
        """å¤„ç†æ¢¯åº¦ï¼ˆè£å‰ªç­‰ï¼‰"""
        if self.clip_grad_norm > 0:
            grads = ops.clip_by_global_norm(grads, clip_norm=self.clip_grad_norm)
        return grads

    def forward_fn(self, input_seq, target):
        """
        å‰å‘ä¼ æ’­å‡½æ•°

        Args:
            input_seq: è¾“å…¥åºåˆ—
            target: ç›®æ ‡å€¼

        Returns:
            æŸå¤±å€¼
        """
        prediction = self.model(input_seq)
        loss = self.criterion(prediction, target)
        return loss

    def train_step(self, batch_data) -> float:
        """
        å•æ­¥è®­ç»ƒ

        Args:
            batch_data: (input_seq, target)

        Returns:
            æŸå¤±å€¼
        """
        input_seq, target = batch_data

        # å‰å‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—
        loss, grads = self.grad_fn(input_seq, target)

        # æ¢¯åº¦å¤„ç†å’Œå‚æ•°æ›´æ–°
        grads = self.process_gradients(grads)
        self.optimizer(grads)

        return float(loss)

    def compute_loss(self, batch_data) -> float:
        """
        è®¡ç®—æ‰¹æ¬¡æŸå¤±

        Args:
            batch_data: (input_seq, target)

        Returns:
            æŸå¤±å€¼
        """
        input_seq, target = batch_data
        prediction = self.model(input_seq)
        loss = self.criterion(prediction, target)
        return float(loss)

    def train_epoch(self, train_loader, epoch_idx: Optional[int] = None) -> float:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch_idx: epochç´¢å¼•

        Returns:
            å¹³å‡è®­ç»ƒæŸå¤±
        """
        self.model.set_train(True)
        total_loss = 0.0
        batch_count = 0

        for batch_data in train_loader:
            loss = self.train_step(batch_data)
            total_loss += loss
            batch_count += 1

        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0
        self.training_metrics['train_losses'].append(avg_loss)

        # æ³¨æ„ï¼šæ—¥å¿—è¾“å‡ºç”±è°ƒç”¨æ–¹ï¼ˆapi.pyï¼‰ç»Ÿä¸€ç®¡ç†ï¼Œè¿™é‡Œä¸è¾“å‡ºæ—¥å¿—
        # å¦‚æœéœ€è¦è°ƒè¯•ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
        # if epoch_idx is not None:
        #     print(f"Epoch [{epoch_idx+1}] Train Loss: {avg_loss:.6f}")

        return avg_loss

    def validate(self, val_loader) -> float:
        """
        éªŒè¯æ¨¡å‹

        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨

        Returns:
            å¹³å‡éªŒè¯æŸå¤±
        """
        self.model.set_train(False)
        total_loss = 0.0
        batch_count = 0

        for batch_data in val_loader:
            loss = self.compute_loss(batch_data)
            total_loss += loss
            batch_count += 1

        avg_loss = total_loss / batch_count if batch_count > 0 else 0.0
        return avg_loss

    def check_early_stopping(self, val_loss: float, patience: int) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ

        Args:
            val_loss: å½“å‰éªŒè¯æŸå¤±
            patience: è€å¿ƒå€¼

        Returns:
            æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= patience

    def train(self, train_loader, num_epochs: int = 50, val_loader=None,
              patience: Optional[int] = None) -> ms.nn.Cell:
        """
        è®­ç»ƒä¸»å¾ªç¯

        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            patience: æ—©åœè€å¿ƒå€¼

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        print(f"ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")

        for epoch in range(num_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self.train_epoch(train_loader, epoch)

            # éªŒè¯
            if val_loader is not None:
                val_loss = self.validate(val_loader)
                self.training_metrics['val_losses'].append(val_loss)

                print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

                # æ—©åœæ£€æŸ¥
                if patience is not None and self.check_early_stopping(val_loss, patience):
                    print(f"â¹ï¸ æ—©åœäºç¬¬ {epoch+1} è½®")
                    break
            else:
                print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}")

        self.training_metrics['epochs_trained'] = epoch + 1
        print("âœ… è®­ç»ƒå®Œæˆ!")

        # æ‰“å°è®­ç»ƒæ€»ç»“
        self._print_training_summary()

        return self.model

    def _print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        print("\nğŸ“Š è®­ç»ƒæ€»ç»“:")
        print(f"  - è®­ç»ƒè½®æ•°: {self.training_metrics['epochs_trained']}")
        print(f"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {self.training_metrics['train_losses'][-1]:.6f}")
        if self.training_metrics['val_losses']:
            print(f"  - æœ€ç»ˆéªŒè¯æŸå¤±: {self.training_metrics['val_losses'][-1]:.6f}")

    def save_model(self, save_path: Union[str, Path]):
        """
        ä¿å­˜æ¨¡å‹æƒé‡

        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        ms.save_checkpoint(self.model, str(save_path))
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")

    def load_model(self, load_path: Union[str, Path]) -> ms.nn.Cell:
        """
        åŠ è½½æ¨¡å‹æƒé‡

        Args:
            load_path: åŠ è½½è·¯å¾„

        Returns:
            åŠ è½½çš„æ¨¡å‹
        """
        load_path = Path(load_path)
        if not load_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")

        param_dict = ms.load_checkpoint(str(load_path))
        ms.load_param_into_net(self.model, param_dict)
        print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {load_path}")

        return self.model

    def get_training_metrics(self) -> Dict[str, Any]:
        """
        è·å–è®­ç»ƒæŒ‡æ ‡

        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        return self.training_metrics.copy()

    def reset_metrics(self):
        """é‡ç½®è®­ç»ƒæŒ‡æ ‡"""
        self.training_metrics = {
            'train_losses': [],
            'val_losses': [],
            'epochs_trained': 0
        }
        self.best_val_loss = float('inf')
        self.patience_counter = 0