"""
å¼‚å¸¸æ£€æµ‹APIæ¨¡å—
å¤„ç†å¼‚å¸¸æ£€æµ‹ç›¸å…³çš„è®­ç»ƒå’Œæ¨ç†è¯·æ±‚
"""

from flask import Blueprint, request, jsonify, send_file
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import threading
import sys
import os

# åˆ›å»ºå¼‚å¸¸æ£€æµ‹Blueprint
anomaly_detection_bp = Blueprint('anomaly_detection', __name__, url_prefix='/api/anomaly_detection')

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥è®­ç»ƒæ¨¡å—
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥çœŸå®çš„è®­ç»ƒç»„ä»¶
try:
    # LSTM Predictoræ¨¡å—ï¼ˆåŸæœ‰ï¼‰
    from anomaly_detection.core.lstm_predicton.data_processor import DataProcessor as LSTMPredictorDataProcessor
    from anomaly_detection.core.lstm_predicton.model_builder import ModelBuilder as LSTMPredictorModelBuilder
    from anomaly_detection.core.lstm_predicton.trainer import Trainer as LSTMPredictorTrainer
    from anomaly_detection.core.lstm_predicton.threshold_calculator import ThresholdCalculator as LSTMPredictorThresholdCalculator
    
    # LSTM Autoencoderæ¨¡å—ï¼ˆæ–°å¢ï¼‰
    from anomaly_detection.core.lstm_autoencoder.data_processor import DataProcessor as LSTMAutoencoderDataProcessor
    from anomaly_detection.core.lstm_autoencoder.model_builder import ModelBuilder as LSTMAutoencoderModelBuilder
    from anomaly_detection.core.lstm_autoencoder.trainer import Trainer as LSTMAutoencoderTrainer
    from anomaly_detection.core.lstm_autoencoder.threshold_calculator import ThresholdCalculator as LSTMAutoencoderThresholdCalculator
    
    # 1D CNN Autoencoderæ¨¡å—ï¼ˆæ–°å¢ï¼‰
    from anomaly_detection.core.cnn_1d_autoencoder.data_processor import DataProcessor as CNN1DAutoencoderDataProcessor
    from anomaly_detection.core.cnn_1d_autoencoder.model_builder import ModelBuilder as CNN1DAutoencoderModelBuilder
    from anomaly_detection.core.cnn_1d_autoencoder.trainer import Trainer as CNN1DAutoencoderTrainer
    from anomaly_detection.core.cnn_1d_autoencoder.threshold_calculator import ThresholdCalculator as CNN1DAutoencoderThresholdCalculator
    
    import torch
    training_available = True
    logger = logging.getLogger(__name__)
    logger.info("Real training modules loaded successfully (LSTM Predictor + LSTM Autoencoder + 1D CNN Autoencoder) [PyTorch]")
except ImportError as e:
    training_available = False
    logger = logging.getLogger(__name__)
    logger.warning(f"Training modules not available: {e}")

# å¯¼å…¥ä»»åŠ¡ç®¡ç†å™¨
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥
    from ..common.task_manager import get_task_manager, TrainingTask, TrainingStatus
except ImportError:
    # ç›¸å¯¹å¯¼å…¥å¤±è´¥æ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
    from common.task_manager import get_task_manager, TrainingTask, TrainingStatus

# æ•°æ®æ–‡ä»¶å­˜å‚¨
uploaded_data_files = {}  # å­˜å‚¨ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶ä¿¡æ¯


def _normalize_device_target(device_value):
    """æ ‡å‡†åŒ–è®¾å¤‡ç±»å‹å­—ç¬¦ä¸²ï¼Œç¡®ä¿PyTorchè¯†åˆ«"""
    if not device_value:
        return 'cpu'
    normalized = str(device_value).strip().lower()
    if normalized in ('gpu', 'cuda'):
        return 'cuda' if torch.cuda.is_available() else 'cpu'
    return 'cpu'


def _get_torch_device(device_target: str = None) -> torch.device:
    """è·å–PyTorchè®¾å¤‡"""
    if device_target is None:
        device_target = 'cuda' if torch.cuda.is_available() else 'cpu'
    return torch.device(device_target)

@anomaly_detection_bp.route('/upload_data', methods=['POST'])
def upload_training_data():
    """æ¥æ”¶è¾¹ç«¯ä¸Šä¼ çš„è®­ç»ƒæ•°æ®ï¼Œä¿å­˜åˆ°äº‘ç«¯è®­ç»ƒæ•°æ®ç›®å½•"""
    try:
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'No file provided'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'No file selected'}), 400
        
        # è·å–task_idï¼ˆå¦‚æœæä¾›ï¼‰
        task_id = request.form.get('task_id', '').strip()
        
        if task_id:
            # ä¿å­˜åˆ°task_idå¯¹åº”çš„ç›®å½•
            training_data_dir = Path('data') / 'ad' / task_id
            training_data_dir.mkdir(parents=True, exist_ok=True)
            # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼ˆä¸æ·»åŠ æ—¶é—´æˆ³ï¼Œå› ä¸ºå·²ç»åœ¨task_idç›®å½•ä¸‹ï¼‰
            filename = file.filename
            file_path = training_data_dir / filename
        else:
            # å…¼å®¹æ—§æ¨¡å¼ï¼šä¿å­˜åˆ°é€šç”¨ç›®å½•
            training_data_dir = Path('data') / 'ad'
            training_data_dir.mkdir(parents=True, exist_ok=True)
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åä»¥é¿å…å†²çª
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            original_name = Path(file.filename)
            filename = f"{timestamp}_{original_name.stem}{original_name.suffix}"
            file_path = training_data_dir / filename
        
        # ä¿å­˜æ–‡ä»¶
        file.save(str(file_path))
        
        # è®°å½•æ–‡ä»¶ä¿¡æ¯
        uploaded_data_files[file.filename] = {
            'original_name': file.filename,
            'saved_name': filename,
            'path': str(file_path),
            'uploaded_at': datetime.now().isoformat(),
            'size': file_path.stat().st_size,
            'source': 'edge_upload',
            'task_id': task_id if task_id else None
        }
        
        logger.debug(f"æ•°æ®æ–‡ä»¶ä¸Šä¼ : {filename} ({file_path.stat().st_size} bytes)")
        
        return jsonify({
            'success': True,
            'original_filename': file.filename,
            'saved_filename': filename,
            'message': 'Training data uploaded successfully to cloud'
        })
        
    except Exception as e:
        logger.error(f"Data upload failed: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# è¾…åŠ©å‡½æ•°
def _notify_edge_model_ready(task_id: str, model_type: str):
    """é€šçŸ¥Edgeç«¯æ¨¡å‹å·²å°±ç»ªï¼Œå¯ä»¥ä¸‹è½½"""
    try:
        import requests
        
        # è¿™é‡Œå¯ä»¥é€šè¿‡APIé€šçŸ¥Edgeç«¯ï¼Œæˆ–è€…ç®€å•åœ°è®°å½•æ—¥å¿—
        # åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œå¯ä»¥é…ç½®Edgeç«¯åœ°å€è¿›è¡Œä¸»åŠ¨é€šçŸ¥
        # æˆ–è€…Edgeç«¯å®šæœŸè½®è¯¢Cloudç«¯è·å–æ–°æ¨¡å‹
        
        logger.info(f"æ¨¡å‹å·²å°±ç»ªï¼Œç­‰å¾…Edgeç«¯ä¸‹è½½: {task_id} (ç±»å‹: {model_type})")
        
        # å¦‚æœçŸ¥é“Edgeç«¯åœ°å€ï¼Œå¯ä»¥ä¸»åŠ¨é€šçŸ¥ï¼š
        # edge_url = os.getenv('EDGE_SERVICE_URL')  # ä¾‹å¦‚ http://edge-device:5000
        # if edge_url:
        #     try:
        #         response = requests.post(f"{edge_url}/api/models/notification", 
        #                                json={'task_id': task_id, 'model_type': model_type},
        #                                timeout=5)
        #         logger.info(f"å·²é€šçŸ¥Edgeç«¯: {response.status_code}")
        #     except Exception as e:
        #         logger.warning(f"é€šçŸ¥Edgeç«¯å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        logger.error(f"é€šçŸ¥Edgeç«¯æ¨¡å‹å°±ç»ªå¤±è´¥: {e}")
        return False

def _create_inference_task_dir(model_type: str) -> Path:
    """åˆ›å»ºæ¨ç†ä»»åŠ¡ç›®å½•"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    counter = 1
    
    base_dir = Path(f'models/anomaly_detection/inference_tasks')
    base_dir.mkdir(parents=True, exist_ok=True)
    
    while True:
        task_id = f"{timestamp}_{counter:03d}"
        inference_dir = base_dir / f'inference_{task_id}'
        if not inference_dir.exists():
            inference_dir.mkdir(parents=True, exist_ok=True)
            return inference_dir
        counter += 1

def _save_inference_config(inference_dir: Path, data: dict, model_artifacts: dict):
    """ä¿å­˜æ¨ç†ä»»åŠ¡é…ç½®"""
    config = {
        'task_id': inference_dir.name.replace('inference_', ''),
        'task_type': 'inference',
        'model_type': data.get('model_type', 'lstm_predictor'),
        'module': 'anomaly_detection',
        'created_at': datetime.now().isoformat(),
        
        # æ¨¡å‹ä¿¡æ¯
        'source_model_dir': str(model_artifacts['model_dir']),
        'source_task_id': model_artifacts.get('task_id'),
        'model_path': str(model_artifacts['model_path']),
        'scaler_path': str(model_artifacts.get('scaler_path', '')),
        'threshold_path': str(model_artifacts.get('threshold_path', '')),
        
        # æ¨ç†é…ç½®
        'sequence_length': model_artifacts.get('sequence_length'),
        'batch_size': data.get('batch_size', 32),
        
        # æ•°æ®ä¿¡æ¯
        'data_path': data.get('data_path', ''),
        'label_column': data.get('label_column'),
        
        # é˜ˆå€¼ä¿¡æ¯
        'threshold_value': model_artifacts.get('threshold_value'),
        'threshold_meta': model_artifacts.get('threshold_meta', {})
    }
    
    config_path = inference_dir / 'config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    return config_path

def _save_inference_results(inference_dir: Path, result: dict, data_info: dict = None):
    """ä¿å­˜æ¨ç†ç»“æœ"""
    # ä¿å­˜ä¸»è¦ç»“æœ
    results_summary = {
        'success': result['success'],
        'model_type': result['model_type'],
        'total_samples': result['total_samples'],
        'anomalies_detected': result['anomalies_detected'],
        'anomaly_percentage': result['anomaly_percentage'],
        'threshold': result['threshold'],
        'threshold_source': result.get('threshold_source'),
        'statistics': result['statistics'],
        'sequence_length': result['sequence_length'],
        'input_dim': result['input_dim'],
        'completed_at': datetime.now().isoformat()
    }
    
    # å¦‚æœæœ‰æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…å«è¿›å»
    if 'performance_metrics' in result:
        results_summary['performance_metrics'] = result['performance_metrics']
    
    # ä¿å­˜æ‘˜è¦ç»“æœ
    summary_path = inference_dir / 'results_summary.json'
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆå¤§æ•°æ®é‡ï¼‰
    detailed_results = {
        'residual_scores': result['residual_scores'],
        'anomaly_mask': result['anomaly_mask'],
        'predictions': result['predictions']
    }
    
    # ä½¿ç”¨numpyæ ¼å¼ä¿å­˜å¤§æ•°æ®
    np.savez_compressed(
        inference_dir / 'detailed_results.npz',
        **detailed_results
    )
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    if data_info:
        data_info_path = inference_dir / 'data_info.json'
        with open(data_info_path, 'w', encoding='utf-8') as f:
            json.dump(data_info, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {inference_dir}")
    print(f"  - é…ç½®æ–‡ä»¶: config.json")
    print(f"  - ç»“æœæ‘˜è¦: results_summary.json")
    print(f"  - è¯¦ç»†ç»“æœ: detailed_results.npz")
    
    return inference_dir

def _load_model_config(model_dir: Path) -> dict:
    """åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶"""
    config_path = model_dir / 'config.json'
    if not config_path.exists():
        return {}
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as exc:
        logger.warning(f"Failed to load model config from {config_path}: {exc}")
        return {}

# APIè·¯ç”±

@anomaly_detection_bp.route('/training', methods=['POST'])
def create_training():
    """åˆ›å»ºå¼‚å¸¸æ£€æµ‹è®­ç»ƒä»»åŠ¡"""
    if not training_available:
        return jsonify({
            'success': False,
            'error': 'Training functionality not available'
        }), 503

    data = request.get_json()
    if not data:
        return jsonify({'success': False, 'error': 'No data provided'}), 400

    # éªŒè¯å¼‚å¸¸æ£€æµ‹æ¨¡å‹ç±»å‹
    valid_models = ['lstm_predictor', 'cnn_autoencoder', 'cnn_1d_autoencoder', 'lstm_autoencoder']
    model_type = data.get('model_type', 'lstm_predictor')
    if model_type not in valid_models:
        return jsonify({
            'success': False,
            'error': f'Invalid model_type for anomaly_detection. Must be one of: {", ".join(valid_models)}'
        }), 400

    data['model_type'] = model_type
    data['module'] = 'anomaly_detection'

    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¥æ”¶åˆ°çš„æ•°æ®
    train_files = data.get('train_files', [])
    test_files = data.get('test_files', [])
    logger.info(f"Cloudç«¯create_trainingæ¥æ”¶åˆ°çš„æ•°æ® - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
    logger.info(f"Cloudç«¯create_trainingæ¥æ”¶åˆ°çš„æ•°æ® - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
    print(f"ğŸ” Cloudç«¯create_trainingæ¥æ”¶åˆ°çš„æ•°æ® - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
    print(f"ğŸ” Cloudç«¯create_trainingæ¥æ”¶åˆ°çš„æ•°æ® - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")

    try:
        # ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨åˆ›å»ºä»»åŠ¡
        task_manager = get_task_manager()
        task = task_manager.create_task(data)
        
        # éªŒè¯ä»»åŠ¡ä¿å­˜çš„config
        saved_train_files = task.config.get('train_files', [])
        saved_test_files = task.config.get('test_files', [])
        logger.info(f"Cloudç«¯ä»»åŠ¡ä¿å­˜çš„config - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {saved_train_files}")
        logger.info(f"Cloudç«¯ä»»åŠ¡ä¿å­˜çš„config - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {saved_test_files}")
        print(f"ğŸ” Cloudç«¯ä»»åŠ¡ä¿å­˜çš„config - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {saved_train_files}")
        print(f"ğŸ” Cloudç«¯ä»»åŠ¡ä¿å­˜çš„config - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {saved_test_files}")

        # å¯åŠ¨å¼‚æ­¥è®­ç»ƒ
        task_manager.start_training(task.task_id, _run_real_training)

        logger.info(f"è®­ç»ƒä»»åŠ¡å·²åˆ›å»º: {task.task_id}")
        return jsonify({
            'success': True,
            'message': 'Anomaly detection training task created',
            'task_id': task.task_id,
            'model_type': model_type,
            'module': 'anomaly_detection'
        })

    except Exception as e:
        logger.error(f"Failed to create training task: {e}")
        return jsonify({
            'success': False,
            'error': f'Failed to create training task: {str(e)}'
        }), 500

def _process_condition_filtered_data(config, task_id, task_manager, model_type):
    """
    å¤„ç†å·¥å†µç­›é€‰æ¨¡å¼çš„æ•°æ®ï¼š
    1. è¯»å–å¤šä¸ªè®­ç»ƒæ–‡ä»¶
    2. ä»å…ƒæ•°æ®æ–‡ä»¶ï¼ˆ.jsonï¼‰è¯»å–å·¥å†µä¿¡æ¯ï¼ˆtags_conditionï¼‰ï¼Œè€Œä¸æ˜¯ä»æ–‡ä»¶å
    3. å°†å·¥å†µä¿¡æ¯æ·»åŠ åˆ°ç‰¹å¾ç»´åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬éƒ½æ·»åŠ ç›¸åŒçš„å·¥å†µå€¼ï¼‰
    4. å¯¹æ¯ä¸ªæ–‡ä»¶åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆéªŒè¯é›†å–æœ€åval_ratioæ¯”ä¾‹ï¼Œä¿æŒæ—¶é—´è¿ç»­æ€§ï¼‰
    5. æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„è®­ç»ƒæ•°æ®ï¼Œç»Ÿä¸€fit scalerï¼ˆåªåŸºäºè®­ç»ƒæ•°æ®ï¼‰
    6. å¯¹æ¯ä¸ªæ–‡ä»¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«æ ‡å‡†åŒ–
    7. å¯¹æ¯ä¸ªæ–‡ä»¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«åˆ›å»ºæ»‘åŠ¨çª—å£
    8. åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„è®­ç»ƒçª—å£å’ŒéªŒè¯çª—å£
    9. ä¿å­˜ä¸ºtrain.npz, dev.npz, test.npz
    """
    import pickle
    from sklearn.preprocessing import StandardScaler
    
    train_files = config.get('train_files', [])
    test_files = config.get('test_files', [])
    conditions = config.get('conditions', {})  # {key: [value1, value2, ...]}
    validation_split = config.get('validation_split', 0.2)
    sequence_length = config.get('sequence_length', 50)
    stride = config.get('stride', 1)
    
    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¥æ”¶åˆ°çš„æ–‡ä»¶åˆ—è¡¨
    logger.info(f"æ¥æ”¶åˆ°çš„è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
    logger.info(f"æ¥æ”¶åˆ°çš„æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
    
    # å¦‚æœæ–‡ä»¶åŒæ—¶å‡ºç°åœ¨train_fileså’Œtest_filesä¸­ï¼Œä¼˜å…ˆå°†å…¶è§†ä¸ºæµ‹è¯•æ–‡ä»¶ï¼Œä»train_filesä¸­ç§»é™¤
    if train_files and test_files:
        train_files_set = set(train_files)
        test_files_set = set(test_files)
        overlap = train_files_set & test_files_set
        if overlap:
            logger.warning(f"å‘ç°æ–‡ä»¶åŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’Œæµ‹è¯•åˆ—è¡¨ä¸­ï¼Œå°†ä»è®­ç»ƒåˆ—è¡¨ä¸­ç§»é™¤: {overlap}")
            task_manager.add_log(task_id, f'è­¦å‘Š: å‘ç° {len(overlap)} ä¸ªæ–‡ä»¶åŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’Œæµ‹è¯•åˆ—è¡¨ä¸­ï¼Œå°†ä»è®­ç»ƒåˆ—è¡¨ä¸­ç§»é™¤: {list(overlap)}')
            train_files = [f for f in train_files if f not in overlap]
            config['train_files'] = train_files
    
    task_manager.add_log(task_id, f'å·¥å†µç­›é€‰æ¨¡å¼: {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶, {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶')
    task_manager.add_log(task_id, f'è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}')
    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}')
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶ç›®å½•
    training_data_dir = Path('data') / 'ad' / task_id
    training_data_dir.mkdir(parents=True, exist_ok=True)
    
    # è·å–å·¥å†µkeyåˆ—è¡¨ï¼ˆç”¨äºæ·»åŠ ç‰¹å¾ï¼‰
    # å¦‚æœæ²¡æœ‰é€‰æ‹©å·¥å†µï¼Œåˆ™ä¸æ·»åŠ å·¥å†µç‰¹å¾
    condition_keys = sorted(list(conditions.keys())) if conditions else []
    if condition_keys:
        task_manager.add_log(task_id, f'å·¥å†µç‰¹å¾: {", ".join(condition_keys)}')
    else:
        task_manager.add_log(task_id, 'æœªé€‰æ‹©å·¥å†µï¼Œå°†ä¸æ·»åŠ å·¥å†µç‰¹å¾')
    
    # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ¯ä¸ªè®­ç»ƒæ–‡ä»¶ï¼ˆæ·»åŠ å·¥å†µã€åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼‰
    all_train_raw_data = []  # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼ˆç”¨äºfit scalerï¼‰
    file_data_list = []  # ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ä¿¡æ¯ï¼ˆåŒ…å«train_dataå’Œval_dataï¼‰
    
    for filename in train_files:
        # task_manager.add_log(task_id, f'è¯»å–è®­ç»ƒæ–‡ä»¶: {filename}')  # å‡å°‘æ—¥å¿—
        
        # æŸ¥æ‰¾æ–‡ä»¶
        file_path = training_data_dir / filename
        if not file_path.exists():
            if filename in uploaded_data_files:
                file_path = Path(uploaded_data_files[filename]['path'])
            else:
                task_manager.add_log(task_id, f'è­¦å‘Š: æ–‡ä»¶æœªæ‰¾åˆ° {filename}ï¼Œè·³è¿‡')
                continue
        
        # è¯»å–æ•°æ®æ–‡ä»¶
        df = pd.read_csv(file_path)
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆå¦‚æœå­˜åœ¨æ—¶é—´æˆ³åˆ—ï¼‰
        time_col = None
        for col in df.columns:
            if col.lower() in ['timestamp', 'time', 'æ—¶é—´']:
                time_col = col
                break
        
        if time_col:
            try:
                # å°è¯•å°†æ—¶é—´åˆ—è½¬æ¢ä¸ºæ•°å€¼æˆ–datetimeç±»å‹
                if pd.api.types.is_numeric_dtype(df[time_col]):
                    df = df.sort_values(by=time_col)
                else:
                    # å°è¯•è½¬æ¢ä¸ºdatetime
                    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
                    df = df.sort_values(by=time_col)
                task_manager.add_log(task_id, f'æ–‡ä»¶ {filename} å·²æŒ‰æ—¶é—´æ’åº')
            except Exception as e:
                task_manager.add_log(task_id, f'è­¦å‘Š: æ—¶é—´æ’åºå¤±è´¥ {filename}: {e}ï¼Œä½¿ç”¨åŸå§‹é¡ºåº')
        
        # è·å–æ•°å€¼åˆ—ï¼ˆæ’é™¤æ—¶é—´æˆ³åˆ—ï¼‰
        numeric_cols = [col for col in df.columns 
                       if pd.api.types.is_numeric_dtype(df[col]) 
                       and col.lower() not in ['timestamp', 'time', 'æ—¶é—´']]
        
        if not numeric_cols:
            task_manager.add_log(task_id, f'è­¦å‘Š: æ–‡ä»¶ {filename} æ²¡æœ‰æ•°å€¼åˆ—ï¼Œè·³è¿‡')
            continue
        
        # ä»å…ƒæ•°æ®æ–‡ä»¶è¯»å–å·¥å†µä¿¡æ¯ï¼ˆä¸ä»æ–‡ä»¶åæå–ï¼‰
        meta_file_path = training_data_dir / (filename.replace('.csv', '.json'))
        condition_values = {}
        
        if meta_file_path.exists():
            try:
                with open(meta_file_path, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                    tags_condition = meta_data.get('tags_condition', [])
                    for cond in tags_condition:
                        if isinstance(cond, dict) and 'key' in cond and 'value' in cond:
                            key = cond['key']
                            if key in condition_keys:
                                condition_values[key] = float(cond['value'])
                # task_manager.add_log(task_id, f'æ–‡ä»¶ {filename} å·¥å†µå€¼: {condition_values}')  # å‡å°‘æ—¥å¿—
            except Exception as e:
                task_manager.add_log(task_id, f'è­¦å‘Š: è¯»å–å…ƒæ•°æ®å¤±è´¥ {filename}: {e}ï¼Œå·¥å†µå€¼å°†ä½¿ç”¨é»˜è®¤å€¼0.0')
        else:
            task_manager.add_log(task_id, f'è­¦å‘Š: æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶ {meta_file_path}ï¼Œå·¥å†µå€¼å°†ä½¿ç”¨é»˜è®¤å€¼0.0')
        
        # æå–ç‰¹å¾æ•°æ®
        feature_data = df[numeric_cols].values.astype(np.float32)
        
        # æ·»åŠ å·¥å†µç‰¹å¾ï¼ˆå¦‚æœé€‰æ‹©äº†å·¥å†µï¼‰
        if condition_keys:
            for key in condition_keys:
                value = condition_values.get(key, 0.0)
                condition_feature = np.full((len(feature_data), 1), value, dtype=np.float32)
                feature_data = np.hstack([feature_data, condition_feature])
        
        # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆéªŒè¯é›†å–æœ€åval_ratioæ¯”ä¾‹ï¼Œä¿æŒæ—¶é—´è¿ç»­æ€§ï¼‰
        n_samples = len(feature_data)
        val_len = int(n_samples * validation_split)
        train_data = feature_data[:-val_len] if val_len > 0 else feature_data
        val_data = feature_data[-val_len:] if val_len > 0 else np.array([]).reshape(0, feature_data.shape[1])
        
        task_manager.add_log(task_id, f'æ–‡ä»¶ {filename}: æ€»æ ·æœ¬æ•°={n_samples}, è®­ç»ƒé›†={len(train_data)}, éªŒè¯é›†={len(val_data)}')
        
        # ä¿å­˜æ–‡ä»¶ä¿¡æ¯
        file_data_list.append({
            'filename': filename,
            'train_data': train_data,
            'val_data': val_data
        })
        
        # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼ˆç”¨äºfit scalerï¼‰
        all_train_raw_data.append(train_data)
    
    if not all_train_raw_data:
        raise ValueError("æ²¡æœ‰è®­ç»ƒæ•°æ®")
    
    # ç¬¬äºŒæ­¥ï¼šç»Ÿä¸€fit scalerï¼ˆåªåŸºäºæ‰€æœ‰æ–‡ä»¶çš„è®­ç»ƒæ•°æ®ï¼‰
    all_train_raw = np.vstack(all_train_raw_data)
    scaler = StandardScaler()
    scaler.fit(all_train_raw)
    task_manager.add_log(task_id, f'Scalerå·²fitï¼ˆåŸºäºæ‰€æœ‰è®­ç»ƒæ•°æ®ï¼‰ï¼Œç‰¹å¾ç»´åº¦: {all_train_raw.shape[1]}')
    
    # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«æ ‡å‡†åŒ–ã€åˆ›å»ºçª—å£
    all_train_sequences = []
    all_val_sequences = []
    all_train_targets = []
    all_val_targets = []
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„çª—å£åˆ›å»ºå‡½æ•°
    if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
        # è‡ªç¼–ç å™¨ï¼štargets = inputsï¼ˆé‡æ„è‡ªå·±ï¼‰
        def create_sequences(data):
            if len(data) < sequence_length:
                return np.array([]).reshape(0, sequence_length, data.shape[1]), np.array([]).reshape(0, sequence_length, data.shape[1])
            sequences = []
            targets = []
            for start in range(0, len(data) - sequence_length + 1, stride):
                end = start + sequence_length
                seq = data[start:end]
                sequences.append(seq)
                targets.append(seq.copy())  # è‡ªç¼–ç å™¨ï¼štarget = input
            return (np.stack(sequences) if sequences else np.array([]).reshape(0, sequence_length, data.shape[1]),
                    np.stack(targets) if targets else np.array([]).reshape(0, sequence_length, data.shape[1]))
    else:
        # LSTMé¢„æµ‹æ¨¡å‹ï¼štargets = futureï¼ˆé¢„æµ‹æœªæ¥å€¼ï¼‰
        prediction_horizon = config.get('prediction_horizon', 1)
        def create_sequences(data):
            if len(data) < sequence_length + prediction_horizon:
                return np.array([]).reshape(0, sequence_length, data.shape[1]), np.array([]).reshape(0, prediction_horizon, data.shape[1])
            sequences = []
            targets = []
            for start in range(0, len(data) - sequence_length - prediction_horizon + 1, stride):
                end = start + sequence_length
                seq = data[start:end]  # past: [x_t, ..., x_{t+L-1}]
                future = data[end:end + prediction_horizon]  # future: [x_{t+L}, ..., x_{t+L+H-1}]
                sequences.append(seq)
                targets.append(future)
            return (np.stack(sequences) if sequences else np.array([]).reshape(0, sequence_length, data.shape[1]),
                    np.stack(targets) if targets else np.array([]).reshape(0, prediction_horizon, data.shape[1]))
    
    for file_info in file_data_list:
        # task_manager.add_log(task_id, f'å¤„ç†æ–‡ä»¶çª—å£: {file_info["filename"]}')  # å‡å°‘æ—¥å¿—
        
        # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«æ ‡å‡†åŒ–
        train_data_scaled = scaler.transform(file_info['train_data'])
        val_data_scaled = scaler.transform(file_info['val_data']) if len(file_info['val_data']) > 0 else np.array([]).reshape(0, train_data_scaled.shape[1])
        
        # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†åˆ«åˆ›å»ºæ»‘åŠ¨çª—å£
        train_seqs, train_tgts = create_sequences(train_data_scaled)
        val_seqs, val_tgts = create_sequences(val_data_scaled)
        
        if len(train_seqs) > 0:
            all_train_sequences.append(train_seqs)
            all_train_targets.append(train_tgts)
        if len(val_seqs) > 0:
            all_val_sequences.append(val_seqs)
            all_val_targets.append(val_tgts)
    
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„çª—å£
    if not all_train_sequences:
        raise ValueError("æ²¡æœ‰ç”Ÿæˆä»»ä½•è®­ç»ƒåºåˆ—")
    
    train_sequences = np.vstack(all_train_sequences)
    train_targets = np.vstack(all_train_targets)
    
    # éªŒè¯é›†çš„targetsç»´åº¦éœ€è¦æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®š
    if all_val_sequences:
        val_sequences = np.vstack(all_val_sequences)
        val_targets = np.vstack(all_val_targets)
    else:
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºç©ºçš„éªŒè¯é›†
        if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
            val_sequences = np.array([]).reshape(0, sequence_length, train_sequences.shape[2])
            val_targets = np.array([]).reshape(0, sequence_length, train_targets.shape[2])
        else:
            prediction_horizon = config.get('prediction_horizon', 1)
            val_sequences = np.array([]).reshape(0, sequence_length, train_sequences.shape[2])
            val_targets = np.array([]).reshape(0, prediction_horizon, train_targets.shape[2])
    
    task_manager.add_log(task_id, f'è®­ç»ƒé›†: {len(train_sequences)} ä¸ªåºåˆ—')
    task_manager.add_log(task_id, f'éªŒè¯é›†: {len(val_sequences)} ä¸ªåºåˆ—')
    
    # ä¿å­˜æ•°æ®ï¼ˆç»Ÿä¸€ä½¿ç”¨npzæ ¼å¼ï¼‰
    train_data_path = training_data_dir / 'train.npz'
    dev_data_path = training_data_dir / 'dev.npz'
    test_data_path = training_data_dir / 'test.npz'
    
    np.savez(train_data_path, sequences=train_sequences, targets=train_targets)
    np.savez(dev_data_path, sequences=val_sequences, targets=val_targets)
    
    # ä¿å­˜scaler
    scaler_path = training_data_dir / 'scaler.pkl'
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    # å¤„ç†æµ‹è¯•é›†æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    if test_files:
        all_test_sequences = []
        all_test_targets = []
        all_test_labels = []  # ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼ˆ0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸ï¼‰
        
        task_manager.add_log(task_id, f'å¼€å§‹å¤„ç†æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}')
        for filename in test_files:
            task_manager.add_log(task_id, f'å¼€å§‹å¤„ç†æµ‹è¯•æ–‡ä»¶: {filename}')
            
            file_path = training_data_dir / filename
            if not file_path.exists():
                if filename in uploaded_data_files:
                    file_path = Path(uploaded_data_files[filename]['path'])
                    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} åœ¨ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ä¸­æ‰¾åˆ°: {file_path}')
                else:
                    task_manager.add_log(task_id, f'è­¦å‘Š: æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ° {filename}ï¼Œè·³è¿‡')
                    logger.warning(f"æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°: {filename}, è®­ç»ƒæ•°æ®ç›®å½•: {training_data_dir}, ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨: {list(uploaded_data_files.keys())}")
                    continue
            
            task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} è·¯å¾„: {file_path}')
            
            df = pd.read_csv(file_path)
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆå¦‚æœå­˜åœ¨æ—¶é—´æˆ³åˆ—ï¼‰
            time_col = None
            for col in df.columns:
                if col.lower() in ['timestamp', 'time', 'æ—¶é—´']:
                    time_col = col
                    break
            
            if time_col:
                try:
                    if pd.api.types.is_numeric_dtype(df[time_col]):
                        df = df.sort_values(by=time_col)
                    else:
                        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
                        df = df.sort_values(by=time_col)
                except Exception as e:
                    task_manager.add_log(task_id, f'è­¦å‘Š: æµ‹è¯•æ–‡ä»¶æ—¶é—´æ’åºå¤±è´¥ {filename}: {e}')
            
            numeric_cols = [col for col in df.columns 
                           if pd.api.types.is_numeric_dtype(df[col]) 
                           and col.lower() not in ['timestamp', 'time', 'æ—¶é—´']]
            
            if not numeric_cols:
                task_manager.add_log(task_id, f'è­¦å‘Š: æµ‹è¯•æ–‡ä»¶ {filename} æ²¡æœ‰æ•°å€¼åˆ—ï¼Œè·³è¿‡')
                continue
            
            # ä»å…ƒæ•°æ®æ–‡ä»¶è¯»å–å·¥å†µä¿¡æ¯å’Œæ ‡ç­¾ä¿¡æ¯
            meta_file_path = training_data_dir / (filename.replace('.csv', '.json'))
            condition_values = {}
            file_label = 0  # é»˜è®¤æ­£å¸¸ï¼ˆ0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸ï¼‰
            
            if meta_file_path.exists():
                try:
                    with open(meta_file_path, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                        
                        # è¯»å–å·¥å†µä¿¡æ¯
                        tags_condition = meta_data.get('tags_condition', [])
                        for cond in tags_condition:
                            if isinstance(cond, dict) and 'key' in cond and 'value' in cond:
                                key = cond['key']
                                if key in condition_keys:
                                    condition_values[key] = float(cond['value'])
                        
                        # è¯»å–æ ‡ç­¾ä¿¡æ¯ï¼ˆä»tags_labelåˆ¤æ–­æ˜¯æ­£å¸¸è¿˜æ˜¯å¼‚å¸¸ï¼‰
                        tags_label = meta_data.get('tags_label', [])
                        task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} å…ƒæ•°æ®tags_label: {tags_label}')
                        label_found = False
                        for label_tag in tags_label:
                            if isinstance(label_tag, dict) and 'value' in label_tag:
                                label_value_raw = label_tag['value']
                                label_value = str(label_value_raw).strip().lower()
                                task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾å€¼: "{label_value_raw}" (å¤„ç†å: "{label_value}")')
                                # åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸ï¼ˆå¯ä»¥æ ¹æ®å®é™…æ ‡ç­¾å€¼è°ƒæ•´ï¼‰
                                if label_value in ['å¼‚å¸¸', 'anomaly', 'abnormal', 'æ•…éšœ', 'fault', '1', 'true']:
                                    file_label = 1  # å¼‚å¸¸
                                    label_found = True
                                    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: å¼‚å¸¸ (ä»å…ƒæ•°æ®: {label_value_raw})')
                                    break
                                elif label_value in ['æ­£å¸¸', 'normal', 'å¥åº·', 'healthy', '0', 'false']:
                                    file_label = 0  # æ­£å¸¸
                                    label_found = True
                                    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: æ­£å¸¸ (ä»å…ƒæ•°æ®: {label_value_raw})')
                                    break
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•ä»æ–‡ä»¶ååˆ¤æ–­ï¼ˆä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰
                        if not label_found:
                            task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æœªåœ¨å…ƒæ•°æ®ä¸­æ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­')
                            if 'å¼‚å¸¸' in filename or 'abnormal' in filename.lower() or 'anomaly' in filename.lower():
                                file_label = 1
                                task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: å¼‚å¸¸ (ä»æ–‡ä»¶åæ¨æ–­)')
                            else:
                                file_label = 0
                                task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: æ­£å¸¸ (é»˜è®¤å€¼)')
                except Exception as e:
                    task_manager.add_log(task_id, f'è­¦å‘Š: è¯»å–æµ‹è¯•å…ƒæ•°æ®å¤±è´¥ {filename}: {e}ï¼Œå·¥å†µå€¼å’Œæ ‡ç­¾å°†ä½¿ç”¨é»˜è®¤å€¼')
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•ä»æ–‡ä»¶ååˆ¤æ–­
                    if 'å¼‚å¸¸' in filename or 'abnormal' in filename.lower() or 'anomaly' in filename.lower():
                        file_label = 1
            else:
                task_manager.add_log(task_id, f'è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•å…ƒæ•°æ®æ–‡ä»¶ {meta_file_path}ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­æ ‡ç­¾')
                # å¦‚æœå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ–‡ä»¶ååˆ¤æ–­
                if 'å¼‚å¸¸' in filename or 'abnormal' in filename.lower() or 'anomaly' in filename.lower():
                    file_label = 1
                    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: å¼‚å¸¸ (ä»æ–‡ä»¶åæ¨æ–­)')
                else:
                    file_label = 0
                    task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename} æ ‡ç­¾: æ­£å¸¸ (é»˜è®¤å€¼)')
            
            feature_data = df[numeric_cols].values.astype(np.float32)
            n_samples = len(feature_data)
            
            # ä¸ºæ•´ä¸ªæ–‡ä»¶åˆ›å»ºæ ‡ç­¾æ•°ç»„ï¼ˆæ‰€æœ‰æ ·æœ¬ä½¿ç”¨ç›¸åŒçš„æ–‡ä»¶æ ‡ç­¾ï¼‰
            file_labels = np.full(n_samples, file_label, dtype=np.int32)
            
            # æ·»åŠ å·¥å†µç‰¹å¾ï¼ˆå¦‚æœé€‰æ‹©äº†å·¥å†µï¼‰
            if condition_keys:
                for key in condition_keys:
                    value = condition_values.get(key, 0.0)
                    condition_feature = np.full((len(feature_data), 1), value, dtype=np.float32)
                    feature_data = np.hstack([feature_data, condition_feature])
            
            # åœ¨ç‰¹å¾æ•°æ®æœ€åæ·»åŠ æ ‡ç­¾åˆ—ï¼ˆ1=å¼‚å¸¸ï¼Œ0=æ­£å¸¸ï¼‰
            # æ³¨æ„ï¼šæ ‡ç­¾åˆ—ä¸å‚ä¸æ ‡å‡†åŒ–ï¼Œåœ¨æ ‡å‡†åŒ–åå†æ·»åŠ 
            label_column = file_labels.reshape(-1, 1).astype(np.float32)
            
            # æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®ï¼ˆä¸åŒ…æ‹¬æ ‡ç­¾åˆ—ï¼‰
            test_data_scaled = scaler.transform(feature_data)
            
            # æ ‡å‡†åŒ–åï¼Œå°†æ ‡ç­¾åˆ—æ·»åŠ åˆ°ç‰¹å¾æ•°æ®çš„æœ€åä¸€åˆ—
            test_data_scaled = np.hstack([test_data_scaled, label_column])
            
            # åˆ›å»ºæ»‘åŠ¨çª—å£ï¼ˆæ•´ä¸ªæ–‡ä»¶ï¼Œä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼‰
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºä¸åŒçš„çª—å£
            if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
                # è‡ªç¼–ç å™¨ï¼štargets = inputs
                test_seqs, test_tgts = create_sequences(test_data_scaled)
            else:
                # LSTMé¢„æµ‹æ¨¡å‹ï¼štargets = future
                prediction_horizon = config.get('prediction_horizon', 1)
                if len(test_data_scaled) < sequence_length + prediction_horizon:
                    test_seqs = np.array([]).reshape(0, sequence_length, test_data_scaled.shape[1])
                    test_tgts = np.array([]).reshape(0, prediction_horizon, test_data_scaled.shape[1])
                else:
                    test_seqs_list = []
                    test_tgts_list = []
                    for start in range(0, len(test_data_scaled) - sequence_length - prediction_horizon + 1, stride):
                        end = start + sequence_length
                        seq = test_data_scaled[start:end]
                        future = test_data_scaled[end:end + prediction_horizon]
                        test_seqs_list.append(seq)
                        test_tgts_list.append(future)
                    test_seqs = np.stack(test_seqs_list) if test_seqs_list else np.array([]).reshape(0, sequence_length, test_data_scaled.shape[1])
                    test_tgts = np.stack(test_tgts_list) if test_tgts_list else np.array([]).reshape(0, prediction_horizon, test_data_scaled.shape[1])
            
            if len(test_seqs) > 0:
                # ä»åºåˆ—çš„æœ€åä¸€åˆ—æå–æ ‡ç­¾ï¼ˆæ ‡ç­¾åˆ—åœ¨æ ‡å‡†åŒ–åæ·»åŠ åˆ°äº†æœ€åä¸€åˆ—ï¼‰
                # æ¯ä¸ªåºåˆ—çš„æ ‡ç­¾å–è¯¥åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ ‡ç­¾å€¼
                seq_labels = test_seqs[:, -1, -1].astype(np.int32)  # å–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€åä¸€åˆ—ï¼ˆæ ‡ç­¾åˆ—ï¼‰
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ ‡ç­¾æå–æ˜¯å¦æ­£ç¡®
                normal_count = int(np.sum(seq_labels == 0))
                anomaly_count = int(np.sum(seq_labels == 1))
                task_manager.add_log(task_id, f'æµ‹è¯•æ–‡ä»¶ {filename}: file_label={file_label}, ç”Ÿæˆåºåˆ—æ•°={len(test_seqs)}, æ ‡ç­¾ç»Ÿè®¡: æ­£å¸¸={normal_count}, å¼‚å¸¸={anomaly_count}')
                logger.info(f"æµ‹è¯•æ–‡ä»¶ {filename}: file_label={file_label}, åºåˆ—æ•°={len(test_seqs)}, æ­£å¸¸={normal_count}, å¼‚å¸¸={anomaly_count}")
                
                # ä»åºåˆ—å’Œtargetsä¸­ç§»é™¤æ ‡ç­¾åˆ—ï¼ˆæœ€åä¸€åˆ—ï¼‰ï¼Œå› ä¸ºæ ‡ç­¾ä¸åº”è¯¥å‚ä¸æ¨¡å‹é¢„æµ‹
                # åºåˆ—å½¢çŠ¶: (n_seqs, seq_len, feature_dim+1)ï¼Œéœ€è¦ç§»é™¤æœ€åä¸€åˆ—
                test_seqs_no_label = test_seqs[:, :, :-1]
                # targetså½¢çŠ¶ä¹Ÿéœ€è¦ç§»é™¤æ ‡ç­¾åˆ—ï¼ˆå¦‚æœæ˜¯è‡ªç¼–ç å™¨ï¼Œtargets=sequencesï¼›å¦‚æœæ˜¯é¢„æµ‹å™¨ï¼Œtargets=futureï¼‰
                if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
                    test_tgts_no_label = test_tgts[:, :, :-1]  # è‡ªç¼–ç å™¨ï¼štargets = sequences
                else:
                    test_tgts_no_label = test_tgts[:, :, :-1]  # é¢„æµ‹å™¨ï¼štargets = futureï¼ˆä¹ŸåŒ…å«æ ‡ç­¾åˆ—ï¼‰
                
                all_test_sequences.append(test_seqs_no_label)
                all_test_targets.append(test_tgts_no_label)
                all_test_labels.append(seq_labels)
            else:
                task_manager.add_log(task_id, f'è­¦å‘Š: æµ‹è¯•æ–‡ä»¶ {filename} æ²¡æœ‰ç”Ÿæˆä»»ä½•åºåˆ—ï¼ˆæ•°æ®é•¿åº¦ä¸è¶³ï¼‰')
                logger.warning(f"æµ‹è¯•æ–‡ä»¶ {filename} æ²¡æœ‰ç”Ÿæˆä»»ä½•åºåˆ—")
        
        if all_test_sequences:
            test_sequences = np.vstack(all_test_sequences)
            test_targets = np.vstack(all_test_targets)
            test_labels = np.concatenate(all_test_labels) if all_test_labels else np.array([], dtype=np.int32)
            
            # è®¡ç®—æ ‡ç­¾ç»Ÿè®¡
            normal_count = int(np.sum(test_labels == 0))
            anomaly_count = int(np.sum(test_labels == 1))
            total_count = len(test_labels)
            
            # åœ¨ç»ˆç«¯è¾“å‡ºæ ‡ç­¾ç»Ÿè®¡ï¼ˆä½¿ç”¨loggerå’ŒprintåŒé‡è¾“å‡ºç¡®ä¿å¯è§ï¼‰
            logger.info(f"ğŸ“Š æµ‹è¯•é›†æ ‡ç­¾ç»Ÿè®¡: æ€»åºåˆ—æ•°={total_count}, æ­£å¸¸æ ·æœ¬={normal_count}, å¼‚å¸¸æ ·æœ¬={anomaly_count}")
            print(f"ğŸ“Š æµ‹è¯•é›†æ ‡ç­¾ç»Ÿè®¡: æ€»åºåˆ—æ•°={total_count}, æ­£å¸¸æ ·æœ¬={normal_count}, å¼‚å¸¸æ ·æœ¬={anomaly_count}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ ‡ç­¾ç»Ÿè®¡
            task_manager.add_log(task_id, f'æµ‹è¯•é›†åˆå¹¶å‰ç»Ÿè®¡: æ–‡ä»¶æ•°={len(all_test_labels)}, æ€»åºåˆ—æ•°={total_count}')
            for i, labels in enumerate(all_test_labels):
                file_normal = int(np.sum(labels == 0))
                file_anomaly = int(np.sum(labels == 1))
                task_manager.add_log(task_id, f'  æ–‡ä»¶{i+1}: åºåˆ—æ•°={len(labels)}, æ­£å¸¸={file_normal}, å¼‚å¸¸={file_anomaly}')
            
            # ä¿å­˜æµ‹è¯•æ•°æ®ï¼ŒåŒ…æ‹¬æ ‡ç­¾ä¿¡æ¯
            np.savez(test_data_path, sequences=test_sequences, targets=test_targets, labels=test_labels)
            task_manager.add_log(task_id, f'æµ‹è¯•é›†: {total_count} ä¸ªåºåˆ—, æ­£å¸¸æ ·æœ¬: {normal_count}, å¼‚å¸¸æ ·æœ¬: {anomaly_count}')
            
            # é¢å¤–æ£€æŸ¥ï¼šéªŒè¯æ ‡ç­¾å€¼
            unique_labels = np.unique(test_labels)
            task_manager.add_log(task_id, f'æµ‹è¯•é›†æ ‡ç­¾å”¯ä¸€å€¼: {unique_labels.tolist()}, æ ‡ç­¾æ•°æ®ç±»å‹: {test_labels.dtype}')
            logger.info(f"æµ‹è¯•é›†æ ‡ç­¾å”¯ä¸€å€¼: {unique_labels.tolist()}, æ ‡ç­¾æ•°æ®ç±»å‹: {test_labels.dtype}")
    
    # åˆ›å»ºTimeSeriesDataå¯¹è±¡
    from anomaly_detection.core.lstm_autoencoder.data_processor import TimeSeriesData
    
    train_data_obj = TimeSeriesData(sequences=train_sequences, targets=train_targets)
    val_data_obj = TimeSeriesData(sequences=val_sequences, targets=val_targets) if len(val_sequences) > 0 else None
    
    # è·å–ç‰¹å¾ç»´åº¦
    feature_dim = train_sequences.shape[2]
    
    return train_data_obj, val_data_obj, feature_dim


def _run_real_training(task_id):
    """æ‰§è¡ŒçœŸå®çš„è®­ç»ƒè¿‡ç¨‹"""
    task_manager = get_task_manager()
    task = task_manager.get_task(task_id)
    
    if task is None:
        logger.error(f"Task {task_id} not found in task manager")
        return
    
    config = task.config
    
    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºä»ä»»åŠ¡è·å–çš„config
    train_files_from_task = config.get('train_files', [])
    test_files_from_task = config.get('test_files', [])
    logger.info(f"Cloudç«¯_run_real_trainingä»ä»»åŠ¡è·å–çš„config - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files_from_task}")
    logger.info(f"Cloudç«¯_run_real_trainingä»ä»»åŠ¡è·å–çš„config - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files_from_task}")
    print(f"ğŸ” Cloudç«¯_run_real_trainingä»ä»»åŠ¡è·å–çš„config - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files_from_task}")
    print(f"ğŸ” Cloudç«¯_run_real_trainingä»ä»»åŠ¡è·å–çš„config - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files_from_task}")
    
    try:
        # è·å–æ¨¡å‹ç±»å‹ï¼ˆæå‰è·å–ï¼Œç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        model_type = config.get('model_type', 'lstm_predictor')
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè®­ç»ƒä¸­
        task_manager.update_task_status(task_id, 'training', 'Initializing training pipeline...')
        task_manager.add_log(task_id, f'Starting {model_type} training task')
        
        # è®°å½•å…³é”®å‚æ•°
        logger.info(f"å¼€å§‹è®­ç»ƒ {model_type} | åºåˆ—é•¿åº¦={config.get('sequence_length', 50)} | "
                   f"éšè—å±‚={config.get('hidden_units', 64)} | å­¦ä¹ ç‡={config.get('learning_rate', 0.001)} | "
                   f"è®­ç»ƒè½®æ•°={config.get('epochs', 50)}")
        
        # è®¾ç½®PyTorchè®¾å¤‡
        device_target = _normalize_device_target(
            config.get('device_target') or config.get('device') or 'cpu'
        )
        device = _get_torch_device(device_target)
        logger.info(f"PyTorch device initialized: {device}")
        
        # 1. æ•°æ®å¤„ç†
        task_manager.update_task_status(task_id, 'training', 'Loading and preprocessing data...')
        task_manager.add_log(task_id, 'Data preprocessing started')
        
        dataset_mode = config.get('dataset_mode', 'processed_file')
        
        # å·¥å†µç­›é€‰æ¨¡å¼ï¼šå¤„ç†å¤šæ–‡ä»¶ã€å·¥å†µä¿¡æ¯ã€æŒ‰æ–‡ä»¶åˆ’åˆ†
        if dataset_mode == 'condition_filtered':
            # ä»configè·å–train_fileså’Œtest_filesï¼ˆEdgeç«¯å‘é€çš„ï¼‰
            train_files = config.get('train_files', [])
            test_files = config.get('test_files', [])
            training_data_dir = Path('data') / 'ad' / task_id
            training_data_dir.mkdir(parents=True, exist_ok=True)
            
            # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºä»configè·å–çš„æ–‡ä»¶åˆ—è¡¨
            logger.info(f"Cloudç«¯_run_real_trainingä»configè·å– - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
            logger.info(f"Cloudç«¯_run_real_trainingä»configè·å– - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
            print(f"ğŸ” Cloudç«¯_run_real_trainingä»configè·å– - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
            print(f"ğŸ” Cloudç«¯_run_real_trainingä»configè·å– - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
            
            # å¦‚æœEdgeç«¯å·²ç»å‘é€äº†train_fileså’Œtest_filesï¼Œéœ€è¦ç­‰å¾…æ–‡ä»¶ä¸Šä¼ å®Œæˆ
            # å› ä¸ºEdgeç«¯æ˜¯å…ˆåˆ›å»ºä»»åŠ¡ï¼Œç„¶åä¸Šä¼ æ–‡ä»¶
            if train_files and len(train_files) > 0:
                task_manager.add_log(task_id, f'ç­‰å¾…Edgeç«¯ä¸Šä¼ æ–‡ä»¶... (è®­ç»ƒæ–‡ä»¶: {len(train_files)} ä¸ª, æµ‹è¯•æ–‡ä»¶: {len(test_files)} ä¸ª)')
                logger.info(f"ç­‰å¾…Edgeç«¯ä¸Šä¼ æ–‡ä»¶... (è®­ç»ƒæ–‡ä»¶: {len(train_files)} ä¸ª, æµ‹è¯•æ–‡ä»¶: {len(test_files)} ä¸ª)")
                
                # ç­‰å¾…æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼Œæ¯2ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
                import time
                max_wait_time = 60
                wait_interval = 2
                waited_time = 0
                last_file_count = 0
                stable_count = 0  # æ–‡ä»¶æ•°é‡ç¨³å®šçš„æ¬¡æ•°ï¼ˆè¿ç»­3æ¬¡ä¸å˜è®¤ä¸ºä¸Šä¼ å®Œæˆï¼‰
                
                while waited_time < max_wait_time:
                    time.sleep(wait_interval)
                    waited_time += wait_interval
                    
                    # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰CSVæ–‡ä»¶
                    if training_data_dir.exists():
                        uploaded_csv_files = [f.name for f in training_data_dir.glob('*.csv') if f.is_file()]
                        uploaded_json_files = [f.name for f in training_data_dir.glob('*.json') if f.is_file()]
                        
                        current_file_count = len(uploaded_csv_files)
                        
                        # æ£€æŸ¥æ‰€æœ‰train_fileså’Œtest_filesæ˜¯å¦éƒ½å·²ä¸Šä¼ 
                        all_expected_files = set(train_files + test_files)
                        uploaded_files_set = set(uploaded_csv_files)
                        
                        if all_expected_files.issubset(uploaded_files_set):
                            # æ‰€æœ‰æ–‡ä»¶éƒ½å·²ä¸Šä¼ ï¼Œæ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶
                            expected_meta_count = len(all_expected_files)
                            if len(uploaded_json_files) >= expected_meta_count:
                                task_manager.add_log(task_id, f'æ‰€æœ‰æ–‡ä»¶å·²ä¸Šä¼ å®Œæˆ: {len(uploaded_csv_files)} ä¸ªCSVæ–‡ä»¶, {len(uploaded_json_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶')
                                logger.info(f"æ‰€æœ‰æ–‡ä»¶å·²ä¸Šä¼ å®Œæˆ: {len(uploaded_csv_files)} ä¸ªCSVæ–‡ä»¶, {len(uploaded_json_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶")
                                break
                        
                        # å¦‚æœæ–‡ä»¶æ•°é‡å‘ç”Ÿå˜åŒ–ï¼Œé‡ç½®ç¨³å®šè®¡æ•°å™¨
                        if current_file_count != last_file_count:
                            stable_count = 0
                            last_file_count = current_file_count
                            if current_file_count > 0:
                                task_manager.add_log(task_id, f'æ£€æµ‹åˆ° {current_file_count} ä¸ªæ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…ä¸Šä¼ å®Œæˆ...')
                        else:
                            stable_count += 1
                        
                        # å¦‚æœæ–‡ä»¶æ•°é‡ç¨³å®šäº†3æ¬¡æ£€æŸ¥ï¼ˆ6ç§’ï¼‰ï¼Œè®¤ä¸ºä¸Šä¼ å®Œæˆ
                        if stable_count >= 3 and current_file_count > 0:
                            task_manager.add_log(task_id, f'æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {current_file_count} ä¸ªCSVæ–‡ä»¶, {len(uploaded_json_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶')
                            logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {current_file_count} ä¸ªCSVæ–‡ä»¶, {len(uploaded_json_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶")
                            break
                    
                    if waited_time % 10 == 0:  # æ¯10ç§’è®°å½•ä¸€æ¬¡
                        task_manager.add_log(task_id, f'ç­‰å¾…æ–‡ä»¶ä¸Šä¼ ä¸­... ({waited_time}/{max_wait_time}ç§’)')
                
                if waited_time >= max_wait_time:
                    task_manager.add_log(task_id, f'è­¦å‘Š: ç­‰å¾…æ–‡ä»¶ä¸Šä¼ è¶…æ—¶ï¼Œç»§ç»­å¤„ç†å·²ä¸Šä¼ çš„æ–‡ä»¶')
                    logger.warning(f"ç­‰å¾…æ–‡ä»¶ä¸Šä¼ è¶…æ—¶ï¼Œç»§ç»­å¤„ç†å·²ä¸Šä¼ çš„æ–‡ä»¶")
            
            # å¦‚æœtrain_filesä¸ºç©ºï¼Œç­‰å¾…æ–‡ä»¶ä¸Šä¼ å¹¶è‡ªåŠ¨åˆ†é…
            elif not train_files or len(train_files) == 0:
                task_manager.add_log(task_id, 'ç­‰å¾…è®­ç»ƒæ–‡ä»¶ä¸Šä¼ ...')
                logger.info(f"ç­‰å¾…è®­ç»ƒæ–‡ä»¶ä¸Šä¼ ...")
                
                # ç­‰å¾…æ–‡ä»¶ä¸Šä¼ ï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼Œæ¯2ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
                import time
                max_wait_time = 60
                wait_interval = 2
                waited_time = 0
                last_file_count = 0
                stable_count = 0  # æ–‡ä»¶æ•°é‡ç¨³å®šçš„æ¬¡æ•°ï¼ˆè¿ç»­3æ¬¡ä¸å˜è®¤ä¸ºä¸Šä¼ å®Œæˆï¼‰
                
                while waited_time < max_wait_time:
                    time.sleep(wait_interval)
                    waited_time += wait_interval
                    
                    # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰CSVæ–‡ä»¶
                    if training_data_dir.exists():
                        uploaded_csv_files = [f for f in training_data_dir.glob('*.csv') if f.is_file()]
                        uploaded_json_files = [f for f in training_data_dir.glob('*.json') if f.is_file()]
                        
                        current_file_count = len(uploaded_csv_files)
                        
                        # å¦‚æœæ–‡ä»¶æ•°é‡å‘ç”Ÿå˜åŒ–ï¼Œé‡ç½®ç¨³å®šè®¡æ•°å™¨
                        if current_file_count != last_file_count:
                            stable_count = 0
                            last_file_count = current_file_count
                            if current_file_count > 0:
                                task_manager.add_log(task_id, f'æ£€æµ‹åˆ° {current_file_count} ä¸ªæ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…ä¸Šä¼ å®Œæˆ...')
                        else:
                            stable_count += 1
                        
                        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œç­‰å¾…å…ƒæ•°æ®æ–‡ä»¶ä¸Šä¼ å®Œæˆ
                        if current_file_count > 0:
                            # ç­‰å¾…å…ƒæ•°æ®æ–‡ä»¶ä¸Šä¼ ï¼ˆæœ€å¤šå†ç­‰10ç§’ï¼‰
                            if len(uploaded_json_files) < len(uploaded_csv_files) and waited_time < max_wait_time - 10:
                                task_manager.add_log(task_id, f'ç­‰å¾…å…ƒæ•°æ®æ–‡ä»¶ä¸Šä¼ ... (å·²ä¸Šä¼  {len(uploaded_json_files)}/{len(uploaded_csv_files)})')
                                continue
                            
                            # å¦‚æœæ–‡ä»¶æ•°é‡ç¨³å®šäº†3æ¬¡æ£€æŸ¥ï¼ˆ6ç§’ï¼‰ï¼Œè®¤ä¸ºä¸Šä¼ å®Œæˆ
                            if stable_count >= 3:
                                # ä»ä¸Šä¼ çš„æ–‡ä»¶ä¸­æå–æ–‡ä»¶å
                                all_files = [f.name for f in uploaded_csv_files]
                                potential_train_files = []
                                potential_test_files = []
                                
                                for fname in all_files:
                                    # æ£€æŸ¥å¯¹åº”çš„å…ƒæ•°æ®æ–‡ä»¶
                                    meta_file = training_data_dir / fname.replace('.csv', '.json')
                                    if meta_file.exists():
                                        try:
                                            with open(meta_file, 'r', encoding='utf-8') as mf:
                                                meta_data = json.load(mf)
                                                tags_label = meta_data.get('tags_label', [])
                                                # æ£€æŸ¥æ ‡ç­¾ï¼šæ­£å¸¸æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œå¼‚å¸¸æ–‡ä»¶ç”¨äºæµ‹è¯•
                                                is_normal = False
                                                for label_tag in tags_label:
                                                    if isinstance(label_tag, dict) and 'value' in label_tag:
                                                        label_value = label_tag['value']
                                                        if label_value in ['æ­£å¸¸', 'normal', 'å¥åº·', 'healthy']:
                                                            is_normal = True
                                                            break
                                                
                                                if is_normal:
                                                    potential_train_files.append(fname)
                                                else:
                                                    potential_test_files.append(fname)
                                        except Exception as e:
                                            logger.warning(f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥ {fname}: {e}")
                                            # å¦‚æœæ— æ³•è¯»å–å…ƒæ•°æ®ï¼Œæ ¹æ®æ–‡ä»¶ååˆ¤æ–­
                                            if 'test' not in fname.lower():
                                                potential_train_files.append(fname)
                                            else:
                                                potential_test_files.append(fname)
                                    else:
                                        # æ²¡æœ‰å…ƒæ•°æ®æ–‡ä»¶ï¼Œæ ¹æ®æ–‡ä»¶ååˆ¤æ–­
                                        logger.warning(f"æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {fname.replace('.csv', '.json')}")
                                        if 'test' not in fname.lower():
                                            potential_train_files.append(fname)
                                        else:
                                            potential_test_files.append(fname)
                                
                                # åªæœ‰åœ¨train_filesä¸ºç©ºæ—¶æ‰è‡ªåŠ¨åˆ†é…
                                # å¦‚æœEdgeç«¯å·²ç»å‘é€äº†train_fileså’Œtest_filesï¼Œåº”è¯¥ä½¿ç”¨Edgeç«¯å‘é€çš„åˆ—è¡¨
                                if potential_train_files and (not train_files or len(train_files) == 0):
                                    train_files = potential_train_files
                                    config['train_files'] = train_files
                                    task_manager.add_log(task_id, f'è‡ªåŠ¨æ£€æµ‹åˆ° {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶å·²ä¸Šä¼ ï¼ˆæ ¹æ®æ ‡ç­¾è‡ªåŠ¨åˆ†é…ï¼‰')
                                
                                if potential_test_files and (not test_files or len(test_files) == 0):
                                    test_files = potential_test_files
                                    config['test_files'] = test_files
                                    if test_files:
                                        task_manager.add_log(task_id, f'è‡ªåŠ¨æ£€æµ‹åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶å·²ä¸Šä¼ ï¼ˆæ ¹æ®æ ‡ç­¾è‡ªåŠ¨åˆ†é…ï¼‰')
                                
                                # å¦‚æœEdgeç«¯å·²ç»å‘é€äº†train_fileså’Œtest_filesï¼Œè®°å½•å®ƒä»¬
                                if train_files and len(train_files) > 0:
                                    task_manager.add_log(task_id, f'ä½¿ç”¨Edgeç«¯å‘é€çš„è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {len(train_files)} ä¸ªæ–‡ä»¶')
                                if test_files and len(test_files) > 0:
                                    task_manager.add_log(task_id, f'ä½¿ç”¨Edgeç«¯å‘é€çš„æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {len(test_files)} ä¸ªæ–‡ä»¶')
                                
                                logger.info(f"æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶, {len(uploaded_json_files)} ä¸ªå…ƒæ•°æ®æ–‡ä»¶")
                                break
                    
                    if waited_time % 10 == 0:  # æ¯10ç§’è®°å½•ä¸€æ¬¡
                        task_manager.add_log(task_id, f'ç­‰å¾…æ–‡ä»¶ä¸Šä¼ ä¸­... ({waited_time}/{max_wait_time}ç§’)')
                
                if not train_files or len(train_files) == 0:
                    raise ValueError("ç­‰å¾…è¶…æ—¶ï¼šè®­ç»ƒæ–‡ä»¶æœªä¸Šä¼ å®Œæˆï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ä¸Šä¼ æ˜¯å¦æˆåŠŸ")
            
            # å¦‚æœ conditions ä¸ºç©ºï¼Œå°è¯•ä»å…ƒæ•°æ®æ–‡ä»¶ä¸­æå–
            if not config.get('conditions') or len(config.get('conditions', {})) == 0:
                task_manager.add_log(task_id, 'ä»å…ƒæ•°æ®æ–‡ä»¶ä¸­æå–å·¥å†µä¿¡æ¯...')
                conditions = {}
                
                # ä»æ‰€æœ‰è®­ç»ƒæ–‡ä»¶çš„å…ƒæ•°æ®ä¸­æå–å·¥å†µkey
                for filename in train_files:
                    meta_file_path = training_data_dir / filename.replace('.csv', '.json')
                    if meta_file_path.exists():
                        try:
                            with open(meta_file_path, 'r', encoding='utf-8') as f:
                                meta_data = json.load(f)
                                tags_condition = meta_data.get('tags_condition', [])
                                for cond in tags_condition:
                                    if isinstance(cond, dict) and 'key' in cond:
                                        key = cond['key']
                                        if key not in conditions:
                                            conditions[key] = []
                                        value = cond.get('value', '')
                                        if value and value not in conditions[key]:
                                            conditions[key].append(value)
                        except Exception as e:
                            logger.warning(f"è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                
                if conditions:
                    config['conditions'] = conditions
                    task_manager.add_log(task_id, f'ä»å…ƒæ•°æ®æå–åˆ°å·¥å†µ: {list(conditions.keys())}')
                    logger.info(f"ä»å…ƒæ•°æ®æå–åˆ°å·¥å†µ: {conditions}")
                else:
                    task_manager.add_log(task_id, 'è­¦å‘Š: æœªæ‰¾åˆ°å·¥å†µä¿¡æ¯ï¼Œå°†ä¸ä½¿ç”¨å·¥å†µç‰¹å¾')
                    logger.warning("æœªæ‰¾åˆ°å·¥å†µä¿¡æ¯ï¼Œå°†ä¸ä½¿ç”¨å·¥å†µç‰¹å¾")
            
            train_data, val_data, feature_dim = _process_condition_filtered_data(
                config, task_id, task_manager, model_type
            )
            
            # åˆ›å»ºprocessorå¯¹è±¡ï¼ˆç”¨äºåç»­ä¿å­˜scalerç­‰æ“ä½œï¼‰
            if model_type == 'lstm_autoencoder':
                processor = LSTMAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            elif model_type == 'cnn_1d_autoencoder':
                processor = CNN1DAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            else:
                processor = LSTMPredictorDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    prediction_horizon=config.get('prediction_horizon', 1),
                    normalize=True
                )
        else:
            # åŸæœ‰æ¨¡å¼ï¼šå•ä¸ªæ–‡ä»¶å¤„ç†
            # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_file = config.get('dataset_file')
            if not dataset_file:
                raise ValueError("No dataset file provided")
            
            # 1. æŸ¥æ‰¾è®­ç»ƒæ•°æ®æ–‡ä»¶
            data_path = None
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»ä¸Šä¼ åˆ°äº‘ç«¯è®­ç»ƒæ•°æ®ç›®å½•
            if dataset_file in uploaded_data_files:
                data_path = Path(uploaded_data_files[dataset_file]['path'])
                task_manager.add_log(task_id, f'Using uploaded training data: {uploaded_data_files[dataset_file]["saved_name"]}')
            else:
                # å°è¯•åœ¨è®­ç»ƒæ•°æ®ç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶ (å¼‚å¸¸æ£€æµ‹: cloud/data/ad)
                training_data_dir = Path('data') / 'ad'
                possible_paths = [
                    training_data_dir / dataset_file,  # äº‘ç«¯å¼‚å¸¸æ£€æµ‹è®­ç»ƒæ•°æ®ç›®å½•
                    Path('data') / dataset_file,  # äº‘ç«¯é€šç”¨dataç›®å½•
                    Path(dataset_file)  # ç›¸å¯¹è·¯å¾„
                ]
                
                for path in possible_paths:
                    if path.exists():
                        data_path = path
                        task_manager.add_log(task_id, f'Found training data at: {path}')
                        break
            
            if data_path is None or not data_path.exists():
                # æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæç¤ºéœ€è¦ä¸Šä¼ 
                task_manager.update_task_status(task_id, 'failed', 'Training data not found, please upload from edge')
                task_manager.add_log(task_id, f'Training data file not found: {dataset_file}')
                task_manager.add_log(task_id, 'Available files: ' + ', '.join(uploaded_data_files.keys()))
                
                raise FileNotFoundError(
                    f"Training data file '{dataset_file}' not available on cloud server. "
                    f"Please upload the data from edge server first. "
                    f"Available files: {list(uploaded_data_files.keys())}"
                )
            
            # å¤„ç†æ•°æ®ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„æ•°æ®å¤„ç†å™¨ï¼‰
            if model_type == 'lstm_autoencoder':
                # ä½¿ç”¨LSTM Autoencoderæ•°æ®å¤„ç†å™¨
                processor = LSTMAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
                
                # å¤„ç†æ•°æ®ï¼ˆè‡ªç¼–ç å™¨è¾“å…¥è¾“å‡ºç›¸åŒï¼‰
                train_data, val_data = processor.process_pipeline(
                    str(data_path),
                    train_ratio=config.get('train_ratio', 0.8)
                )
            elif model_type == 'cnn_1d_autoencoder':
                # ä½¿ç”¨1D CNN Autoencoderæ•°æ®å¤„ç†å™¨
                processor = CNN1DAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
                
                # å¤„ç†æ•°æ®ï¼ˆè‡ªç¼–ç å™¨è¾“å…¥è¾“å‡ºç›¸åŒï¼‰
                train_data, val_data = processor.process_pipeline(
                    str(data_path),
                    train_ratio=config.get('train_ratio', 0.8)
                )
            else:
                # ä½¿ç”¨LSTM Predictoræ•°æ®å¤„ç†å™¨ï¼ˆé»˜è®¤ï¼‰
                processor = LSTMPredictorDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    prediction_horizon=config.get('prediction_horizon', 1),
                    normalize=True
                )
                
                # è®¡ç®—train_ratioï¼šä»validation_splitè½¬æ¢ä¸ºtrain_ratio
                validation_split = config.get('validation_split', 0.2)  # é»˜è®¤éªŒè¯é›†20%
                train_ratio = 1.0 - validation_split
                logger.info(f"ä½¿ç”¨validation_split={validation_split}, è®¡ç®—train_ratio={train_ratio}")
                
                # å¤„ç†æ•°æ®ï¼ˆé¢„æµ‹å™¨è¾“å…¥è¾“å‡ºä¸åŒï¼‰
                train_data, val_data = processor.process_pipeline(
                    str(data_path),
                    train_ratio=train_ratio
                )
            
            # è·å–ç‰¹å¾ç»´åº¦
            feature_dim = train_data.sequences.shape[2]
            # åˆ›å»ºprocessorå¯¹è±¡ï¼ˆç”¨äºåç»­ä¿å­˜scalerç­‰æ“ä½œï¼‰
            if model_type == 'lstm_autoencoder':
                processor = LSTMAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            elif model_type == 'cnn_1d_autoencoder':
                processor = CNN1DAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            else:
                processor = LSTMPredictorDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    prediction_horizon=config.get('prediction_horizon', 1),
                    normalize=True
                )
        
        task_manager.update_task_status(task_id, 'training', f'Data loaded: {len(train_data.sequences)} training samples')
        task_manager.add_log(task_id, f'Training dataset: {len(train_data.sequences)} samples')
        if val_data:
            task_manager.add_log(task_id, f'Validation dataset: {len(val_data.sequences)} samples')
        
        # 2. æ„å»ºæ¨¡å‹ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„æ„å»ºå™¨ï¼‰
        task_manager.update_task_status(task_id, 'training', 'Building model architecture...')
        task_manager.add_log(task_id, f'Building {model_type} model')
        
        # è·å–åºåˆ—é•¿åº¦ï¼ˆç”¨äºæ—¥å¿—å’Œæ¨¡å‹æ„å»ºï¼‰
        sequence_length = config.get('sequence_length', 50)
        input_shape = (sequence_length, feature_dim)
        
        if model_type == 'lstm_autoencoder':
            # æ„å»ºLSTM Autoencoderæ¨¡å‹
            model = LSTMAutoencoderModelBuilder.create_model(
                'lstm_autoencoder',
                input_shape=input_shape,
                hidden_size=config.get('hidden_units', 128),
                num_layers=config.get('num_layers', 2),
                bottleneck_dim=config.get('bottleneck_dim', 64),
                dropout=config.get('dropout', 0.1)
            )
        elif model_type == 'cnn_1d_autoencoder':
            # æ„å»º1D CNN Autoencoderæ¨¡å‹
            model = CNN1DAutoencoderModelBuilder.create_model(
                'cnn_1d_autoencoder',
                input_shape=input_shape,
                num_filters=config.get('num_filters', 64),
                kernel_size=config.get('kernel_size', 3),
                bottleneck_dim=config.get('bottleneck_dim', 64),
                num_conv_layers=config.get('num_conv_layers', config.get('num_layers', 3)),
                dropout=config.get('dropout', 0.1),
                activation=config.get('activation', 'relu')
            )
        else:
            # æ„å»ºLSTM Predictoræ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
            hidden_units = config.get('hidden_units', 128)
            num_layers = config.get('num_layers', 2)
            dropout = config.get('dropout', 0.1)
            activation = config.get('activation', 'tanh')
            
            model = LSTMPredictorModelBuilder.build_lstm_predictor(
                input_shape=input_shape,
                hidden_units=hidden_units,
                num_layers=num_layers,
                dropout=dropout,
                activation=activation
            )
        
        # 3. è®­ç»ƒæ¨¡å‹ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„è®­ç»ƒå™¨ï¼‰
        # è·å–è®­ç»ƒè½®æ•°ï¼ˆæå‰è·å–ï¼Œç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        num_epochs = config.get('epochs', 50)
        
        task_manager.update_task_status(task_id, 'training', 'Starting model training...')
        task_manager.add_log(task_id, f'Starting {model_type} model training')
        logger.info(f"å¼€å§‹æ¨¡å‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        
        if model_type == 'lstm_autoencoder':
            trainer = LSTMAutoencoderTrainer(
                model=model,
                learning_rate=config.get('learning_rate', 0.001)
            )
        elif model_type == 'cnn_1d_autoencoder':
            trainer = CNN1DAutoencoderTrainer(
                model=model,
                learning_rate=config.get('learning_rate', 0.001)
            )
        else:
            learning_rate = config.get('learning_rate', 0.001)
            weight_decay = config.get('weight_decay', 0.0001)
            
            trainer = LSTMPredictorTrainer(
                model=model,
                learning_rate=learning_rate,
                weight_decay=weight_decay
            )
        
        # åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨
        from torch.utils.data import TensorDataset, DataLoader
        
        def create_dataloader(sequences, targets, batch_size, shuffle=True):
            """åˆ›å»ºPyTorch DataLoader"""
            sequences_tensor = torch.from_numpy(sequences.astype(np.float32))
            targets_tensor = torch.from_numpy(targets.astype(np.float32))
            dataset = TensorDataset(sequences_tensor, targets_tensor)
            return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)
        
        train_dataset = create_dataloader(
            train_data.sequences,
            train_data.targets,
            batch_size=config.get('batch_size', 32),
            shuffle=True
        )
        
        val_dataset = create_dataloader(
            val_data.sequences,
            val_data.targets,
            batch_size=config.get('batch_size', 32),
            shuffle=False
        ) if val_data is not None else None
        
        # è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ä»¥æ”¯æŒè¿›åº¦å›è°ƒ
        for epoch in range(num_epochs):
            # æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
            current_task = task_manager.get_task(task_id)
            if current_task and current_task.status == 'cancelled':
                task_manager.update_task_status(task_id, 'cancelled', 'Training was cancelled')
                return
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = trainer.train_epoch(train_dataset, epoch)
            
            # éªŒè¯ï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
            val_loss = None
            if val_dataset is not None:
                val_loss = trainer.validate(val_dataset)
            
            # è®°å½•epochå®Œæˆæ—¥å¿—ï¼ˆåˆå¹¶ä¸ºä¸€è¡Œï¼‰
            epoch_log = f'Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}'
            if val_loss is not None:
                epoch_log += f', Val: {val_loss:.6f}'
            # æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochè®°å½•ä¸€æ¬¡
            if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:
                logger.info(f"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}" + (f", Val: {val_loss:.6f}" if val_loss is not None else ""))
            
            # æ›´æ–°è®­ç»ƒè¿›åº¦
            progress = ((epoch + 1) / num_epochs) * 100
            task_manager.update_task_status(
                task_id, 
                'training',
                epoch_log,
                round(progress, 2),
                epoch + 1,
                train_loss,
                val_loss
            )
            task_manager.add_log(task_id, epoch_log)
        
        # 4. ä¿å­˜æ¨¡å‹
        task_manager.update_task_status(task_id, 'training', 'Saving trained model...', current_epoch=num_epochs)
        task_manager.add_log(task_id, f'Saving {model_type} model and artifacts')
        # ä¿å­˜æ¨¡å‹
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½• - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å­ç›®å½•
        if model_type == 'lstm_autoencoder':
            model_dir = Path(f'models/anomaly_detection/lstm_autoencoder/{task_id}')
        elif model_type == 'cnn_1d_autoencoder':
            model_dir = Path(f'models/anomaly_detection/cnn_1d_autoencoder/{task_id}')
        else:
            model_dir = Path(f'models/anomaly_detection/lstm_prediction/{task_id}')
            
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        model_path = model_dir / 'model.pth'
        trainer.save_model(str(model_path))
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨ï¼ˆä»è®­ç»ƒæ•°æ®ç›®å½•å¤åˆ¶ï¼Œæˆ–ä»processorè·å–ï¼‰
        scaler_path = model_dir / 'scaler.pkl'
        training_data_dir = Path('data') / 'ad' / task_id
        training_scaler_path = training_data_dir / 'scaler.pkl'
        
        if training_scaler_path.exists():
            # ä»è®­ç»ƒæ•°æ®ç›®å½•å¤åˆ¶scalerï¼ˆå·¥å†µç­›é€‰æ¨¡å¼ï¼‰
            import shutil
            shutil.copy2(training_scaler_path, scaler_path)
            logger.info(f"å·²å¤åˆ¶scaler: {training_scaler_path} -> {scaler_path}")
        elif 'processor' in locals() and hasattr(processor, 'scaler') and processor.scaler is not None:
            # ä»processorä¿å­˜scalerï¼ˆå…¼å®¹æ—§æ¨¡å¼ï¼‰
            import pickle
            with open(scaler_path, 'wb') as f:
                pickle.dump(processor.scaler, f)
        
        # ä¿å­˜é…ç½®
        config_path = model_dir / 'config.json'
        # è·å–sequence_lengthï¼ˆä»processoræˆ–configï¼‰
        seq_len = config.get('sequence_length', 50)
        if 'processor' in locals() and hasattr(processor, 'sequence_length'):
            seq_len = processor.sequence_length
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({
                **config,
                'feature_dim': feature_dim,
                'sequence_length': seq_len,
                'model_path': str(model_path),
                'scaler_path': str(scaler_path),
                'trained_at': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        # 5. åœ¨éªŒè¯é›†ä¸Šè®¡ç®—é˜ˆå€¼ï¼ˆè®­ç»ƒå®Œæˆåï¼‰
        task_manager.update_task_status(task_id, 'training', 'Calculating threshold on validation set...', 90)
        task_manager.add_log(task_id, 'å¼€å§‹åœ¨éªŒè¯é›†ä¸Šè®¡ç®—é˜ˆå€¼')
        
        threshold = None
        threshold_file = model_dir / 'threshold.json'
        
        try:
            # åŠ è½½éªŒè¯é›†æ•°æ®
            dev_data_path = training_data_dir / 'dev.npz'
            if dev_data_path.exists():
                dev_data = np.load(dev_data_path, allow_pickle=True)
                dev_sequences = dev_data['sequences']
                dev_targets = dev_data['targets']
                
                if len(dev_sequences) > 0:
                    # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
                    model.eval()
                    dev_predictions = []
                    batch_size = config.get('batch_size', 32)
                    
                    with torch.no_grad():
                        for i in range(0, len(dev_sequences), batch_size):
                            batch_sequences = dev_sequences[i:i+batch_size]
                            batch_tensor = torch.from_numpy(batch_sequences.astype(np.float32)).to(device)
                            pred = model(batch_tensor)
                            dev_predictions.append(pred.cpu().numpy())
                    
                    dev_predictions = np.vstack(dev_predictions)
                    
                    # è®¡ç®—éªŒè¯é›†è¯¯å·®ï¼ˆé€æ ·æœ¬è®¡ç®—ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
                    dev_errors = []
                    for i in range(len(dev_targets)):
                        error = np.mean(np.square(dev_targets[i] - dev_predictions[i]))
                        dev_errors.append(error)
                    dev_errors = np.array(dev_errors)
                    
                    # æ ¹æ®é…ç½®çš„é˜ˆå€¼æ–¹æ³•è®¡ç®—é˜ˆå€¼
                    threshold_method = config.get('threshold_method', 'percentile')
                    
                    if threshold_method == 'percentile':
                        percentile = config.get('threshold_percentile', 95.0)
                        threshold = float(np.percentile(dev_errors, percentile))
                        task_manager.add_log(task_id, f'ä½¿ç”¨{percentile}åˆ†ä½æ•°è®¡ç®—é˜ˆå€¼: {threshold:.6f}')
                    elif threshold_method == '3-sigma':
                        mean_error = np.mean(dev_errors)
                        std_error = np.std(dev_errors)
                        threshold = float(mean_error + 3 * std_error)
                        task_manager.add_log(task_id, f'ä½¿ç”¨3-sigmaæ–¹æ³•è®¡ç®—é˜ˆå€¼: {threshold:.6f} (mean={mean_error:.6f}, std={std_error:.6f})')
                    elif threshold_method == 'contamination':
                        contamination = config.get('threshold_contamination', 0.01)
                        threshold = float(np.percentile(dev_errors, (1 - contamination) * 100))
                        task_manager.add_log(task_id, f'ä½¿ç”¨contaminationæ–¹æ³•è®¡ç®—é˜ˆå€¼: {threshold:.6f} (contamination={contamination})')
                    else:
                        # é»˜è®¤ä½¿ç”¨95åˆ†ä½æ•°
                        threshold = float(np.percentile(dev_errors, 95.0))
                        task_manager.add_log(task_id, f'ä½¿ç”¨é»˜è®¤95åˆ†ä½æ•°è®¡ç®—é˜ˆå€¼: {threshold:.6f}')
                    
                    # ä¿å­˜é˜ˆå€¼
                    threshold_data = {
                        'threshold': threshold,
                        'threshold_method': threshold_method,
                        'threshold_params': {
                            'percentile': config.get('threshold_percentile', 95.0) if threshold_method == 'percentile' else None,
                            'contamination': config.get('threshold_contamination', 0.01) if threshold_method == 'contamination' else None,
                        },
                        'validation_error_stats': {
                            'mean': float(np.mean(dev_errors)),
                            'std': float(np.std(dev_errors)),
                            'min': float(np.min(dev_errors)),
                            'max': float(np.max(dev_errors)),
                            'percentiles': {
                                'p50': float(np.percentile(dev_errors, 50)),
                                'p75': float(np.percentile(dev_errors, 75)),
                                'p90': float(np.percentile(dev_errors, 90)),
                                'p95': float(np.percentile(dev_errors, 95)),
                                'p99': float(np.percentile(dev_errors, 99))
                            }
                        },
                        'calculated_at': datetime.now().isoformat()
                    }
                    
                    with open(threshold_file, 'w', encoding='utf-8') as f:
                        json.dump(threshold_data, f, indent=2, ensure_ascii=False)
                    
                    logger.info(f'é˜ˆå€¼å·²ä¿å­˜: {threshold}')
                    task_manager.add_log(task_id, f'âœ… é˜ˆå€¼è®¡ç®—å®Œæˆ: {threshold:.6f}')
                else:
                    task_manager.add_log(task_id, 'âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡é˜ˆå€¼è®¡ç®—')
            else:
                task_manager.add_log(task_id, 'âš ï¸ éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡é˜ˆå€¼è®¡ç®—')
        except Exception as e:
            logger.warning(f'é˜ˆå€¼è®¡ç®—å¤±è´¥: {e}', exc_info=True)
            task_manager.add_log(task_id, f'âš ï¸ é˜ˆå€¼è®¡ç®—å¤±è´¥: {str(e)}')
        
        # 6. å¦‚æœæä¾›äº†æµ‹è¯•é›†ï¼Œè¿›è¡Œè¯„ä¼°
        evaluation_results = None
        test_files = config.get('test_files', [])
        test_file = config.get('test_file')  # å…¼å®¹æ—§æ¨¡å¼
        training_data_dir = Path('data') / 'ad' / task_id
        
        # å·¥å†µç­›é€‰æ¨¡å¼ï¼šä»ä¿å­˜çš„test.npzæ–‡ä»¶åŠ è½½
        if dataset_mode == 'condition_filtered' and test_files:
            task_manager.update_task_status(task_id, 'training', 'Evaluating model on test set...', 95)
            task_manager.add_log(task_id, f'å¼€å§‹ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹ï¼ˆä»ä¿å­˜çš„test.npzæ–‡ä»¶åŠ è½½ï¼‰')
            
            try:
                test_data_path = training_data_dir / 'test.npz'
                if test_data_path.exists():
                    evaluation_results = _evaluate_from_npz(
                        model=model,
                        test_data_path=test_data_path,
                        model_type=model_type,
                        config=config,
                        model_dir=model_dir,
                        task_id=task_id
                    )
                    
                    if evaluation_results:
                        task_manager.add_log(task_id, f'âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ')
                        task_manager.add_log(task_id, f'å¹³å‡é‡æ„è¯¯å·®: {evaluation_results.get("mean_error", "N/A"):.6f}')
                        task_manager.add_log(task_id, f'è¯¯å·®æ ‡å‡†å·®: {evaluation_results.get("std_error", "N/A"):.6f}')
                        logger.info(f'æµ‹è¯•é›†è¯„ä¼°å®Œæˆ: mean_error={evaluation_results.get("mean_error")}')
                else:
                    task_manager.add_log(task_id, f'âš ï¸ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}')
            except Exception as e:
                logger.warning(f'æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}')
                task_manager.add_log(task_id, f'âš ï¸ æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {str(e)}')
        elif test_file:
            # æ—§æ¨¡å¼ï¼šä»CSVæ–‡ä»¶åŠ è½½
            task_manager.update_task_status(task_id, 'training', 'Evaluating model on test set...', 95)
            task_manager.add_log(task_id, f'å¼€å§‹ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹')
            
            try:
                evaluation_results = _evaluate_anomaly_detection_model(
                    model=model,
                    processor=processor,
                    test_file=test_file,
                    model_type=model_type,
                    config=config,
                    model_dir=model_dir,
                    task_id=task_id
                )
                
                if evaluation_results:
                    task_manager.add_log(task_id, f'âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ')
                    task_manager.add_log(task_id, f'å¹³å‡é‡æ„è¯¯å·®: {evaluation_results.get("mean_error", "N/A"):.6f}')
                    task_manager.add_log(task_id, f'è¯¯å·®æ ‡å‡†å·®: {evaluation_results.get("std_error", "N/A"):.6f}')
                    logger.info(f'æµ‹è¯•é›†è¯„ä¼°å®Œæˆ: mean_error={evaluation_results.get("mean_error")}')
            except Exception as e:
                logger.warning(f'æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}')
                task_manager.add_log(task_id, f'âš ï¸ æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {str(e)}')
        
        # è®­ç»ƒå®Œæˆåé€šçŸ¥Edgeç«¯ä¸‹è½½æ¨¡å‹
        try:
            _notify_edge_model_ready(task_id, model_dir.parent.name)
            logger.info(f"å·²é€šçŸ¥Edgeç«¯æ¨¡å‹å°±ç»ª: {task_id}")
        except Exception as e:
            logger.warning(f"é€šçŸ¥Edgeç«¯å¤±è´¥: {e}")
        
        # è®­ç»ƒå®Œæˆ - ä¸è‡ªåŠ¨è®¡ç®—é˜ˆå€¼ï¼Œç­‰å¾…ç”¨æˆ·ç‚¹å‡»
        completion_message = 'Training completed successfully'
        if evaluation_results:
            completion_message += f' (Test MAE: {evaluation_results.get("mean_error", 0):.6f})'
        
        task_manager.update_task_status(
            task_id, 
            'completed', 
            completion_message,
            100,
            num_epochs  # ä¼ é€’æ€»epochæ•°ä½œä¸ºcurrent_epoch
        )
        task_manager.update_model_save_path(task_id, str(model_path))
        task_manager.update_scaler_path(task_id, str(scaler_path))
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°ä»»åŠ¡
        if evaluation_results:
            task = task_manager.get_task(task_id)
            if task:
                task.evaluation_results = evaluation_results
        
        logger.info(f'âœ… {model_type} è®­ç»ƒå®Œæˆ | æ¨¡å‹: {model_path} | æ ‡å‡†åŒ–å™¨: {scaler_path}')
        
        # è¿”å›æˆåŠŸç»“æœ
        return {
            'success': True,
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'task_id': task_id,
            'message': 'Training completed successfully'
        }
        
    except Exception as e:
        error_msg = f"Training failed: {str(e)}"
        logger.error(f"Training error for task {task_id}: {e}", exc_info=True)
        
        task_manager.update_task_status(task_id, 'failed', error_msg)
        logger.error(f'ERROR: {error_msg}')
        
        # è¿”å›å¤±è´¥ç»“æœ
        return {
            'success': False,
            'error': error_msg,
            'task_id': task_id
        }


def _evaluate_from_npz(
    model,
    test_data_path: Path,
    model_type: str,
    config: dict,
    model_dir: Path,
    task_id: str
) -> dict:
    """
    ä»ä¿å­˜çš„test.npzæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®å¹¶è¯„ä¼°æ¨¡å‹
    
    æµ‹è¯•æ•°æ®åŒ…å«æ ‡ç­¾ä¿¡æ¯ï¼ˆä»å…ƒæ•°æ®æ–‡ä»¶è¯»å–ï¼‰ï¼š
    - labels: æ¯ä¸ªåºåˆ—çš„æ ‡ç­¾ï¼ˆ0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸ï¼‰
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_data_path: test.npzæ–‡ä»¶è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        config: é…ç½®å­—å…¸
        model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        task_id: ä»»åŠ¡ID
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«è¯¯å·®ç»Ÿè®¡å’Œåˆ†ç±»æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
    """
    try:
        logger.info(f"ä»NPZæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = np.load(test_data_path, allow_pickle=True)
        test_sequences = test_data['sequences']
        test_targets = test_data['targets']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾ä¿¡æ¯ï¼ˆä¼˜å…ˆä»npzçš„labelså­—æ®µè¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»åºåˆ—æœ€åä¸€åˆ—æå–ï¼‰
        has_labels = 'labels' in test_data
        if has_labels:
            test_labels = test_data['labels']
            logger.info(f"ä»NPZæ–‡ä»¶åŠ è½½æ ‡ç­¾: labelså½¢çŠ¶={test_labels.shape}, æ ‡ç­¾å€¼èŒƒå›´=[{test_labels.min()}, {test_labels.max()}]")
            logger.info(f"æ ‡ç­¾ç»Ÿè®¡: 0çš„æ•°é‡={np.sum(test_labels == 0)}, 1çš„æ•°é‡={np.sum(test_labels == 1)}, å…¶ä»–å€¼={np.sum((test_labels != 0) & (test_labels != 1))}")
        else:
            # å¦‚æœnpzä¸­æ²¡æœ‰labelså­—æ®µï¼Œå°è¯•ä»åºåˆ—çš„æœ€åä¸€åˆ—æå–æ ‡ç­¾
            # æ³¨æ„ï¼šå¦‚æœåºåˆ—å·²ç»ç§»é™¤äº†æ ‡ç­¾åˆ—ï¼Œè¿™é‡Œä¼šå¤±è´¥ï¼Œæ‰€ä»¥ä¼˜å…ˆä½¿ç”¨npzä¸­çš„labelså­—æ®µ
            test_labels = None
            logger.warning("NPZæ–‡ä»¶ä¸­æ²¡æœ‰labelså­—æ®µ")
        
        if test_labels is not None:
            logger.info(f"æµ‹è¯•æ•°æ®: {len(test_sequences)} ä¸ªåºåˆ—, æ­£å¸¸æ ·æœ¬: {np.sum(test_labels == 0)}, å¼‚å¸¸æ ·æœ¬: {np.sum(test_labels == 1)}")
        else:
            logger.info(f"æµ‹è¯•æ•°æ®: {len(test_sequences)} ä¸ªåºåˆ—ï¼ˆæ— æ ‡ç­¾ä¿¡æ¯ï¼‰")
        
        # è¿›è¡Œé¢„æµ‹
        model.eval()
        predictions = []
        batch_size = config.get('batch_size', 32)
        device = next(model.parameters()).device
        
        with torch.no_grad():
            for i in range(0, len(test_sequences), batch_size):
                batch_sequences = test_sequences[i:i+batch_size]
                batch_tensor = torch.from_numpy(batch_sequences.astype(np.float32)).to(device)
                
                if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
                    # è‡ªç¼–ç å™¨ï¼šé¢„æµ‹é‡æ„ç»“æœ
                    pred = model(batch_tensor)
                    predictions.append(pred.cpu().numpy())
                else:
                    # é¢„æµ‹å™¨ï¼šé¢„æµ‹æœªæ¥å€¼
                    pred = model(batch_tensor)
                    predictions.append(pred.cpu().numpy())
        
        predictions = np.vstack(predictions)
        
        # ç¡®ä¿predictionså’Œtest_targetsçš„å½¢çŠ¶åŒ¹é…
        logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: sequences={test_sequences.shape}, targets={test_targets.shape}, predictions={predictions.shape}")
        
        # è®¡ç®—é‡æ„è¯¯å·®ï¼ˆè‡ªç¼–ç å™¨ï¼‰æˆ–é¢„æµ‹è¯¯å·®ï¼ˆé¢„æµ‹å™¨ï¼‰
        if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
            # è‡ªç¼–ç å™¨ï¼šè®¡ç®—é‡æ„è¯¯å·®
            # test_targetså’Œpredictionséƒ½æ˜¯ (n_samples, sequence_length, feature_dim)
            # éœ€è¦é€æ ·æœ¬è®¡ç®—MSEï¼Œé¿å…å†…å­˜æº¢å‡º
            errors = []
            for i in range(len(test_targets)):
                error = np.mean(np.square(test_targets[i] - predictions[i]))
                errors.append(error)
            errors = np.array(errors)
        else:
            # é¢„æµ‹å™¨ï¼šè®¡ç®—é¢„æµ‹è¯¯å·®
            # test_targetså’Œpredictionséƒ½æ˜¯ (n_samples, prediction_horizon, feature_dim)
            # éœ€è¦é€æ ·æœ¬è®¡ç®—MSEï¼Œé¿å…å†…å­˜æº¢å‡º
            errors = []
            for i in range(len(test_targets)):
                error = np.mean(np.square(test_targets[i] - predictions[i]))
                errors.append(error)
            errors = np.array(errors)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_error = float(np.mean(errors))
        std_error = float(np.std(errors))
        min_error = float(np.min(errors))
        max_error = float(np.max(errors))
        
        percentiles = {
            'p50': float(np.percentile(errors, 50)),
            'p75': float(np.percentile(errors, 75)),
            'p90': float(np.percentile(errors, 90)),
            'p95': float(np.percentile(errors, 95)),
            'p99': float(np.percentile(errors, 99))
        }
        
        evaluation_results = {
            'mean_error': mean_error,
            'std_error': std_error,
            'min_error': min_error,
            'max_error': max_error,
            'percentiles': percentiles,
            'test_samples': len(errors),
            'has_labels': has_labels
        }
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œè®¡ç®—åˆ†ç±»æ€§èƒ½æŒ‡æ ‡
        if has_labels and test_labels is not None:
            # åŠ è½½é˜ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            threshold = None
            threshold_file = model_dir / 'threshold.json'
            if threshold_file.exists():
                try:
                    with open(threshold_file, 'r', encoding='utf-8') as f:
                        threshold_data = json.load(f)
                        threshold = threshold_data.get('threshold')
                        logger.info(f"ä½¿ç”¨å·²ä¿å­˜çš„é˜ˆå€¼: {threshold}")
                except Exception as e:
                    logger.warning(f"è¯»å–é˜ˆå€¼æ–‡ä»¶å¤±è´¥: {e}")
            
            # å¦‚æœæ²¡æœ‰é˜ˆå€¼ï¼Œä½¿ç”¨95åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            if threshold is None:
                threshold = percentiles['p95']
                logger.info(f"ä½¿ç”¨95åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼: {threshold}")
            
            # åŸºäºé˜ˆå€¼é¢„æµ‹å¼‚å¸¸
            predicted_labels = (errors > threshold).astype(int)
            
            # è®¡ç®—åˆ†ç±»æ€§èƒ½æŒ‡æ ‡
            true_positive = np.sum((predicted_labels == 1) & (test_labels == 1))
            true_negative = np.sum((predicted_labels == 0) & (test_labels == 0))
            false_positive = np.sum((predicted_labels == 1) & (test_labels == 0))
            false_negative = np.sum((predicted_labels == 0) & (test_labels == 1))
            
            accuracy = (true_positive + true_negative) / len(test_labels) if len(test_labels) > 0 else 0
            precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
            recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬çš„è¯¯å·®ç»Ÿè®¡
            normal_mask = test_labels == 0
            anomaly_mask = test_labels == 1
            
            normal_errors = errors[normal_mask] if np.any(normal_mask) else np.array([])
            anomaly_errors = errors[anomaly_mask] if np.any(anomaly_mask) else np.array([])
            
            evaluation_results.update({
                'threshold': float(threshold),
                'classification': {
                    'accuracy': float(accuracy),
                    'precision': float(precision),
                    'recall': float(recall),
                    'f1_score': float(f1_score),
                    'confusion_matrix': {
                        'true_positive': int(true_positive),
                        'true_negative': int(true_negative),
                        'false_positive': int(false_positive),
                        'false_negative': int(false_negative)
                    }
                },
                'normal_samples': int(np.sum(test_labels == 0)),
                'anomaly_samples': int(np.sum(test_labels == 1)),
                'error_by_label': {
                    'normal_error_mean': float(np.mean(normal_errors)) if len(normal_errors) > 0 else None,
                    'normal_error_std': float(np.std(normal_errors)) if len(normal_errors) > 0 else None,
                    'anomaly_error_mean': float(np.mean(anomaly_errors)) if len(anomaly_errors) > 0 else None,
                    'anomaly_error_std': float(np.std(anomaly_errors)) if len(anomaly_errors) > 0 else None
                }
            })
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_result_path = model_dir / f'{task_id}_evaluation_results.json'
        with open(eval_result_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_result_path}")
        
        return evaluation_results
        
    except Exception as e:
        logger.error(f"ä»NPZæ–‡ä»¶è¯„ä¼°æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
        raise


def _evaluate_anomaly_detection_model(
    model,
    processor,
    test_file: str,
    model_type: str,
    config: dict,
    model_dir: Path,
    task_id: str
) -> dict:
    """
    è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆæ—§æ¨¡å¼ï¼šä»CSVæ–‡ä»¶åŠ è½½ï¼‰
    
    æµ‹è¯•æ•°æ®æ ¼å¼ï¼šæœ€åä¸€åˆ—æ˜¯æ ‡ç­¾åˆ—ï¼Œ0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸
    
    è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š
    - é‡æ„è¯¯å·®ç»Ÿè®¡ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€æœ€å¤§å€¼ã€åˆ†ä½æ•°ï¼‰
    - åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰- å¦‚æœæµ‹è¯•é›†æœ‰æ ‡ç­¾
    - æ­£å¸¸/å¼‚å¸¸æ ·æœ¬çš„è¯¯å·®åˆ†å¸ƒå¯¹æ¯”
    """
    try:
        import pandas as pd
        
        logger.info(f"å¼€å§‹è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹: {model_type}, æµ‹è¯•æ–‡ä»¶: {test_file}")
        
        # æŸ¥æ‰¾æµ‹è¯•æ•°æ®æ–‡ä»¶
        test_data_path = None
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶ä¸­
        if test_file in uploaded_data_files:
            test_data_path = Path(uploaded_data_files[test_file]['path'])
        else:
            # å°è¯•åœ¨è®­ç»ƒæ•°æ®ç›®å½•ä¸­æŸ¥æ‰¾
            training_data_dir = Path('data') / 'ad'
            possible_paths = [
                training_data_dir / test_file,
                Path('data') / test_file,
                Path(test_file)
            ]
            
            for path in possible_paths:
                if path.exists():
                    test_data_path = path
                    break
        
        if not test_data_path or not test_data_path.exists():
            logger.warning(f"æµ‹è¯•æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {test_file}")
            return None
        
        logger.info(f"æ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶: {test_data_path}")
        
        # è¯»å–åŸå§‹æµ‹è¯•æ•°æ®ï¼Œæå–æ ‡ç­¾åˆ—
        test_df = pd.read_csv(test_data_path)
        logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}, åˆ—: {list(test_df.columns)}")
        
        # æ£€æµ‹æ ‡ç­¾åˆ—ï¼ˆæœ€åä¸€åˆ—ï¼Œ0=æ­£å¸¸ï¼Œ1=å¼‚å¸¸ï¼‰
        has_labels = False
        labels = None
        label_column = None
        
        # æ£€æŸ¥æœ€åä¸€åˆ—æ˜¯å¦æ˜¯æ ‡ç­¾åˆ—
        last_col = test_df.columns[-1]
        last_col_values = test_df[last_col].unique()
        
        # å¦‚æœæœ€åä¸€åˆ—åªåŒ…å«0å’Œ1ï¼Œè®¤ä¸ºæ˜¯æ ‡ç­¾åˆ—
        if set(last_col_values).issubset({0, 1, 0.0, 1.0}):
            has_labels = True
            label_column = last_col
            labels = test_df[last_col].values.astype(int)
            # ä»ç‰¹å¾ä¸­ç§»é™¤æ ‡ç­¾åˆ—
            feature_df = test_df.iloc[:, :-1]
            logger.info(f"æ£€æµ‹åˆ°æ ‡ç­¾åˆ—: {last_col}, æ­£å¸¸æ ·æœ¬: {np.sum(labels == 0)}, å¼‚å¸¸æ ·æœ¬: {np.sum(labels == 1)}")
        else:
            feature_df = test_df
            logger.info("æœªæ£€æµ‹åˆ°æ ‡ç­¾åˆ—ï¼Œå°†åªè®¡ç®—è¯¯å·®ç»Ÿè®¡")
        
        # ä¿å­˜ç‰¹å¾æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ç”¨äºæ•°æ®å¤„ç†å™¨
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
            feature_df.to_csv(tmp.name, index=False)
            temp_feature_path = tmp.name
        
        try:
            # å¤„ç†æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„å¤„ç†å™¨ï¼‰
            if model_type == 'lstm_autoencoder':
                test_processor = LSTMAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            elif model_type == 'cnn_1d_autoencoder':
                test_processor = CNN1DAutoencoderDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    stride=config.get('stride', 1),
                    normalize=True
                )
            else:
                test_processor = LSTMPredictorDataProcessor(
                    sequence_length=config.get('sequence_length', 50),
                    prediction_horizon=config.get('prediction_horizon', 1),
                    normalize=True
                )
            
            # åŠ è½½å·²æœ‰çš„scaler
            scaler_path = model_dir / 'scaler.pkl'
            if scaler_path.exists():
                import pickle
                with open(scaler_path, 'rb') as f:
                    test_processor.scaler = pickle.load(f)
                logger.info(f"å·²åŠ è½½æ ‡å‡†åŒ–å™¨: {scaler_path}")
            
            # å¤„ç†æµ‹è¯•æ•°æ® - åªéœ€è¦æµ‹è¯•é›†ï¼Œä¸éœ€è¦åˆ’åˆ†
            test_data, _ = test_processor.process_pipeline(
                temp_feature_path,
                train_ratio=1.0  # å…¨éƒ¨ä½œä¸ºæµ‹è¯•æ•°æ®
            )
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            if os.path.exists(temp_feature_path):
                os.unlink(temp_feature_path)
        
        test_sequences = test_data.sequences
        test_targets = test_data.targets
        
        logger.info(f"æµ‹è¯•æ•°æ®: {len(test_sequences)} ä¸ªåºåˆ—")
        
        # è¿›è¡Œé¢„æµ‹
        model.eval()
        predictions = []
        batch_size = config.get('batch_size', 32)
        device = next(model.parameters()).device
        
        with torch.no_grad():
            for i in range(0, len(test_sequences), batch_size):
                batch_seq = test_sequences[i:i+batch_size]
                batch_tensor = torch.from_numpy(batch_seq.astype(np.float32)).to(device)
                batch_pred = model(batch_tensor)
                predictions.extend(batch_pred.cpu().numpy())
        
        predictions = np.array(predictions)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        if model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
            # è‡ªç¼–ç å™¨ï¼šè®¡ç®—è¾“å…¥ä¸é‡æ„ä¹‹é—´çš„è¯¯å·®
            errors = np.mean((test_sequences - predictions) ** 2, axis=(1, 2))  # MSE per sample
        else:
            # é¢„æµ‹å™¨ï¼šè®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®
            errors = np.mean((test_targets - predictions) ** 2, axis=-1)  # MSE per sample
            if len(errors.shape) > 1:
                errors = np.mean(errors, axis=1)
        
        # è®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
        evaluation_results = {
            'total_samples': len(test_sequences),
            'mean_error': float(np.mean(errors)),
            'std_error': float(np.std(errors)),
            'min_error': float(np.min(errors)),
            'max_error': float(np.max(errors)),
            'median_error': float(np.median(errors)),
            'percentile_90': float(np.percentile(errors, 90)),
            'percentile_95': float(np.percentile(errors, 95)),
            'percentile_99': float(np.percentile(errors, 99)),
            'test_file': test_file,
            'model_type': model_type,
            'has_labels': has_labels,
            'evaluated_at': datetime.now().isoformat()
        }
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œè®¡ç®—åˆ†ç±»æ€§èƒ½æŒ‡æ ‡
        if has_labels and labels is not None:
            sequence_length = config.get('sequence_length', 50)
            stride = config.get('stride', 1)
            
            # å°†åŸå§‹æ ‡ç­¾æ˜ å°„åˆ°åºåˆ—æ ‡ç­¾ï¼ˆæ¯ä¸ªåºåˆ—çš„æ ‡ç­¾å–è¯¥åºåˆ—ä¸­çš„ä¸»è¦æ ‡ç­¾ï¼‰
            sequence_labels = []
            for i in range(len(test_sequences)):
                # è®¡ç®—è¯¥åºåˆ—å¯¹åº”çš„åŸå§‹æ•°æ®ç´¢å¼•èŒƒå›´
                start_idx = i * stride
                end_idx = start_idx + sequence_length
                if end_idx <= len(labels):
                    # å¦‚æœåºåˆ—ä¸­ä»»ä½•ä¸€ä¸ªç‚¹æ˜¯å¼‚å¸¸ï¼Œåˆ™æ ‡è®°è¯¥åºåˆ—ä¸ºå¼‚å¸¸
                    seq_label = 1 if np.any(labels[start_idx:end_idx] == 1) else 0
                else:
                    seq_label = 0
                sequence_labels.append(seq_label)
            
            sequence_labels = np.array(sequence_labels)
            
            # ä½¿ç”¨95%åˆ†ä½æ•°ä½œä¸ºé»˜è®¤é˜ˆå€¼
            threshold = evaluation_results['percentile_95']
            
            # åŸºäºé˜ˆå€¼é¢„æµ‹å¼‚å¸¸
            predicted_labels = (errors > threshold).astype(int)
            
            # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
            true_positive = np.sum((predicted_labels == 1) & (sequence_labels == 1))
            true_negative = np.sum((predicted_labels == 0) & (sequence_labels == 0))
            false_positive = np.sum((predicted_labels == 1) & (sequence_labels == 0))
            false_negative = np.sum((predicted_labels == 0) & (sequence_labels == 1))
            
            accuracy = (true_positive + true_negative) / len(sequence_labels) if len(sequence_labels) > 0 else 0
            precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
            recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬çš„è¯¯å·®ç»Ÿè®¡
            normal_mask = sequence_labels == 0
            anomaly_mask = sequence_labels == 1
            
            normal_errors = errors[normal_mask] if np.any(normal_mask) else np.array([])
            anomaly_errors = errors[anomaly_mask] if np.any(anomaly_mask) else np.array([])
            
            evaluation_results.update({
                'normal_samples': int(np.sum(sequence_labels == 0)),
                'anomaly_samples': int(np.sum(sequence_labels == 1)),
                'threshold_used': float(threshold),
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1_score),
                'true_positive': int(true_positive),
                'true_negative': int(true_negative),
                'false_positive': int(false_positive),
                'false_negative': int(false_negative),
                'normal_error_mean': float(np.mean(normal_errors)) if len(normal_errors) > 0 else None,
                'normal_error_std': float(np.std(normal_errors)) if len(normal_errors) > 0 else None,
                'anomaly_error_mean': float(np.mean(anomaly_errors)) if len(anomaly_errors) > 0 else None,
                'anomaly_error_std': float(np.std(anomaly_errors)) if len(anomaly_errors) > 0 else None,
            })
            
            logger.info(f"åˆ†ç±»æ€§èƒ½ - å‡†ç¡®ç‡: {accuracy:.4f}, ç²¾ç¡®ç‡: {precision:.4f}, å¬å›ç‡: {recall:.4f}, F1: {f1_score:.4f}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
        eval_file = model_dir / f'{task_id}_evaluation_results.json'
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¯å·®åˆ†å¸ƒæ•°æ®ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        errors_file = model_dir / f'{task_id}_error_distribution.npz'
        save_data = {'errors': errors}
        if has_labels:
            save_data['labels'] = sequence_labels
        np.savez_compressed(errors_file, **save_data)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_file}")
        logger.info(f"è¯„ä¼°å®Œæˆ - å¹³å‡è¯¯å·®: {evaluation_results['mean_error']:.6f}, æ ‡å‡†å·®: {evaluation_results['std_error']:.6f}")
        
        return evaluation_results
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
        return None


@anomaly_detection_bp.route('/training_status/<task_id>', methods=['GET'])
def get_training_status(task_id):
    """è·å–è®­ç»ƒä»»åŠ¡çŠ¶æ€"""
    try:
        def safe_load_json(file_path):
            """Load JSON content from disk without raising."""
            if not file_path:
                return None
            try:
                path_obj = Path(file_path)
                if not path_obj.exists():
                    return None
                with open(path_obj, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as exc:
                logger.warning(f"æ— æ³•è§£ææ–‡ä»¶ {file_path}: {exc}")
                return None

        # ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡çŠ¶æ€
        task_manager = get_task_manager()
        task = task_manager.get_task(task_id)
        
        if task is None:
            # å¦‚æœä»»åŠ¡ç®¡ç†å™¨ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»æ–‡ä»¶ç³»ç»Ÿæ¢å¤
            models_dir = Path(project_root) / 'cloud' / 'models' / 'anomaly_detection'
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ä»»åŠ¡ç›®å½•ï¼ˆéå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•ï¼‰
            task_dirs = []
            if models_dir.exists():
                # éå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•ï¼šlstm_prediction, lstm_autoencoder, cnn_1d_autoencoder
                for model_type_dir in models_dir.iterdir():
                    if not model_type_dir.is_dir():
                        continue
                    
                    # é¦–å…ˆæŸ¥æ‰¾ç²¾ç¡®åŒ¹é…çš„ä»»åŠ¡ID
                    exact_match = model_type_dir / task_id
                    if exact_match.exists() and exact_match.is_dir():
                        task_dirs.append(exact_match)
                    else:
                        # æŸ¥æ‰¾åŒ…å«ä»»åŠ¡IDçš„ç›®å½•
                        for item in model_type_dir.iterdir():
                            if item.is_dir() and (task_id in item.name or item.name.endswith('001')):
                                task_dirs.append(item)
            
            # å¦‚æœæ‰¾åˆ°ä»»åŠ¡ç›®å½•ï¼Œæ¢å¤çŠ¶æ€
            if task_dirs:
                # ä½¿ç”¨æœ€æ–°çš„ç›®å½•
                task_dir = max(task_dirs, key=lambda p: p.stat().st_mtime)
                logger.info(f"ä»æ–‡ä»¶ç³»ç»Ÿæ¢å¤ä»»åŠ¡çŠ¶æ€: {task_dir}")
                
                # æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€
                threshold_path = task_dir / 'threshold.json'
                model_path = task_dir / 'model.pth'
                
                # æ„å»ºçŠ¶æ€ä¿¡æ¯
                status = 'completed'
                threshold_value = None
                threshold_metadata = None
                
                # å¦‚æœå­˜åœ¨é˜ˆå€¼æ–‡ä»¶ï¼Œæ ‡è®°ä¸ºé˜ˆå€¼å·²è®¡ç®—
                if threshold_path.exists():
                    try:
                        with open(threshold_path, 'r', encoding='utf-8') as f:
                            threshold_data = json.load(f)
                        
                        status = 'threshold_completed'
                        threshold_value = threshold_data.get('threshold', threshold_data.get('threshold_value'))
                        threshold_metadata = threshold_data
                        logger.info(f"æ¢å¤é˜ˆå€¼ä¿¡æ¯: {threshold_value}")
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–é˜ˆå€¼æ–‡ä»¶: {e}")
                
                # å°è¯•åŠ è½½è¯„ä¼°ç»“æœ
                evaluation_results = None
                eval_file = task_dir / f'{task_id}_evaluation_results.json'
                if eval_file.exists():
                    try:
                        with open(eval_file, 'r', encoding='utf-8') as f:
                            evaluation_results = json.load(f)
                        logger.info(f"æ¢å¤è¯„ä¼°ç»“æœ: {eval_file}")
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–è¯„ä¼°ç»“æœæ–‡ä»¶: {e}")
                
                config_path = task_dir / 'config.json'
                training_config_path = task_dir / 'training_config.json'
                metadata_path = task_dir / 'metadata.json'

                config_data = safe_load_json(config_path) or {}
                if not isinstance(config_data, dict):
                    config_data = {}

                training_config = safe_load_json(training_config_path) or config_data
                if not isinstance(training_config, dict):
                    training_config = config_data

                metadata = safe_load_json(metadata_path) or config_data.get('metadata') or {}
                if not isinstance(metadata, dict):
                    metadata = {}
                dataset_config = {}
                if isinstance(config_data.get('dataset_config'), dict):
                    dataset_config = config_data.get('dataset_config')
                elif isinstance(training_config, dict) and isinstance(training_config.get('dataset_config'), dict):
                    dataset_config = training_config.get('dataset_config')

                # è¿”å›edgeç«¯traineræœŸæœ›çš„æ ¼å¼
                return jsonify({
                    'success': True,
                    'task': {
                        'id': task_id,
                        'status': status,
                        'current_epoch': 100,
                        'epoch': 100,
                        'completed_epochs': 100,
                        'config': config_data,
                        'training_config': training_config,
                        'dataset_config': dataset_config,
                        'metadata': metadata,
                        'total_epochs': 100,
                        'current_train_loss': None,  # æ–‡ä»¶æ¢å¤æ—¶æ²¡æœ‰å½“å‰æŸå¤±
                        'current_val_loss': None,
                        'loss': None,                # å…¼å®¹å‰ç«¯
                        'val_loss': None,
                        'final_train_loss': None,
                        'final_val_loss': None,
                        'message': f'ä»»åŠ¡å·²å®Œæˆï¼ˆä»æ–‡ä»¶æ¢å¤ï¼‰',
                        'logs': [f'ä»æ–‡ä»¶ç³»ç»Ÿæ¢å¤ä»»åŠ¡çŠ¶æ€: {task_dir.name}'],
                        'threshold_value': threshold_value,
                        'threshold_path': str(threshold_path) if threshold_path.exists() else None,
                        'threshold_metadata': threshold_metadata,
                        'learning_rate': config_data.get('learning_rate'),
                        'dataset_mode': config_data.get('dataset_mode'),
                        'model_type': config_data.get('model_type'),
                        'model_save_path': str(model_path) if model_path.exists() else None,
                        'created_at': task_dir.stat().st_ctime,
                        'updated_at': task_dir.stat().st_mtime,
                        'progress': 100,
                        'evaluation': evaluation_results,
                        'evaluation_results': evaluation_results
                    }
                })
            
            # å¦‚æœå®Œå…¨æ‰¾ä¸åˆ°ä»»åŠ¡
            return jsonify({
                'success': False,
                'error': f'Task {task_id} not found'
            }), 404
        
        # ä»»åŠ¡å­˜åœ¨ï¼Œä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨çš„æ•°æ®
        # è·å–è¯„ä¼°ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        evaluation_results = getattr(task, 'evaluation_results', None)
        task_config = task.config if isinstance(task.config, dict) else {}
        dataset_config = task_config.get('dataset_config') if isinstance(task_config.get('dataset_config'), dict) else {}
        metadata = task_config.get('metadata') if isinstance(task_config.get('metadata'), dict) else {}
        
        # è½¬æ¢ä¸ºedgeç«¯traineræœŸæœ›çš„æ ¼å¼ - åŒ…è£…åœ¨taskå­—æ®µä¸­
        return jsonify({
            'success': True,
            'task': {
                'id': task.task_id,
                'status': task.status,
                'current_epoch': task.current_epoch,
                'epoch': task.current_epoch,  # å…¼å®¹ä¸¤ä¸ªå­—æ®µ
                'completed_epochs': task.current_epoch,
            'config': task_config,
            'training_config': task_config,
            'dataset_config': dataset_config,
            'metadata': metadata,
                'total_epochs': task.config.get('epochs', 100),
                'current_train_loss': task.current_train_loss,  # å½“å‰è®­ç»ƒæŸå¤±
                'current_val_loss': task.current_val_loss,    # å½“å‰éªŒè¯æŸå¤±
                'loss': task.current_train_loss,              # å…¼å®¹å‰ç«¯æœŸæœ›çš„losså­—æ®µ
                'val_loss': task.current_val_loss,            # å…¼å®¹å‰ç«¯æœŸæœ›çš„val_losså­—æ®µ
                'final_train_loss': task.final_train_loss,
                'final_val_loss': task.final_val_loss,
                'message': task.message,
                'logs': task.logs.split('\n')[-20:] if isinstance(task.logs, str) and len(task.logs.split('\n')) > 20 else (task.logs.split('\n') if isinstance(task.logs, str) else []),  # æœ€å20æ¡æ—¥å¿—æˆ–å…¨éƒ¨
                'threshold_value': task.threshold_value,
                'threshold_path': task.threshold_path,
                'threshold_metadata': task.threshold_metadata,
                'learning_rate': getattr(task, 'learning_rate', None),
                'dataset_mode': getattr(task, 'dataset_mode', None),
                'model_type': getattr(task, 'model_type', None),
                'progress': task.progress,
                'created_at': task.created_at if isinstance(task.created_at, str) else None,
                'updated_at': task.updated_at if isinstance(task.updated_at, str) else None,
                'model_save_path': task.model_save_path,
                'scaler_path': task.scaler_path,
                'evaluation': evaluation_results,
                'evaluation_results': evaluation_results
            }
        })
        
    except Exception as e:
        logger.error(f"Failed to get training status for {task_id}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# æ—§çš„æ¨¡å‹åˆ—è¡¨è·¯ç”±å·²ç§»é™¤ï¼Œä½¿ç”¨ä¸‹é¢çš„ list_anomaly_detection_models() æ›¿ä»£
# @anomaly_detection_bp.route('/models', methods=['GET'])
# def list_models():
#     """åˆ—å‡ºå¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ list_anomaly_detection_modelsï¼‰"""
#     ...

@anomaly_detection_bp.route('/inference_tasks', methods=['GET'])
def list_inference_tasks():
    """è·å–æ¨ç†ä»»åŠ¡åˆ—è¡¨"""
    try:
        base_dir = Path('models/anomaly_detection/inference_tasks')
        if not base_dir.exists():
            return jsonify({'success': True, 'tasks': []})
        
        tasks = []
        for task_dir in sorted(base_dir.glob('inference_*'), reverse=True):
            config_path = task_dir / 'config.json'
            summary_path = task_dir / 'results_summary.json'
            
            if config_path.exists() and summary_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    with open(summary_path, 'r', encoding='utf-8') as f:
                        summary = json.load(f)
                    
                    task_info = {
                        'task_id': config['task_id'],
                        'created_at': config['created_at'],
                        'completed_at': summary.get('completed_at'),
                        'model_type': config['model_type'],
                        'source_task_id': config.get('source_task_id'),
                        'total_samples': summary['total_samples'],
                        'anomalies_detected': summary['anomalies_detected'],
                        'anomaly_percentage': summary['anomaly_percentage'],
                        'threshold': summary['threshold'],
                        'has_performance_metrics': 'performance_metrics' in summary,
                        'task_dir': str(task_dir)
                    }
                    tasks.append(task_info)
                    
                except Exception as e:
                    logger.warning(f"Failed to load task info for {task_dir}: {e}")
                    continue
        
        return jsonify({
            'success': True,
            'tasks': tasks,
            'total_count': len(tasks)
        })
        
    except Exception as e:
        logger.error(f"Failed to list inference tasks: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@anomaly_detection_bp.route('/inference_tasks/<task_id>', methods=['GET'])
def get_inference_task_detail(task_id):
    """è·å–æ¨ç†ä»»åŠ¡è¯¦ç»†ä¿¡æ¯"""
    try:
        task_dir = Path(f'models/anomaly_detection/inference_tasks/inference_{task_id}')
        if not task_dir.exists():
            return jsonify({
                'success': False,
                'error': f'Inference task {task_id} not found'
            }), 404
        
        result = {'task_id': task_id}
        
        # åŠ è½½é…ç½®
        config_path = task_dir / 'config.json'
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                result['config'] = json.load(f)
        
        # åŠ è½½ç»“æœæ‘˜è¦
        summary_path = task_dir / 'results_summary.json'
        if summary_path.exists():
            with open(summary_path, 'r', encoding='utf-8') as f:
                result['results_summary'] = json.load(f)
        
        # åŠ è½½æ•°æ®ä¿¡æ¯
        data_info_path = task_dir / 'data_info.json'
        if data_info_path.exists():
            with open(data_info_path, 'r', encoding='utf-8') as f:
                result['data_info'] = json.load(f)
        
        # æ£€æŸ¥è¯¦ç»†ç»“æœæ–‡ä»¶
        detailed_path = task_dir / 'detailed_results.npz'
        result['has_detailed_results'] = detailed_path.exists()
        
        return jsonify({
            'success': True,
            'task': result
        })
        
    except Exception as e:
        logger.error(f"Failed to get inference task detail: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@anomaly_detection_bp.route('/calculate_threshold/<task_id>', methods=['POST'])
def calculate_threshold(task_id):
    """è®¡ç®—è®­ç»ƒå®Œæˆåçš„å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
    try:
        logger.info(f"å¼€å§‹è®¡ç®—ä»»åŠ¡ {task_id} çš„é˜ˆå€¼")
        
        # è·å–è¯·æ±‚ä¸­çš„é˜ˆå€¼å‚æ•°
        request_data = request.get_json() or {}
        threshold_method = request_data.get('threshold_method', 'percentile')
        percentile = float(request_data.get('percentile', 95.0))
        residual_metric = request_data.get('residual_metric', 'rmse')
        contamination = request_data.get('contamination')  # è·å–contaminationå‚æ•°
        
        # å¦‚æœcontaminationæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæµ®ç‚¹æ•°
        if contamination is not None:
            if isinstance(contamination, str):
                contamination = float(contamination)
            else:
                contamination = float(contamination)
            # å¦‚æœcontaminationæ˜¯ç™¾åˆ†æ¯”å½¢å¼ï¼ˆå¦‚20.0è¡¨ç¤º20%ï¼‰ï¼Œè½¬æ¢ä¸ºå°æ•°ï¼ˆ0.2ï¼‰
            if contamination > 1.0:
                contamination = contamination / 100.0
        
        logger.info(f"ä½¿ç”¨é˜ˆå€¼å‚æ•°: method={threshold_method}, percentile={percentile}, metric={residual_metric}, contamination={contamination}")
        
        # ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡
        task_manager = get_task_manager()
        task = task_manager.get_task(task_id)
        
        if task is None:
            return jsonify({
                'success': False,
                'error': f'è®­ç»ƒä»»åŠ¡ {task_id} ä¸å­˜åœ¨'
            }), 404
        
        # æ£€æŸ¥è®­ç»ƒçŠ¶æ€
        if task.status != 'completed':
            return jsonify({
                'success': False,
                'error': 'è®­ç»ƒå°šæœªå®Œæˆï¼Œæ— æ³•è®¡ç®—é˜ˆå€¼'
            }), 400
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè®¡ç®—é˜ˆå€¼ä¸­
        task_manager.update_task_status(task_id, 'calculating_threshold', 'æ­£åœ¨è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼...')
        
        # è·å–è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ - æ”¯æŒå¤šç§å¯èƒ½çš„æ¨¡å‹å­˜å‚¨ä½ç½®
        config = task.config
        model_type = config.get('model_type', 'lstm_predictor')
        
        # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šæ¨¡å‹ç›®å½•
        if model_type == 'lstm_autoencoder':
            model_type_dir = 'lstm_autoencoder'
        elif model_type == 'cnn_1d_autoencoder':
            model_type_dir = 'cnn_1d_autoencoder'
        else:
            model_type_dir = 'lstm_prediction'
        
        possible_model_dirs = [
            Path(project_root) / 'cloud' / 'models' / 'anomaly_detection' / model_type_dir,
            Path('models') / 'anomaly_detection' / model_type_dir,  # ç›¸å¯¹è·¯å¾„ï¼ˆäº‘ç«¯æœåŠ¡è¿è¡Œæ—¶ï¼‰
            Path.cwd() / 'models' / 'anomaly_detection' / model_type_dir,  # å½“å‰å·¥ä½œç›®å½•
        ]
        
        model_dir = None
        
        # å°è¯•åœ¨æ‰€æœ‰å¯èƒ½çš„ä½ç½®æŸ¥æ‰¾æ¨¡å‹ç›®å½•
        for models_dir in possible_model_dirs:
            if models_dir.exists():
                task_model_dir = models_dir / task_id
                if task_model_dir.exists():
                    model_dir = task_model_dir
                    logger.info(f'æ‰¾åˆ°æ¨¡å‹ç›®å½•: {model_dir}')
                    break
                else:
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…
                    for item in models_dir.iterdir():
                        if item.is_dir() and (task_id in item.name or item.name.startswith(task_id[:10])):
                            model_dir = item
                            logger.info(f'æ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹ç›®å½•: {model_dir}')
                            break
                    if model_dir:
                        break
        
        if not model_dir:
            logger.error(f'æ— æ³•æ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œä»»åŠ¡ID: {task_id}')
            logger.error(f'å°è¯•çš„è·¯å¾„: {[str(d) for d in possible_model_dirs]}')
            task_manager.update_task_status(task_id, 'completed', f'é˜ˆå€¼è®¡ç®—å¤±è´¥: æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½•')
            return jsonify({
                'success': False,
                'error': f'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»»åŠ¡ID: {task_id}'
            }), 400
        
        config = task.config
        data_file = config.get('dataset_file')
        
        # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        if data_file and data_file in uploaded_data_files:
            # ä½¿ç”¨ä¸Šä¼ çš„è®­ç»ƒæ•°æ®
            data_file_path = Path(uploaded_data_files[data_file]['path'])
        elif data_file:
            # å°è¯•åœ¨è®­ç»ƒæ•°æ®ç›®å½•ä¸­æŸ¥æ‰¾ (å¼‚å¸¸æ£€æµ‹: cloud/data/ad)
            training_data_dir = Path('data') / 'ad'
            data_file_path = training_data_dir / data_file
            if not data_file_path.exists():
                data_file_path = Path('data') / data_file
        else:
            data_file_path = None
        
        if not data_file_path or not data_file_path.exists():
            task_manager.update_task_status(task_id, 'completed', 'é˜ˆå€¼è®¡ç®—å¤±è´¥: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶')
            return jsonify({
                'success': False,
                'error': 'è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿æ•°æ®å·²ä»è¾¹ç«¯ä¸Šä¼ åˆ°äº‘ç«¯'
            }), 400
        
        # model_dirå·²ç»åœ¨å‰é¢éªŒè¯è¿‡å­˜åœ¨ï¼Œè¿™é‡Œä¸éœ€è¦å†æ£€æŸ¥
        
        # ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹è®¡ç®—é˜ˆå€¼
        if training_available:
            try:
                # ä»ä»»åŠ¡ä¸­è·å–ä¿å­˜çš„é…ç½®ä¿¡æ¯
                config_path = model_dir / 'config.json'
                if not config_path.exists():
                    task_manager.update_task_status(task_id, 'completed', 'é˜ˆå€¼è®¡ç®—å¤±è´¥: æ‰¾ä¸åˆ°è®­ç»ƒé…ç½®æ–‡ä»¶')
                    return jsonify({
                        'success': False,
                        'error': 'æ‰¾ä¸åˆ°è®­ç»ƒé…ç½®æ–‡ä»¶'
                    }), 400
                
                with open(config_path, 'r', encoding='utf-8') as f:
                    model_config = json.load(f)
                
                # é‡æ–°åŠ è½½æ•°æ®å¤„ç†å™¨å’Œæ•°æ®ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ï¼‰
                model_type = model_config.get('model_type', 'lstm_predictor')
                
                if model_type == 'lstm_autoencoder':
                    data_processor = LSTMAutoencoderDataProcessor(
                        sequence_length=model_config.get('sequence_length', 50),
                        stride=model_config.get('stride', 1),
                        normalize=True
                    )
                elif model_type == 'cnn_1d_autoencoder':
                    data_processor = CNN1DAutoencoderDataProcessor(
                        sequence_length=model_config.get('sequence_length', 50),
                        stride=model_config.get('stride', 1),
                        normalize=True
                    )
                else:
                    data_processor = LSTMPredictorDataProcessor(
                        sequence_length=model_config.get('sequence_length', 50),
                        prediction_horizon=model_config.get('prediction_horizon', 1),
                        normalize=True
                    )
                
                # åŠ è½½è®­ç»ƒæ•°æ®
                train_data, _ = data_processor.process_pipeline(
                    str(data_file_path),
                    train_ratio=0.8  # åªéœ€è¦è®­ç»ƒæ•°æ®ç”¨äºè®¡ç®—é˜ˆå€¼
                )
                
                # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
                model_path = model_dir / 'model.pth'
                if not model_path.exists():
                    task_manager.update_task_status(task_id, 'completed', 'é˜ˆå€¼è®¡ç®—å¤±è´¥: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶')
                    return jsonify({
                        'success': False,
                        'error': 'æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶'
                    }), 400
                
                # é‡å»ºæ¨¡å‹æ¶æ„ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ï¼‰
                feature_dim = model_config.get('feature_dim')
                input_shape = (model_config.get('sequence_length', 50), feature_dim)
                
                if model_type == 'lstm_autoencoder':
                    model = LSTMAutoencoderModelBuilder.create_model(
                        'lstm_autoencoder',
                        input_shape=input_shape,
                        hidden_size=model_config.get('hidden_units', 128),
                        num_layers=model_config.get('num_layers', 2),
                        bottleneck_dim=model_config.get('bottleneck_dim', 64),
                        dropout=model_config.get('dropout', 0.1)
                    )
                elif model_type == 'cnn_1d_autoencoder':
                    model = CNN1DAutoencoderModelBuilder.create_model(
                        'cnn_1d_autoencoder',
                        input_shape=input_shape,
                        num_filters=model_config.get('num_filters', 64),
                        kernel_size=model_config.get('kernel_size', 3),
                        bottleneck_dim=model_config.get('bottleneck_dim', 64),
                        num_conv_layers=model_config.get('num_conv_layers', model_config.get('num_layers', 3)),
                        dropout=model_config.get('dropout', 0.1),
                        activation=model_config.get('activation', 'relu')
                    )
                else:
                    model = LSTMPredictorModelBuilder.build_lstm_predictor(
                        input_shape=input_shape,
                        hidden_units=model_config.get('hidden_units', 128),
                        num_layers=model_config.get('num_layers', 2),
                        dropout=model_config.get('dropout', 0.1),
                        activation=model_config.get('activation', 'tanh')
                    )
                
                # åŠ è½½æ¨¡å‹å‚æ•°
                device = _get_torch_device()
                model = model.to(device)
                state_dict = torch.load(str(model_path), map_location=device)
                model.load_state_dict(state_dict)
                model.eval()
                logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
                
                # ä½¿ç”¨è®­ç»ƒæ•°æ®è®¡ç®—é˜ˆå€¼
                sample_size = min(1000, len(train_data.sequences))
                sample_sequences = train_data.sequences[:sample_size]
                sample_targets = train_data.targets[:sample_size]
                
                # æ‰¹é‡é¢„æµ‹
                predictions = []
                batch_size = 32
                with torch.no_grad():
                    for i in range(0, len(sample_sequences), batch_size):
                        batch_seq = sample_sequences[i:i+batch_size]
                        batch_tensor = torch.from_numpy(batch_seq.astype(np.float32)).to(device)
                        batch_pred = model(batch_tensor)
                        predictions.extend(batch_pred.cpu().numpy())
                
                predictions = np.array(predictions)
                actuals = sample_targets
                
                # è®¡ç®—é˜ˆå€¼ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é˜ˆå€¼è®¡ç®—å™¨ï¼‰
                if model_type == 'lstm_autoencoder':
                    threshold_calc = LSTMAutoencoderThresholdCalculator(
                        method=threshold_method,
                        residual_metric=residual_metric
                    )
                    # å¯¹äºè‡ªç¼–ç å™¨ï¼Œç”¨é‡æ„è¯¯å·®è®¡ç®—é˜ˆå€¼
                    threshold_value = threshold_calc.fit(
                        predictions, 
                        actuals,
                        percentile=percentile,
                        contamination=contamination
                    )
                elif model_type == 'cnn_1d_autoencoder':
                    threshold_calc = CNN1DAutoencoderThresholdCalculator(
                        method=threshold_method,
                        residual_metric=residual_metric
                    )
                    # å¯¹äºè‡ªç¼–ç å™¨ï¼Œç”¨é‡æ„è¯¯å·®è®¡ç®—é˜ˆå€¼
                    threshold_value = threshold_calc.fit(
                        predictions, 
                        actuals,
                        percentile=percentile,
                        contamination=contamination
                    )
                else:
                    threshold_calc = LSTMPredictorThresholdCalculator(residual_method='l2_norm')
                    threshold_value = threshold_calc.fit_threshold(
                        predictions, 
                        actuals,
                        method=threshold_method,
                        percentile=percentile,
                        contamination=contamination
                    )
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªç¼–ç å™¨ç±»å‹
                is_autoencoder = model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']
                threshold_metadata = {
                    'method': threshold_method if is_autoencoder else threshold_method,
                    'percentile': percentile if is_autoencoder else percentile,
                    'residual_metric': residual_metric if is_autoencoder else 'l2_norm',
                    'contamination': contamination,  # æ·»åŠ contaminationä¿¡æ¯
                    'sample_size': sample_size,
                    'statistics': threshold_calc.stats if hasattr(threshold_calc, 'stats') else getattr(threshold_calc, 'residual_stats', {})
                }
                
                # ä¿å­˜é˜ˆå€¼æ–‡ä»¶
                threshold_path = model_dir / 'threshold.json'
                threshold_data = {
                    'threshold_value': float(threshold_value),
                    'method': threshold_method if is_autoencoder else 'percentile',
                    'percentile': percentile if is_autoencoder else 95.0,
                    'residual_metric': residual_metric if is_autoencoder else 'l2_norm',
                    'sample_size': sample_size,
                    'calculated_at': datetime.now().isoformat(),
                    'task_id': task_id,
                    'model_type': model_type,
                    'statistics': threshold_calc.stats if hasattr(threshold_calc, 'stats') else getattr(threshold_calc, 'residual_stats', {})
                }
                
                with open(threshold_path, 'w', encoding='utf-8') as f:
                    json.dump(threshold_data, f, indent=2, ensure_ascii=False)
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé˜ˆå€¼è®¡ç®—å®Œæˆ
                task_manager.update_threshold_info(
                    task_id, 
                    str(threshold_path), 
                    float(threshold_value), 
                    threshold_metadata
                )
                task_manager.update_task_status(task_id, 'threshold_completed', f'é˜ˆå€¼è®¡ç®—å®Œæˆ: {threshold_value:.6f}')
                
                # é˜ˆå€¼è®¡ç®—å®Œæˆåé€šçŸ¥Edgeç«¯ä¸‹è½½æ›´æ–°çš„æ¨¡å‹
                try:
                    _notify_edge_model_ready(task_id, model_dir.parent.name)
                    logger.info(f"å·²é€šçŸ¥Edgeç«¯é˜ˆå€¼æ–‡ä»¶å°±ç»ª: {task_id}")
                except Exception as e:
                    logger.warning(f"é€šçŸ¥Edgeç«¯é˜ˆå€¼æ›´æ–°å¤±è´¥: {e}")
                
                logger.info(f"é˜ˆå€¼è®¡ç®—å®Œæˆ: {threshold_value}")
                
                return jsonify({
                    'success': True,
                    'threshold_value': float(threshold_value),
                    'threshold_path': str(threshold_path),
                    'metadata': threshold_metadata
                })
                
            except Exception as e:
                logger.error(f"é˜ˆå€¼è®¡ç®—å¤±è´¥: {e}")
                task_manager.update_task_status(task_id, 'completed', f'é˜ˆå€¼è®¡ç®—å¤±è´¥: {str(e)}')
                
                return jsonify({
                    'success': False,
                    'error': f'é˜ˆå€¼è®¡ç®—å¤±è´¥: {str(e)}'
                }), 500
        else:
            # è®­ç»ƒæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è®¡ç®—çœŸå®é˜ˆå€¼
            error_msg = 'è®­ç»ƒæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼ã€‚è¯·ç¡®ä¿PyTorchç­‰ä¾èµ–å·²æ­£ç¡®å®‰è£…ã€‚'
            task_manager.update_task_status(task_id, 'failed', error_msg)
            logger.error(f"é˜ˆå€¼è®¡ç®—å¤±è´¥: {error_msg}")
            
            return jsonify({
                'success': False,
                'error': error_msg
            }), 500
            
    except Exception as e:
        logger.error(f"é˜ˆå€¼è®¡ç®—å¼‚å¸¸: {e}")
        return jsonify({
            'success': False,
            'error': f'é˜ˆå€¼è®¡ç®—å¼‚å¸¸: {str(e)}'
        }), 500


@anomaly_detection_bp.route('/models', methods=['GET'])
def list_anomaly_detection_models():
    """è·å–å¼‚å¸¸æ£€æµ‹æ¨¡å‹åˆ—è¡¨"""
    print("=" * 80)
    print("[æ¨¡å‹åˆ—è¡¨API] ===== å¼€å§‹å¤„ç†æ¨¡å‹åˆ—è¡¨è¯·æ±‚ =====")
    print("=" * 80)
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ¨¡å‹ç›®å½•è·¯å¾„
        from pathlib import Path
        import os
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾api.pyåœ¨cloud/src/anomaly_detection/ï¼‰
        current_file = Path(__file__).resolve()
        project_root = current_file.parents[3]  # ä» cloud/src/anomaly_detection/api.py å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        
        # ä½¿ç”¨ print ç¡®ä¿æ—¥å¿—è¾“å‡ºï¼ˆå› ä¸º logger å¯èƒ½çº§åˆ«è®¾ç½®é—®é¢˜ï¼‰
        print(f'[æ¨¡å‹åˆ—è¡¨API] å½“å‰æ–‡ä»¶è·¯å¾„: {current_file}')
        print(f'[æ¨¡å‹åˆ—è¡¨API] è®¡ç®—çš„é¡¹ç›®æ ¹ç›®å½•: {project_root}')
        print(f'[æ¨¡å‹åˆ—è¡¨API] å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}')
        logger.info(f'å½“å‰æ–‡ä»¶è·¯å¾„: {current_file}')
        logger.info(f'è®¡ç®—çš„é¡¹ç›®æ ¹ç›®å½•: {project_root}')
        logger.info(f'å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}')
        
        # æ„å»ºå¯èƒ½çš„æ¨¡å‹ç›®å½•è·¯å¾„
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        models_dir_from_env = os.environ.get('MODELS_DIR')
        if models_dir_from_env:
            env_models_dir = Path(models_dir_from_env) / 'anomaly_detection'
        else:
            env_models_dir = None
        
        possible_model_dirs = []
        
        # 1. ç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if env_models_dir:
            possible_model_dirs.append(env_models_dir)
        
        # 2. æ ‡å‡†é¡¹ç›®è·¯å¾„
        possible_model_dirs.extend([
            project_root / 'cloud' / 'models' / 'anomaly_detection',  # æ ‡å‡†è·¯å¾„ï¼šé¡¹ç›®æ ¹/cloud/models/anomaly_detection
            Path('models/anomaly_detection'),  # ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚æœä»cloudç›®å½•è¿è¡Œï¼‰
            Path.cwd() / 'models' / 'anomaly_detection',  # å½“å‰å·¥ä½œç›®å½•
            Path.cwd() / 'cloud' / 'models' / 'anomaly_detection',  # å½“å‰å·¥ä½œç›®å½•ä¸‹çš„cloud/models
        ])
        
        print(f'[æ¨¡å‹åˆ—è¡¨API] å°è¯•æŸ¥æ‰¾æ¨¡å‹ç›®å½•ï¼Œå¯èƒ½çš„è·¯å¾„:')
        logger.info(f'å°è¯•æŸ¥æ‰¾æ¨¡å‹ç›®å½•ï¼Œå¯èƒ½çš„è·¯å¾„:')
        for i, possible_dir in enumerate(possible_model_dirs, 1):
            exists = possible_dir.exists()
            print(f'  {i}. {possible_dir} - å­˜åœ¨: {exists}')
            logger.info(f'  {i}. {possible_dir} - å­˜åœ¨: {exists}')
            if exists:
                # åˆ—å‡ºç›®å½•å†…å®¹ä»¥ç¡®è®¤
                try:
                    subdirs = [d.name for d in possible_dir.iterdir() if d.is_dir()]
                    print(f'     å­ç›®å½•: {subdirs}')
                    logger.info(f'     å­ç›®å½•: {subdirs}')
                except Exception as e:
                    print(f'     æ— æ³•åˆ—å‡ºå­ç›®å½•: {e}')
                    logger.warning(f'     æ— æ³•åˆ—å‡ºå­ç›®å½•: {e}')
        
        models_dir = None
        for possible_dir in possible_model_dirs:
            if possible_dir.exists():
                models_dir = possible_dir
                print(f'[æ¨¡å‹åˆ—è¡¨API] âœ“ æ‰¾åˆ°æ¨¡å‹ç›®å½•: {models_dir}')
                logger.info(f'âœ“ æ‰¾åˆ°æ¨¡å‹ç›®å½•: {models_dir}')
                break
        
        models = []
        
        if models_dir and models_dir.exists():
            print(f'[æ¨¡å‹åˆ—è¡¨API] å¼€å§‹æ‰«ææ¨¡å‹ç›®å½•: {models_dir}')
            logger.info(f'å¼€å§‹æ‰«ææ¨¡å‹ç›®å½•: {models_dir}')
            # éå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•
            for model_type_dir in models_dir.iterdir():
                if not model_type_dir.is_dir():
                    print(f'[æ¨¡å‹åˆ—è¡¨API] è·³è¿‡éç›®å½•: {model_type_dir.name}')
                    logger.debug(f'è·³è¿‡éç›®å½•: {model_type_dir.name}')
                    continue
                
                print(f'[æ¨¡å‹åˆ—è¡¨API] æ‰«ææ¨¡å‹ç±»å‹ç›®å½•: {model_type_dir.name}')
                logger.info(f'æ‰«ææ¨¡å‹ç±»å‹ç›®å½•: {model_type_dir.name}')
                task_count = 0
                    
                # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
                for task_dir in model_type_dir.iterdir():
                    if not task_dir.is_dir():
                        print(f'[æ¨¡å‹åˆ—è¡¨API] è·³è¿‡éç›®å½•ä»»åŠ¡: {task_dir.name}')
                        logger.debug(f'è·³è¿‡éç›®å½•ä»»åŠ¡: {task_dir.name}')
                        continue
                    
                    task_count += 1
                    print(f'[æ¨¡å‹åˆ—è¡¨API] æ£€æŸ¥ä»»åŠ¡ç›®å½•: {task_dir.name}')
                    logger.debug(f'æ£€æŸ¥ä»»åŠ¡ç›®å½•: {task_dir.name}')
                    
                    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
                    config_path = task_dir / 'config.json'
                    # å°è¯•å¤šç§æ¨¡å‹æ–‡ä»¶æ‰©å±•å
                    model_files = (
                        list(task_dir.glob('*.pth')) + 
                        list(task_dir.glob('*.pth')) + 
                        list(task_dir.glob('*.mindir')) +
                        list(task_dir.glob('model.*'))  # åŒ¹é…ä»»ä½•ä»¥model.å¼€å¤´çš„æ–‡ä»¶
                    )
                    
                    # åˆ—å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œç”¨äºè°ƒè¯•
                    all_files = list(task_dir.iterdir())
                    print(f'[æ¨¡å‹åˆ—è¡¨API]   ç›®å½•ä¸­çš„æ–‡ä»¶: {[f.name for f in all_files]}')
                    print(f'[æ¨¡å‹åˆ—è¡¨API]   config.json å­˜åœ¨: {config_path.exists()}')
                    print(f'[æ¨¡å‹åˆ—è¡¨API]   æ¨¡å‹æ–‡ä»¶æ•°é‡: {len(model_files)}, æ–‡ä»¶: {[f.name for f in model_files]}')
                    logger.debug(f'  ç›®å½•ä¸­çš„æ–‡ä»¶: {[f.name for f in all_files]}')
                    logger.debug(f'  config.json å­˜åœ¨: {config_path.exists()}')
                    logger.debug(f'  æ¨¡å‹æ–‡ä»¶æ•°é‡: {len(model_files)}, æ–‡ä»¶: {[f.name for f in model_files]}')
                    
                    if not config_path.exists():
                        print(f'[æ¨¡å‹åˆ—è¡¨API]   è·³è¿‡ {task_dir.name}: ç¼ºå°‘ config.json')
                        logger.warning(f'  è·³è¿‡ {task_dir.name}: ç¼ºå°‘ config.json')
                        continue
                    
                    if not model_files:
                        print(f'[æ¨¡å‹åˆ—è¡¨API]   è·³è¿‡ {task_dir.name}: ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ (.pth)')
                        logger.warning(f'  è·³è¿‡ {task_dir.name}: ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ (.pth)')
                        continue
                    
                    try:
                        # åŠ è½½é…ç½®
                        with open(config_path, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                        
                        model_path = model_files[0]
                        model_stat = model_path.stat()
                        
                        # æ£€æŸ¥é˜ˆå€¼æ–‡ä»¶
                        threshold_info = None
                        threshold_path = task_dir / 'threshold.json'
                        if threshold_path.exists():
                            try:
                                with open(threshold_path, 'r', encoding='utf-8') as f:
                                    threshold_info = json.load(f)
                            except:
                                pass
                        
                        model_info = {
                            'task_id': task_dir.name,
                            'model_type': model_type_dir.name,
                            'filename': model_path.name,
                            'size': model_stat.st_size,
                            'created_at': datetime.fromtimestamp(model_stat.st_ctime).isoformat(),
                            'modified_at': datetime.fromtimestamp(model_stat.st_mtime).isoformat(),
                            'config': {
                                'sequence_length': config.get('sequence_length', 50),
                                'hidden_units': config.get('hidden_units', 128),
                                'num_layers': config.get('num_layers', 2),
                                'epochs': config.get('epochs', 50),
                                'batch_size': config.get('batch_size', 32),
                                'feature_dim': config.get('feature_dim'),
                            },
                            'files': {
                                'model': {
                                    'exists': True,
                                    'size': model_stat.st_size,
                                    'filename': model_path.name
                                },
                                'scaler': {
                                    'exists': (task_dir / 'scaler.pkl').exists()
                                },
                                'threshold': {
                                    'exists': threshold_path.exists(),
                                    'value': threshold_info.get('threshold_value') if threshold_info else None,
                                    'method': threshold_info.get('method') if threshold_info else None
                                }
                            }
                        }
                        
                        models.append(model_info)
                        print(f'[æ¨¡å‹åˆ—è¡¨API] âœ“ æ·»åŠ æ¨¡å‹: {task_dir.name} (ç±»å‹: {model_type_dir.name})')
                        logger.info(f'âœ“ æ·»åŠ æ¨¡å‹: {task_dir.name} (ç±»å‹: {model_type_dir.name})')
                        
                    except Exception as e:
                        print(f'[æ¨¡å‹åˆ—è¡¨API] âŒ è¯»å–æ¨¡å‹é…ç½®å¤±è´¥ {task_dir}: {e}')
                        logger.error(f"è¯»å–æ¨¡å‹é…ç½®å¤±è´¥ {task_dir}: {e}", exc_info=True)
                        continue
                
                print(f'[æ¨¡å‹åˆ—è¡¨API] æ¨¡å‹ç±»å‹ {model_type_dir.name} å…±æ‰«æäº† {len(list(model_type_dir.iterdir()))} ä¸ªé¡¹ç›®')
                logger.info(f'æ¨¡å‹ç±»å‹ {model_type_dir.name} å…±æ‰«æäº† {len(list(model_type_dir.iterdir()))} ä¸ªé¡¹ç›®')
        else:
            print(f'[æ¨¡å‹åˆ—è¡¨API] âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼å°è¯•çš„è·¯å¾„:')
            logger.error(f'âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼å°è¯•çš„è·¯å¾„:')
            for i, path in enumerate(possible_model_dirs, 1):
                print(f'  {i}. {path} (å­˜åœ¨: {path.exists()})')
                logger.error(f'  {i}. {path} (å­˜åœ¨: {path.exists()})')
        
        # æŒ‰åˆ›å»ºæ—¶é—´é™åºæ’åˆ—
        models.sort(key=lambda x: x.get('created_at', ''), reverse=True)
        
        print(f'[æ¨¡å‹åˆ—è¡¨API] ğŸ“Š æ€»å…±æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹')
        logger.info(f'ğŸ“Š æ€»å…±æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹')
        if len(models) > 0:
            print(f'[æ¨¡å‹åˆ—è¡¨API] æ¨¡å‹åˆ—è¡¨: {[m["task_id"] for m in models]}')
            logger.info(f'æ¨¡å‹åˆ—è¡¨: {[m["task_id"] for m in models]}')
        
        return jsonify({
            'success': True,
            'models': models,
            'total_count': len(models)
        })
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"[æ¨¡å‹åˆ—è¡¨API] âŒ å¼‚å¸¸å‘ç”Ÿ: {e}")
        print(f"[æ¨¡å‹åˆ—è¡¨API] å¼‚å¸¸å †æ ˆ:\n{error_trace}")
        logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': f'è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500
    finally:
        print("[æ¨¡å‹åˆ—è¡¨API] ===== æ¨¡å‹åˆ—è¡¨è¯·æ±‚å¤„ç†å®Œæˆ =====")
        print("=" * 80)


@anomaly_detection_bp.route('/models/<task_id>/download', methods=['GET'])
def download_model(task_id):
    """ä¸‹è½½æŒ‡å®šæ¨¡å‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆæ‰“åŒ…ä¸ºZIPï¼‰"""
    try:
        models_dir = Path('models/anomaly_detection')
        task_dir = None
        
        # æŸ¥æ‰¾æ¨¡å‹ç›®å½• - éå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•
        if models_dir.exists():
            for model_type_dir in models_dir.iterdir():
                if not model_type_dir.is_dir():
                    continue
                potential_task_dir = model_type_dir / task_id
                if potential_task_dir.exists():
                    task_dir = potential_task_dir
                    break
        
        if not task_dir:
            return jsonify({'error': 'æ¨¡å‹ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        import tempfile
        import zipfile
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:
            with zipfile.ZipFile(tmp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # æ·»åŠ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶åˆ°ZIP
                for file_path in task_dir.glob('*'):
                    if file_path.is_file():
                        zipf.write(file_path, file_path.name)
            
            return send_file(
                tmp_file.name,
                as_attachment=True,
                download_name=f"{task_id}_model.zip",
                mimetype='application/zip'
            )
    
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹å¤±è´¥ {task_id}: {e}")
        return jsonify({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}), 500


@anomaly_detection_bp.route('/models/<task_id>/info', methods=['GET'])
def get_model_info(task_id):
    """è·å–æŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        models_dir = Path('models/anomaly_detection')
        task_dir = None
        
        # æŸ¥æ‰¾æ¨¡å‹ç›®å½• - éå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•
        if models_dir.exists():
            for model_type_dir in models_dir.iterdir():
                if not model_type_dir.is_dir():
                    continue
                potential_task_dir = model_type_dir / task_id
                if potential_task_dir.exists():
                    task_dir = potential_task_dir
                    break
        
        if not task_dir:
            return jsonify({'error': 'æ¨¡å‹ä¸å­˜åœ¨'}), 404
        
        # æ”¶é›†æ‰€æœ‰ä¿¡æ¯
        model_info = {
            'task_id': task_id,
            'model_type': task_dir.parent.name,
            'files': {},
            'config': {},
            'training_logs': []
        }
        
        # é…ç½®æ–‡ä»¶
        config_path = task_dir / 'config.json'
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                model_info['config'] = json.load(f)
        
        # æ–‡ä»¶ä¿¡æ¯
        for file_path in task_dir.glob('*'):
            if file_path.is_file():
                stat = file_path.stat()
                model_info['files'][file_path.name] = {
                    'size': stat.st_size,
                    'created_at': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    'modified_at': datetime.fromtimestamp(stat.st_mtime).isoformat()
                }
        
        return jsonify({
            'success': True,
            'model_info': model_info
        })
        
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥ {task_id}: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}'
        }), 500


@anomaly_detection_bp.route('/models/<task_id>/download_package', methods=['GET'])
def download_model_package(task_id):
    """ä¸‹è½½æŒ‡å®šæ¨¡å‹çš„å®Œæ•´åŒ…ï¼ˆä¾›Edgeç«¯è°ƒç”¨ï¼‰"""
    try:
        models_dir = Path('models/anomaly_detection')
        task_dir = None
        
        # æŸ¥æ‰¾æ¨¡å‹ç›®å½• - éå†æ‰€æœ‰æ¨¡å‹ç±»å‹ç›®å½•
        if models_dir.exists():
            for model_type_dir in models_dir.iterdir():
                if not model_type_dir.is_dir():
                    continue
                potential_task_dir = model_type_dir / task_id
                if potential_task_dir.exists():
                    task_dir = potential_task_dir
                    break
        
        if not task_dir:
            return jsonify({'error': 'æ¨¡å‹ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºä¸´æ—¶ZIPæ–‡ä»¶
        import tempfile
        import zipfile
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:
            with zipfile.ZipFile(tmp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # æ·»åŠ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶åˆ°ZIP
                for file_path in task_dir.glob('*'):
                    if file_path.is_file():
                        zipf.write(file_path, file_path.name)
            
            return send_file(
                tmp_file.name,
                as_attachment=True,
                download_name=f"{task_id}_model_package.zip",
                mimetype='application/zip'
            )
    
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹åŒ…å¤±è´¥ {task_id}: {e}")
        return jsonify({'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'}), 500