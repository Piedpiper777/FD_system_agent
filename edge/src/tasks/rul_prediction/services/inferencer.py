"""
RULé¢„æµ‹æ¨ç†æœåŠ¡
"""

import logging
import numpy as np
import pandas as pd
import mindspore as ms
from typing import Dict, Any, Optional, Union, List
from pathlib import Path
import pickle
import json
from datetime import datetime

# ä»æœ¬åœ° core æ¨¡å—å¯¼å…¥æ¨¡å‹ç±»
from ..core.BiLSTMGRU.model_builder import ModelBuilder as BiLSTMModelBuilder
from ..core.cnn_1d_regressor.model_builder import ModelBuilder as CNN1DModelBuilder
from ..core.transformer_regressor.model_builder import ModelBuilder as TransformerModelBuilder


MODEL_BUILDERS = {
    'bilstm_gru_regressor': BiLSTMModelBuilder,
    'cnn_1d_regressor': CNN1DModelBuilder,
    'transformer_encoder_regressor': TransformerModelBuilder,
}

MODEL_PARAM_KEYS = {
    'bilstm_gru_regressor': {
        'hidden_units', 'num_layers', 'dropout', 'activation', 'bidirectional',
        'use_attention', 'use_layer_norm', 'rnn_type'
    },
    'cnn_1d_regressor': {
        'conv_channels', 'kernel_sizes', 'activation', 'dropout', 'pooling',
        'use_batch_norm', 'fc_units'
    },
    'transformer_encoder_regressor': {
        'embed_dim', 'num_heads', 'num_layers', 'ffn_dim', 'dropout',
        'activation', 'pooling', 'use_positional_encoding'
    }
}


class RULPredictionInferencer:
    """
    æœ¬åœ°RULé¢„æµ‹æ¨ç†æœåŠ¡
    
    è´Ÿè´£ï¼š
    - åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œscaler
    - é¢„å¤„ç†è¾“å…¥æ•°æ®
    - æ‰§è¡Œæ‰¹é‡RULé¢„æµ‹
    - è¿”å›é¢„æµ‹ç»“æœ
    """

    def __init__(self, model_path: Union[str, Path],
                 scaler_path: Optional[Union[str, Path]] = None,
                 label_scaler_path: Optional[Union[str, Path]] = None,
                 config_path: Union[str, Path] = None,
                 sequence_length: int = 50):
        """
        åˆå§‹åŒ–RULé¢„æµ‹æ¨ç†æœåŠ¡
        
        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            scaler_path: ç‰¹å¾æ ‡å‡†åŒ–å™¨æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœæ•°æ®å·²å½’ä¸€åŒ–å¯èƒ½ä¸å­˜åœ¨ï¼‰
            label_scaler_path: æ ‡ç­¾å½’ä¸€åŒ–å™¨æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼Œç”¨äºåå½’ä¸€åŒ–é¢„æµ‹ç»“æœï¼‰
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            sequence_length: åºåˆ—é•¿åº¦
        """
        # è®¾ç½®MindSporeä¸Šä¸‹æ–‡
        ms.set_context(mode=ms.GRAPH_MODE)
        ms.set_device('CPU')

        self.sequence_length = sequence_length
        self.model = None
        self.scaler = None
        self.label_scaler = None
        self.config = None

        # æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)

        # åŠ è½½ç»„ä»¶ï¼ˆæ³¨æ„é¡ºåºï¼šå¿…é¡»å…ˆåŠ è½½configï¼Œå†åŠ è½½modelï¼‰
        # 1. åŠ è½½é…ç½®ï¼ˆå¿…éœ€ï¼Œå› ä¸ºæ¨¡å‹åŠ è½½éœ€è¦é…ç½®ï¼‰
        if config_path:
            self._load_config(config_path)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›config_pathï¼Œå°è¯•ä»æ¨¡å‹ç›®å½•è‡ªåŠ¨æŸ¥æ‰¾
            model_dir = Path(model_path).parent
            potential_config = model_dir / 'model_config.json'
            if potential_config.exists():
                self._load_config(potential_config)
            else:
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸”æ— æ³•åœ¨æ¨¡å‹ç›®å½•ä¸­æ‰¾åˆ°: {model_dir}")
        
        # 2. åŠ è½½ç‰¹å¾scalerï¼ˆå¯é€‰ï¼‰
        if scaler_path:
            self._load_scaler(scaler_path)
        else:
            # å°è¯•ä»æ¨¡å‹ç›®å½•è‡ªåŠ¨æŸ¥æ‰¾scaler
            model_dir = Path(model_path).parent
            potential_scaler = model_dir / 'scaler.pkl'
            if potential_scaler.exists():
                self._load_scaler(potential_scaler)
        
        # 3. åŠ è½½æ ‡ç­¾scalerï¼ˆå¿…éœ€ï¼Œç”¨äºåå½’ä¸€åŒ–ï¼‰
        if label_scaler_path:
            self._load_label_scaler(label_scaler_path)
        else:
            # å°è¯•ä»æ¨¡å‹ç›®å½•è‡ªåŠ¨æŸ¥æ‰¾label_scaler
            model_dir = Path(model_path).parent
            potential_label_scaler = model_dir / 'label_scaler.pkl'
            if potential_label_scaler.exists():
                self._load_label_scaler(potential_label_scaler)
            else:
                raise FileNotFoundError(f"æ ‡ç­¾å½’ä¸€åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸”æ— æ³•åœ¨æ¨¡å‹ç›®å½•ä¸­æ‰¾åˆ°: {model_dir}")
        
        # 4. æœ€ååŠ è½½æ¨¡å‹ï¼ˆéœ€è¦configï¼‰
        self._load_model(model_path)

        print(f"âœ… RULé¢„æµ‹æ¨ç†æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ¨¡å‹: {model_path}")
        if config_path:
            print(f"  - é…ç½®æ–‡ä»¶: {config_path}")
        if scaler_path:
            print(f"  - ç‰¹å¾æ ‡å‡†åŒ–å™¨: {scaler_path}")
        if self.label_scaler:
            print(f"  - æ ‡ç­¾å½’ä¸€åŒ–å™¨: å·²åŠ è½½")
        print(f"  - åºåˆ—é•¿åº¦: {sequence_length}")

    def _load_config(self, config_path: Union[str, Path]):
        """åŠ è½½æ¨¡å‹é…ç½®"""
        try:
            config_path = Path(config_path)
            if not config_path.exists():
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            
            # æ›´æ–°åºåˆ—é•¿åº¦ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
            if 'sequence_length' in self.config:
                self.sequence_length = self.config['sequence_length']
            
            print(f"ğŸ“‚ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def _load_model(self, model_path: Union[str, Path]):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_path = Path(model_path)
            if not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

            model_type = self.config.get('model_type', 'bilstm_gru_regressor')
            input_dim = self.config.get('input_dim', 1)
            input_shape = (self.sequence_length, input_dim)

            builder = MODEL_BUILDERS.get(model_type)
            if builder is None:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

            allowed_keys = MODEL_PARAM_KEYS.get(model_type, set())
            builder_kwargs = {
                key: value for key, value in self.config.items()
                if key in allowed_keys
            }

            self.model = builder.create_model(
                model_type=model_type,
                input_shape=input_shape,
                **builder_kwargs
            )
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise

    def _load_scaler(self, scaler_path: Union[str, Path]):
        """åŠ è½½ç‰¹å¾æ ‡å‡†åŒ–å™¨"""
        try:
            scaler_path = Path(scaler_path)
            if not scaler_path.exists():
                self.logger.warning(f"ç‰¹å¾æ ‡å‡†åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}ï¼Œæ•°æ®å¯èƒ½å·²å½’ä¸€åŒ–")
                return
            
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            print(f"ğŸ“‚ ç‰¹å¾æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ: {scaler_path}")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç‰¹å¾æ ‡å‡†åŒ–å™¨å¤±è´¥: {e}")
            raise
    
    def _load_label_scaler(self, label_scaler_path: Union[str, Path]):
        """åŠ è½½æ ‡ç­¾å½’ä¸€åŒ–å™¨"""
        try:
            label_scaler_path = Path(label_scaler_path)
            if not label_scaler_path.exists():
                raise FileNotFoundError(f"æ ‡ç­¾å½’ä¸€åŒ–å™¨æ–‡ä»¶ä¸å­˜åœ¨: {label_scaler_path}")
            
            with open(label_scaler_path, 'rb') as f:
                self.label_scaler = pickle.load(f)
            
            print(f"ğŸ“‚ æ ‡ç­¾å½’ä¸€åŒ–å™¨åŠ è½½æˆåŠŸ: {label_scaler_path}")
        except Exception as e:
            self.logger.error(f"åŠ è½½æ ‡ç­¾å½’ä¸€åŒ–å™¨å¤±è´¥: {e}")
            raise

    def _create_sequences(self, data: np.ndarray, sequence_length: int, stride: int = 1) -> np.ndarray:
        """
        åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—
        
        Args:
            data: æ•°æ®æ•°ç»„ (N, features)
            sequence_length: åºåˆ—é•¿åº¦
            stride: æ­¥é•¿
            
        Returns:
            sequences: (n_samples, sequence_length, features)
        """
        sequences = []
        
        for i in range(0, len(data) - sequence_length + 1, stride):
            sequences.append(data[i:i + sequence_length])
        
        if len(sequences) == 0:
            return np.array([]).reshape(0, sequence_length, data.shape[1])
        
        return np.array(sequences)

    def predict(self, data: Union[np.ndarray, pd.DataFrame], 
                batch_size: int = 32) -> np.ndarray:
        """
        é¢„æµ‹RULå€¼
        
        Args:
            data: è¾“å…¥æ•°æ® (N, features) æˆ– DataFrame
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            rul_predictions: RULé¢„æµ‹å€¼ (N,)
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(data, pd.DataFrame):
                data = data.values
            
            if len(data) < self.sequence_length:
                raise ValueError(f"æ•°æ®é•¿åº¦ ({len(data)}) å°äºåºåˆ—é•¿åº¦ ({self.sequence_length})")
            
            # åˆ›å»ºåºåˆ—
            sequences = self._create_sequences(data, self.sequence_length, stride=1)
            
            if len(sequences) == 0:
                return np.array([])
            
            # æ ‡å‡†åŒ–ï¼ˆå¦‚æœscalerå­˜åœ¨ï¼‰
            if self.scaler is not None:
                sequences_2d = sequences.reshape(-1, sequences.shape[2])
                sequences_2d_scaled = self.scaler.transform(sequences_2d)
                sequences_scaled = sequences_2d_scaled.reshape(sequences.shape)
            else:
                # æ•°æ®å·²ç»å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨
                sequences_scaled = sequences
            
            # æ‰¹é‡é¢„æµ‹
            all_predictions = []
            
            for i in range(0, len(sequences_scaled), batch_size):
                batch = sequences_scaled[i:i + batch_size]
                batch_tensor = ms.Tensor(batch.astype(np.float32))
                
                # é¢„æµ‹
                with ms._no_grad():
                    predictions = self.model(batch_tensor)
                    predictions_np = predictions.asnumpy()
                
                all_predictions.append(predictions_np.flatten())
            
            # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœï¼ˆå½’ä¸€åŒ–åçš„é¢„æµ‹å€¼ï¼‰
            rul_predictions_normalized = np.concatenate(all_predictions)
            
            # åå½’ä¸€åŒ–ï¼šå°†å½’ä¸€åŒ–çš„é¢„æµ‹å€¼è½¬æ¢å›åŸå§‹RULå°ºåº¦
            if self.label_scaler is not None:
                rul_predictions_normalized_reshaped = rul_predictions_normalized.reshape(-1, 1)
                rul_predictions = self.label_scaler.inverse_transform(rul_predictions_normalized_reshaped).flatten()
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç­¾scalerï¼Œç›´æ¥ä½¿ç”¨é¢„æµ‹å€¼ï¼ˆå¯èƒ½æ˜¯æ—§æ¨¡å‹ï¼‰
                self.logger.warning("æœªæ‰¾åˆ°æ ‡ç­¾å½’ä¸€åŒ–å™¨ï¼Œé¢„æµ‹å€¼å¯èƒ½æœªåå½’ä¸€åŒ–")
                rul_predictions = rul_predictions_normalized
            
            # å¯¹äºæ¯ä¸ªåŸå§‹æ•°æ®ç‚¹ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªåŒ…å«å®ƒçš„åºåˆ—çš„é¢„æµ‹å€¼
            # ç”±äºæˆ‘ä»¬ä½¿ç”¨stride=1ï¼Œæœ€åä¸€ä¸ªåºåˆ—çš„é¢„æµ‹å€¼å¯¹åº”æœ€åä¸€ä¸ªæ•°æ®ç‚¹
            # å¯¹äºå‰é¢çš„ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æ‰©å±•é¢„æµ‹ç»“æœ
            if len(rul_predictions) < len(data):
                # æ‰©å±•ï¼šå‰é¢çš„ç‚¹ä½¿ç”¨ç¬¬ä¸€ä¸ªé¢„æµ‹å€¼ï¼Œåé¢çš„ç‚¹ä½¿ç”¨å¯¹åº”çš„é¢„æµ‹å€¼
                extended_predictions = np.zeros(len(data))
                extended_predictions[:self.sequence_length - 1] = rul_predictions[0]
                extended_predictions[self.sequence_length - 1:] = rul_predictions
                rul_predictions = extended_predictions
            
            return rul_predictions
            
        except Exception as e:
            self.logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            raise

    def predict_rul(self, data_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        é¢„æµ‹RULï¼ˆæ¥å£æ–¹æ³•ï¼‰
        
        Args:
            data_config: æ•°æ®é…ç½®ï¼ŒåŒ…å«:
                - data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
                - batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆå¯é€‰ï¼‰
                
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        try:
            data_file = data_config.get('data_file')
            batch_size = data_config.get('batch_size', 32)
            
            if not data_file:
                raise ValueError("ç¼ºå°‘data_fileå‚æ•°")
            
            # è¯»å–æ•°æ®
            data_path = Path(data_file)
            if not data_path.exists():
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            
            df = pd.read_csv(data_path)
            
            # å‡†å¤‡ç‰¹å¾æ•°æ®ï¼šå·¥å†µä¹Ÿä¼šä½œä¸ºç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼ˆä¸è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´ï¼‰
            condition_keys = self.config.get('condition_keys', [])
            
            # æ’é™¤æ—¶é—´æˆ³åˆ—
            timestamp_col = None
            for col in df.columns:
                if 'time' in col.lower() or 'timestamp' in col.lower():
                    timestamp_col = col
                    break
            
            # å¦‚æœé…ç½®ä¸­æœ‰å·¥å†µï¼Œéœ€è¦è¡¥å…¨å·¥å†µåˆ—
            if condition_keys:
                # ä»æ•°æ®é…ç½®ä¸­è·å–å·¥å†µå€¼ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
                condition_values = data_config.get('condition_values', {})
                for key in condition_keys:
                    if key not in df.columns:
                        if key in condition_values:
                            df[key] = condition_values[key]
                        else:
                            # ä½¿ç”¨é»˜è®¤å€¼ï¼ˆå–é…ç½®ä¸­ç¬¬ä¸€ä¸ªå€¼ï¼‰
                            conditions = self.config.get('conditions', [])
                            for cond in conditions:
                                if isinstance(cond, dict) and cond.get('name') == key:
                                    values = cond.get('values', [])
                                    if values:
                                        df[key] = values[0]
                                    else:
                                        df[key] = 0
                                    break
                            else:
                                df[key] = 0
            
            # åˆ†ç¦»ä¼ æ„Ÿå™¨ç‰¹å¾å’Œå·¥å†µç‰¹å¾ï¼ˆä¸è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´ï¼‰
            exclude_cols = set(condition_keys)
            if timestamp_col:
                exclude_cols.add(timestamp_col)
            sensor_feature_cols = [col for col in df.columns if col not in exclude_cols]
            condition_cols = condition_keys
            
            # æå–ä¼ æ„Ÿå™¨ç‰¹å¾å’Œå·¥å†µç‰¹å¾
            sensor_features = df[sensor_feature_cols].values.astype(np.float32)
            condition_features = df[condition_cols].values.astype(np.float32) if condition_cols else None
            
            # åˆå¹¶ä¼ æ„Ÿå™¨ç‰¹å¾å’Œå·¥å†µç‰¹å¾ï¼ˆå·¥å†µä½œä¸ºç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼‰
            # æœ€ç»ˆç‰¹å¾ = [ä¼ æ„Ÿå™¨ç‰¹å¾, å·¥å†µç‰¹å¾]ï¼ˆä¸è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´ï¼‰
            if condition_features is not None:
                data = np.hstack([sensor_features, condition_features])
            else:
                data = sensor_features
            
            # é¢„æµ‹
            rul_predictions = self.predict(data, batch_size=batch_size)
            
            # æ„å»ºç»“æœ
            result = {
                'success': True,
                'predictions': rul_predictions.tolist(),
                'num_samples': len(rul_predictions),
                'data_file': str(data_file),
                'model_config': {
                    'model_type': self.config.get('model_type'),
                    'sequence_length': self.sequence_length,
                    'input_dim': self.config.get('input_dim'),
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"RULé¢„æµ‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
