"""
å¼‚å¸¸æ£€æµ‹è®­ç»ƒæœåŠ¡ - å¢å¼ºç‰ˆ
æ”¯æŒäº‘ç«¯è®­ç»ƒï¼Œä¼˜åŒ–æ•°æ®ä¼ è¾“ã€ä»»åŠ¡ç®¡ç†å’ŒçŠ¶æ€ç›‘æ§
"""

import json
import requests
import threading
import time
import uuid
import os
from pathlib import Path
from flask import current_app, request
from werkzeug.utils import secure_filename
import logging
from datetime import datetime


class AnomalyDetectionTrainer:
    """å¼‚å¸¸æ£€æµ‹è®­ç»ƒæœåŠ¡ - å¢å¼ºç‰ˆ
    
    ä¸»è¦åŠŸèƒ½:
    - äº‘ç«¯è®­ç»ƒä»»åŠ¡ç®¡ç†å’ŒçŠ¶æ€ç›‘æ§
    - æ–‡ä»¶ä¸Šä¼ å’Œæ•°æ®ä¼ è¾“ä¼˜åŒ–
    - è®­ç»ƒè¿›åº¦å®æ—¶è·Ÿè¸ª
    - é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
    - æ¨¡å‹ä¸‹è½½å’Œæœ¬åœ°ç¼“å­˜
    """

    def __init__(self):
        """åˆå§‹åŒ–è®­ç»ƒæœåŠ¡"""
        # å»¶è¿Ÿè·å–é…ç½®ï¼Œé¿å…åœ¨åº”ç”¨ä¸Šä¸‹æ–‡å¤–è®¿é—® current_app
        self.cloud_url = None
        self.edge_host = None
        self.edge_port = None
        
        # å°è¯•ä»é…ç½®ä¸­è·å–ï¼Œå¦‚æœä¸åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­åˆ™ä½¿ç”¨é»˜è®¤å€¼
        try:
            self.cloud_url = current_app.config.get('CLOUD_BASE_URL', 'http://localhost:5001')
            self.edge_host = current_app.config.get('EDGE_HOST', '10.15.192.149')
            self.edge_port = current_app.config.get('EDGE_PORT', 5000)
        except RuntimeError:
            # ä¸åœ¨ Flask åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.cloud_url = 'http://localhost:5001'
            self.edge_host = '10.15.192.149'
            self.edge_port = 5000
        
        # è®­ç»ƒä»»åŠ¡çŠ¶æ€è¿½è¸ª
        self.training_tasks = {}
        self.task_locks = {}
        
        # çŠ¶æ€ç›‘æ§çº¿ç¨‹
        self.monitoring_thread = None
        self.monitoring_active = False
        
        # æ–‡ä»¶ä¸Šä¼ ç¼“å­˜
        self.uploaded_files = {}
        
        # æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        print(f"ğŸš€ å¼‚å¸¸æ£€æµ‹è®­ç»ƒæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        print(f"  - äº‘ç«¯åœ°å€: {self._get_cloud_url()}")
        print(f"  - è¾¹ç¼˜ç«¯åœ°å€: {self._get_edge_host()}:{self._get_edge_port()}")
    
    def _get_cloud_url(self):
        """è·å–äº‘ç«¯æœåŠ¡URLï¼ˆå»¶è¿Ÿè·å–ï¼Œç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼‰"""
        if self.cloud_url is None:
            try:
                self.cloud_url = current_app.config.get('CLOUD_BASE_URL', 'http://localhost:5001')
            except RuntimeError:
                self.cloud_url = 'http://localhost:5001'
        return self.cloud_url
    
    def _get_edge_host(self):
        """è·å–è¾¹ç¼˜ç«¯IPï¼ˆå»¶è¿Ÿè·å–ï¼Œç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼‰"""
        if self.edge_host is None:
            try:
                self.edge_host = current_app.config.get('EDGE_HOST', '10.15.192.149')
            except RuntimeError:
                self.edge_host = '10.15.192.149'
        return self.edge_host
    
    def _get_edge_port(self):
        """è·å–è¾¹ç¼˜ç«¯ç«¯å£ï¼ˆå»¶è¿Ÿè·å–ï¼Œç¡®ä¿åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼‰"""
        if self.edge_port is None:
            try:
                self.edge_port = current_app.config.get('EDGE_PORT', 5000)
            except RuntimeError:
                self.edge_port = 5000
        return self.edge_port

    def _normalize_device_target(self, value):
        """æ ‡å‡†åŒ–è®¾å¤‡ç±»å‹ï¼Œç¡®ä¿MindSporeè¯†åˆ«"""
        if not value:
            return 'CPU'
        normalized = str(value).strip().lower()
        if normalized in ('gpu', 'cuda'):
            return 'GPU'
        if normalized in ('ascend', 'npu', 'atlas'):
            return 'Ascend'
        return 'CPU'
        
    def start_monitoring(self):
        """å¯åŠ¨è®­ç»ƒçŠ¶æ€ç›‘æ§çº¿ç¨‹"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
            self.monitoring_thread.start()
            self.logger.info("è®­ç»ƒçŠ¶æ€ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢è®­ç»ƒçŠ¶æ€ç›‘æ§çº¿ç¨‹"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
            self.logger.info("è®­ç»ƒçŠ¶æ€ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯ï¼Œå®šæœŸæ›´æ–°è®­ç»ƒçŠ¶æ€"""
        while self.monitoring_active:
            try:
                active_tasks = {tid: task for tid, task in self.training_tasks.items() 
                              if task.get('status') in ['training', 'running']}
                
                if active_tasks:
                    self.logger.info(f"ç›‘æ§ {len(active_tasks)} ä¸ªæ´»è·ƒè®­ç»ƒä»»åŠ¡")
                    
                    for task_id, task in active_tasks.items():
                        # è·å–äº‘ç«¯æœ€æ–°çŠ¶æ€
                        cloud_status = self._get_cloud_training_status(task_id)
                        if cloud_status:
                            with self.task_locks.get(task_id, threading.Lock()):
                                self.training_tasks[task_id].update(cloud_status)
                
                time.sleep(10)  # æ¯10ç§’æ›´æ–°ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(30)  # é”™è¯¯æ—¶ç­‰å¾…30ç§’å†é‡è¯•

    def train(self, model_config):
        """è®­ç»ƒæ¨¡å‹ - å¢å¼ºç‰ˆï¼Œæ”¯æŒæ–‡ä»¶å¤„ç†å’Œæ™ºèƒ½å‚æ•°é…ç½®
        
        Args:
            model_config (dict): è®­ç»ƒé…ç½®ï¼Œæ”¯æŒ:
                - LSTMé¢„æµ‹å™¨ä¸“ç”¨é…ç½®
                - è‡ªåŠ¨å‚æ•°æ¨å¯¼å’ŒéªŒè¯
                - æ™ºèƒ½æ–‡ä»¶å¤„ç†
        
        Returns:
            dict: è®­ç»ƒç»“æœ
        """
        try:
            self.logger.info(f"å¼€å§‹å¤„ç†è®­ç»ƒè¯·æ±‚: {model_config.get('model_type', 'unknown')}")
            
            # å¯åŠ¨ç›‘æ§æœåŠ¡
            self.start_monitoring()
            
            # å‚æ•°éªŒè¯å’Œæ ‡å‡†åŒ–
            validated_config = self._validate_and_normalize_config(model_config)
            if 'error' in validated_config:
                return validated_config
            
            
            # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
            file_processing_result = self._process_training_files(validated_config)
            if 'error' in file_processing_result:
                return file_processing_result
            
            
            # æ›´æ–°é…ç½®ä¸­çš„æ–‡ä»¶è·¯å¾„
            validated_config.update(file_processing_result)
            
            
            # æ‰§è¡Œäº‘ç«¯è®­ç»ƒ
            return self._execute_cloud_training(validated_config)
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'è®­ç»ƒå¤±è´¥: {str(e)}'
            }
    
    def _validate_and_normalize_config(self, config):
        """éªŒè¯å¹¶æ ‡å‡†åŒ–é…ç½®"""
        # åŸºç¡€éªŒè¯
        if not isinstance(config, dict):
            return {'status': 'error', 'message': 'é…ç½®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼'}
        
        # è¾…åŠ©ç±»å‹è½¬æ¢ï¼ˆæ”¹è¿›ç‰ˆï¼šæ­£ç¡®å¤„ç†Noneå’Œç©ºå­—ç¬¦ä¸²ï¼‰
        def to_int(value, default):
            if value is None or value == '':
                return default
            try:
                return int(value)
            except (TypeError, ValueError):
                return default

        def to_float(value, default):
            if value is None or value == '':
                return default
            try:
                return float(value)
            except (TypeError, ValueError):
                return default

        
        # è®¾ç½®é»˜è®¤å€¼ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å€¼ï¼Œåªæœ‰åœ¨å‚æ•°ä¸å­˜åœ¨æ—¶æ‰ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        device_target = self._normalize_device_target(
            config.get('device_target') or config.get('device') or 'CPU'
        )
        normalized = {
            'module': 'anomaly_detection',
            'model_type': config.get('model_type', 'lstm_predictor'),
            'dataset_mode': config.get('dataset_mode', 'processed_file'),
            'epochs': to_int(config.get('epochs'), 50),
            'batch_size': to_int(config.get('batch_size'), 32),
            'learning_rate': to_float(config.get('learning_rate'), 0.001),
            'weight_decay': to_float(config.get('weight_decay'), 0.0001),
            'validation_split': to_float(config.get('validation_split') or config.get('val_ratio'), 0.2),
            'device_target': device_target,
            'sequence_length': to_int(config.get('sequence_length') or config.get('seq_len'), 50),
            'input_dim': to_int(config.get('input_dim'), 38),
            'output_path': config.get('output_path')
        }
        
        # LSTMç‰¹å®šå‚æ•°
        if normalized['model_type'] == 'lstm_predictor':
            bidirectional_raw = config.get('bidirectional', False)
            if isinstance(bidirectional_raw, str):
                bidirectional = bidirectional_raw.lower() in ('true', '1', 'yes', 'y')
            else:
                bidirectional = bool(bidirectional_raw)

            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å€¼ï¼Œåªæœ‰åœ¨å‚æ•°ä¸å­˜åœ¨æ—¶æ‰ä½¿ç”¨é»˜è®¤å€¼
            normalized.update({
                'hidden_units': to_int(config.get('hidden_units') or config.get('hidden_dim'), 128),
                'num_layers': to_int(config.get('num_layers'), 2),
                'dropout': to_float(config.get('dropout'), 0.1),
                'activation': config.get('activation', 'tanh'),
                'bidirectional': bidirectional,
                'prediction_horizon': to_int(config.get('prediction_horizon') or config.get('pred_len'), 1)
            })
            
        
        elif normalized['model_type'] == 'lstm_autoencoder':
            # LSTM Autoencoderç‰¹å®šå‚æ•°
            # æ³¨æ„ï¼šé˜ˆå€¼è®¡ç®—ç›¸å…³å‚æ•°ï¼ˆthreshold_method, percentile, residual_metricï¼‰åœ¨è®­ç»ƒå®Œæˆåå•ç‹¬é…ç½®
            normalized.update({
                'hidden_units': to_int(config.get('hidden_units', 128), 128),
                'num_layers': to_int(config.get('num_layers', 2), 2),
                'bottleneck_size': to_int(config.get('bottleneck_size', 64), 64),
                'dropout': to_float(config.get('dropout', 0.1), 0.1),
                'stride': to_int(config.get('stride', 1), 1),
                'random_seed': to_int(config.get('random_seed', 42), 42)
            })
        
        elif normalized['model_type'] == 'cnn_1d_autoencoder':
            # 1D CNN Autoencoderç‰¹å®šå‚æ•°
            # æ³¨æ„ï¼šé˜ˆå€¼è®¡ç®—ç›¸å…³å‚æ•°ï¼ˆthreshold_method, percentile, residual_metricï¼‰åœ¨è®­ç»ƒå®Œæˆåå•ç‹¬é…ç½®
            normalized.update({
                'num_filters': to_int(config.get('num_filters', 64), 64),
                'kernel_size': to_int(config.get('kernel_size', 3), 3),
                'bottleneck_size': to_int(config.get('bottleneck_size', config.get('bottleneck_dim', 64)), 64),
                'num_conv_layers': to_int(config.get('num_conv_layers', config.get('num_layers', 3)), 3),
                'dropout': to_float(config.get('dropout', 0.1), 0.1),
                'activation': config.get('activation', 'relu'),
                'stride': to_int(config.get('stride', 1), 1),
                'random_seed': to_int(config.get('random_seed', 42), 42)
            })
        
        # æ•°æ®é›†æ‹†åˆ†å‚æ•°ï¼ˆæ ¹æ®æ•°æ®æ¨¡å¼è°ƒæ•´ï¼‰
        dataset_mode = normalized['dataset_mode']
        if dataset_mode == 'processed_file':
            # é¢„å¤„ç†æ–‡ä»¶æ¨¡å¼ï¼šæ ¹æ®validation_splitè®¡ç®—æ¯”ä¾‹å‚æ•°
            validation_split = to_float(config.get('validation_split', 0.2), 0.2)
            normalized['validation_split'] = validation_split
            # ä¼˜å…ˆä½¿ç”¨ dataset_fileï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ data_file
            dataset_file = config.get('dataset_file') or config.get('data_file', '')
            normalized['dataset_file'] = dataset_file
            normalized['data_file'] = dataset_file  # ä¿æŒå…¼å®¹æ€§
            
            # ğŸ”§ æ ¹æ®validation_splitè®¡ç®—æ¯”ä¾‹å‚æ•°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç¡¬ç¼–ç å€¼
            # å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä½¿ç”¨processed_fileæ¨¡å¼ï¼Œåªéœ€è¦train_ratioå’Œval_ratioï¼Œä¸éœ€è¦test_ratio
            train_ratio = 1.0 - validation_split
            normalized['train_ratio'] = train_ratio
            normalized['val_ratio'] = validation_split
            # ä¸è®¾ç½®test_ratioï¼Œå› ä¸ºå¼‚å¸¸æ£€æµ‹æ¨¡å‹ä¸ä½¿ç”¨æµ‹è¯•é›†
        elif dataset_mode == 'condition_filtered':
            # å·¥å†µç­›é€‰æ¨¡å¼ï¼švalidation_split åœ¨åç»­å¤„ç†ä¸­è®¾ç½®
            validation_split = to_float(config.get('validation_split', 0.2), 0.2)
            normalized['validation_split'] = validation_split

        # é¢„å¤„ç†ç­–ç•¥
        if 'preprocess_method' in config:
            normalized['preprocess_method'] = config['preprocess_method']

        # å…¼å®¹æ—§å­—æ®µï¼Œç¡®ä¿ dataset_file / train_file / val_file / test_file å¾—ä»¥ä¿ç•™
        # ä½†ä¸è¦åœ¨ condition_filtered æ¨¡å¼ä¸‹è®¾ç½®è¿™äº›å­—æ®µï¼Œå› ä¸ºè¯¥æ¨¡å¼ä½¿ç”¨ train_files å’Œ test_files
        if dataset_mode != 'condition_filtered':
            for file_key in ['dataset_file', 'train_file', 'val_file', 'test_file']:
                if config.get(file_key):
                    normalized[file_key] = config[file_key]

        # éªŒè¯æ¨¡å‹ç±»å‹
        valid_models = ['lstm_predictor', 'cnn_autoencoder', 'cnn_1d_autoencoder', 'lstm_autoencoder']
        if normalized['model_type'] not in valid_models:
            return {
                'status': 'error',
                'message': f'ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {normalized["model_type"]}ï¼Œæ”¯æŒ: {", ".join(valid_models)}'
            }
        
        # éªŒè¯æ•°æ®é›†æ¨¡å¼ï¼ˆåªæ”¯æŒ processed_file å’Œ condition_filteredï¼‰
        valid_modes = ['processed_file', 'condition_filtered']
        if normalized['dataset_mode'] not in valid_modes:
            return {
                'status': 'error',
                'message': f'ä¸æ”¯æŒçš„æ•°æ®é›†æ¨¡å¼: {normalized["dataset_mode"]}ï¼Œæ”¯æŒ: {", ".join(valid_modes)}'
            }
        
        # å·¥å†µç­›é€‰æ¨¡å¼ï¼šå¤„ç†train_fileså’Œtest_files
        if dataset_mode == 'condition_filtered':
            train_files = config.get('train_files', [])
            test_files = config.get('test_files', [])
            conditions = config.get('conditions', {})
            
            if not train_files or len(train_files) == 0:
                return {
                    'status': 'error',
                    'message': 'å·¥å†µç­›é€‰æ¨¡å¼éœ€è¦è‡³å°‘é€‰æ‹©ä¸€ä¸ªè®­ç»ƒé›†æ–‡ä»¶'
                }
            
            normalized['train_files'] = train_files
            normalized['test_files'] = test_files if test_files else []
            normalized['conditions'] = conditions
            normalized['validation_split'] = to_float(config.get('validation_split', 0.2), 0.2)
        
        self.logger.debug(f"é…ç½®éªŒè¯é€šè¿‡: {normalized['model_type']} ({normalized['dataset_mode']}æ¨¡å¼)")
        return normalized
    
    def _process_training_files(self, config):
        """å¤„ç†è®­ç»ƒæ•°æ®æ–‡ä»¶"""
        dataset_mode = config['dataset_mode']
        processed_files = {}
        
        try:
            if dataset_mode == 'condition_filtered':
                # å·¥å†µç­›é€‰æ¨¡å¼ï¼šä»labeledç›®å½•è¯»å–æ–‡ä»¶å¹¶ä¸Šä¼ 
                train_files = config.get('train_files', [])
                test_files = config.get('test_files', [])
                
                if not train_files:
                    return {
                        'status': 'error',
                        'message': 'å·¥å†µç­›é€‰æ¨¡å¼éœ€è¦è‡³å°‘é€‰æ‹©ä¸€ä¸ªè®­ç»ƒé›†æ–‡ä»¶'
                    }
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä»labeledç›®å½•ï¼‰
                from pathlib import Path
                edge_root = Path(__file__).resolve().parents[4]
                labeled_dir = edge_root / 'data' / 'labeled' / 'AnomalyDetection'
                
                # éªŒè¯è®­ç»ƒé›†æ–‡ä»¶
                for filename in train_files:
                    file_path = labeled_dir / filename
                    if not file_path.exists():
                        return {
                            'status': 'error',
                            'message': f'è®­ç»ƒé›†æ–‡ä»¶ä¸å­˜åœ¨: {filename}'
                        }
                
                # éªŒè¯æµ‹è¯•é›†æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                for filename in test_files:
                    file_path = labeled_dir / filename
                    if not file_path.exists():
                        return {
                            'status': 'error',
                            'message': f'æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {filename}'
                        }
                
                processed_files['train_files'] = train_files
                processed_files['test_files'] = test_files
                processed_files['conditions'] = config.get('conditions', {})
                processed_files['use_condition_filtered'] = True
                self.logger.info(f"å·¥å†µç­›é€‰æ¨¡å¼: {len(train_files)} ä¸ªè®­ç»ƒæ–‡ä»¶, {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
                
            elif dataset_mode == 'processed_file':
                # ä½¿ç”¨å·²é¢„å¤„ç†çš„æ•°æ®æ–‡ä»¶ - LSTM Autoencoderä½¿ç”¨
                data_file = config.get('dataset_file') or config.get('data_file')  # å…ˆå°è¯•dataset_fileï¼Œå†å°è¯•data_file
                if not data_file:
                    return {
                        'status': 'error',
                        'message': 'LSTM Autoencoderæ¨¡å¼éœ€è¦é€‰æ‹©é¢„å¤„ç†çš„æ•°æ®æ–‡ä»¶'
                    }
                
                # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç›¸å¯¹äºedgeç›®å½•çš„è·¯å¾„ï¼‰
                from pathlib import Path
                edge_root = Path(__file__).resolve().parents[4]  # ä» trainer.py åˆ° edge ç›®å½•
                processed_dir = edge_root / 'data' / 'processed' / 'AnomalyDetection'
                file_path = processed_dir / data_file
                if not file_path.exists():
                    return {
                        'status': 'error',
                        'message': f'é€‰æ‹©çš„é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {data_file}'
                    }
                
                processed_files['dataset_file'] = data_file
                processed_files['use_processed_data'] = True  # æ ‡è®°ä½¿ç”¨é¢„å¤„ç†æ•°æ®
                self.logger.info(f"é¢„å¤„ç†æ•°æ®æ–‡ä»¶: {data_file}")
                
            
            return processed_files
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'
            }
    
    def _save_uploaded_file(self, file):
        """å®‰å…¨ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°edge/data/uploadedç›®å½•"""
        if not file or not file.filename:
            raise ValueError("æ— æ•ˆçš„æ–‡ä»¶")
        
        # å®‰å…¨æ–‡ä»¶åå¤„ç†
        filename = secure_filename(file.filename)
        if not filename:
            filename = f"uploaded_data_{int(time.time())}.csv"
        
        # ä½¿ç”¨è§„èŒƒçš„æ•°æ®å­˜å‚¨ç»“æ„ï¼šedge/data/uploaded
        edge_root = Path(__file__).resolve().parents[4]  # ä» trainer.py åˆ° edge ç›®å½•
        data_uploaded_dir = edge_root / 'data' / 'uploaded'
        data_uploaded_dir.mkdir(parents=True, exist_ok=True)
        
        # æ·»åŠ æ—¶é—´æˆ³é¿å…æ–‡ä»¶åå†²çª
        name_parts = filename.rsplit('.', 1)
        if len(name_parts) == 2:
            timestamp = int(time.time())
            filename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
        else:
            filename = f"{filename}_{int(time.time())}"
        
        file_path = data_uploaded_dir / filename
        file.save(str(file_path))
        
        # è®°å½•æ–‡ä»¶ä¿¡æ¯ç”¨äºåç»­ç®¡ç†
        self.uploaded_files[filename] = {
            'path': str(file_path),
            'upload_time': time.time(),
            'size': os.path.getsize(file_path),
            'original_name': file.filename,
            'storage_location': 'edge/data/uploaded'
        }
        
        self.logger.info(f"æ–‡ä»¶ä¿å­˜åˆ°è¾¹ç«¯æ•°æ®ç›®å½•: {file_path} ({self.uploaded_files[filename]['size']} bytes)")
        return str(file_path)
    
    def _execute_cloud_training(self, config):
        """æ‰§è¡Œäº‘ç«¯è®­ç»ƒ - å¢å¼ºç‰ˆ"""
        try:
            # æ‰§è¡Œäº‘ç«¯è®­ç»ƒæµç¨‹
            
            # 1. å…ˆåˆ›å»ºè®­ç»ƒä»»åŠ¡ï¼Œè·å–task_id
                # åˆ›å»ºäº‘ç«¯è®­ç»ƒä»»åŠ¡
            cloud_url = self._get_cloud_url()
            
            # æ·»åŠ è¾¹ç¼˜ç«¯è¿æ¥ä¿¡æ¯
            training_config = config.copy()
            training_config.update({
                'edge_host': self._get_edge_host(),
                'edge_port': self._get_edge_port(),
            })

            if not training_config.get('output_path'):
                timestamp = int(time.time())
                training_config['output_path'] = f"models/{training_config['model_type']}_{timestamp}"
            
            # è®°å½•è®­ç»ƒè¯·æ±‚
            dataset_mode = training_config.get('dataset_mode', 'processed_file')
            self.logger.info(f"åˆ›å»ºäº‘ç«¯è®­ç»ƒä»»åŠ¡: {training_config.get('model_type')} ({dataset_mode}æ¨¡å¼)")
            
            # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºå³å°†å‘é€ç»™Cloudçš„æ–‡ä»¶åˆ—è¡¨
            train_files = training_config.get('train_files', [])
            test_files = training_config.get('test_files', [])
            self.logger.info(f"Edgeç«¯å‡†å¤‡å‘é€ç»™Cloud - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
            self.logger.info(f"Edgeç«¯å‡†å¤‡å‘é€ç»™Cloud - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
            print(f"ğŸ” Edgeç«¯å‡†å¤‡å‘é€ç»™Cloud - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {train_files}")
            print(f"ğŸ” Edgeç«¯å‡†å¤‡å‘é€ç»™Cloud - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {test_files}")
            
            # å…ˆåˆ›å»ºä»»åŠ¡ï¼ˆä¸å¯åŠ¨è®­ç»ƒï¼‰ï¼Œè·å–task_id
            create_response = requests.post(
                f"{cloud_url}/api/anomaly_detection/training",
                json=training_config,
                timeout=30
            )
            
            if create_response.status_code != 200:
                error_msg = f"åˆ›å»ºè®­ç»ƒä»»åŠ¡å¤±è´¥: {create_response.status_code}"
                self.logger.error(error_msg)
                return {'status': 'error', 'message': error_msg}
            
            result = create_response.json()
            if not result.get('success'):
                error_msg = f"åˆ›å»ºè®­ç»ƒä»»åŠ¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                self.logger.error(error_msg)
                return {'status': 'error', 'message': error_msg}
            
            task_id = result.get('task_id')
            if not task_id:
                error_msg = "åˆ›å»ºè®­ç»ƒä»»åŠ¡æˆåŠŸï¼Œä½†æœªè¿”å›task_id"
                self.logger.error(error_msg)
                return {'status': 'error', 'message': error_msg}
            
            # å°†task_idæ·»åŠ åˆ°configä¸­ï¼Œç”¨äºåç»­ä¸Šä¼ æ–‡ä»¶
            config['task_id'] = task_id
            self.logger.info(f"è®­ç»ƒä»»åŠ¡å·²åˆ›å»º: {task_id}")
            
            # ä¸Šä¼ æ•°æ®åˆ°äº‘ç«¯
            upload_success = self._upload_data_to_cloud(config)
            if not upload_success:
                self.logger.error("æ•°æ®ä¸Šä¼ å¤±è´¥ï¼Œç»ˆæ­¢è®­ç»ƒ")
                return {
                    'status': 'error',
                    'message': 'æ•°æ®ä¸Šä¼ åˆ°äº‘ç«¯å¤±è´¥'
                }
            
            # ä¸Šä¼ å®Œæˆåï¼Œæ›´æ–°training_configä¸­çš„train_fileså’Œtest_filesï¼ˆç”¨äºåç»­æ—¥å¿—è®°å½•ï¼‰
            if config.get('train_files'):
                training_config['train_files'] = config['train_files']
            if config.get('test_files'):
                training_config['test_files'] = config['test_files']
            if config.get('conditions'):
                training_config['conditions'] = config['conditions']
            
            self.logger.info("æ•°æ®ä¸Šä¼ å®Œæˆï¼Œè®­ç»ƒå·²å¯åŠ¨")
            
            # ä»»åŠ¡å·²ç»åœ¨åˆ›å»ºæ—¶å¯åŠ¨ï¼Œç›´æ¥ä½¿ç”¨è¿”å›çš„task_id
            # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
            self.training_tasks[task_id] = {
                'task_id': task_id,
                'cloud_task_id': task_id,
                'status': 'running',
                'config': training_config,
                'start_time': time.time(),
                'epoch': 0,
                'total_epochs': training_config.get('epochs', 50),
                'loss': 0.0,
                'progress': 0,
                'message': 'è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨',
                'logs': [
                    f"âœ… è®­ç»ƒä»»åŠ¡åˆ›å»ºæˆåŠŸ (ID: {task_id})",
                    f"ğŸ“Š æ¨¡å‹ç±»å‹: {training_config['model_type']}",
                    f"ğŸ“ˆ è®­ç»ƒè½®æ•°: {training_config['epochs']}",
                    f"ğŸ”„ æ‰¹æ¬¡å¤§å°: {training_config['batch_size']}",
                    f"âš¡ å­¦ä¹ ç‡: {training_config['learning_rate']}",
                    f"ğŸŒ äº‘ç«¯è®­ç»ƒå·²å¯åŠ¨..."
                ],
                'paused': False,
                'created_at': datetime.now().isoformat(),
                'model_path': None,
                'performance_metrics': None
            }
            self.task_locks[task_id] = threading.Lock()
            
            self.logger.info(f"è®­ç»ƒä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")
            
            return {
                'status': 'success',
                'success': True,
                'task_id': task_id,
                'cloud_task_id': task_id,
                'message': f'è®­ç»ƒä»»åŠ¡å·²æäº¤åˆ°äº‘ç«¯ (ID: {task_id})',
                'mode': 'cloud',
                'created_at': datetime.now().isoformat(),
                'estimated_duration': self._estimate_training_duration(training_config)
            }
            
        except requests.exceptions.ConnectionError:
            error_msg = f'æ— æ³•è¿æ¥åˆ°äº‘ç«¯: {self._get_cloud_url()}'
            self.logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}
        except requests.exceptions.Timeout:
            error_msg = 'äº‘ç«¯è¯·æ±‚è¶…æ—¶'
            self.logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}
        except Exception as e:
            error_msg = f'äº‘ç«¯è®­ç»ƒå¼‚å¸¸: {str(e)}'
            self.logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}
    
    def _estimate_training_duration(self, config):
        """ä¼°ç®—è®­ç»ƒæ—¶é•¿"""
        base_time_per_epoch = 30  # ç§’
        epochs = config.get('epochs', 50)
        batch_size = config.get('batch_size', 32)
        
        # æ ¹æ®æ‰¹æ¬¡å¤§å°è°ƒæ•´
        if batch_size < 16:
            multiplier = 1.5
        elif batch_size > 64:
            multiplier = 0.8
        else:
            multiplier = 1.0
        
        estimated_seconds = epochs * base_time_per_epoch * multiplier
        return {
            'estimated_seconds': int(estimated_seconds),
            'estimated_minutes': round(estimated_seconds / 60, 1),
            'estimated_hours': round(estimated_seconds / 3600, 2) if estimated_seconds > 3600 else None
        }

    def _train_via_cloud(self, model_config):
        """é€šè¿‡äº‘ç«¯è¿›è¡Œè®­ç»ƒ - å…¼å®¹æ€§æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # é‡å®šå‘åˆ°æ–°çš„æ‰§è¡Œæ–¹æ³•
        return self._execute_cloud_training(model_config)

    def get_training_status(self, task_id):
        """è·å–è®­ç»ƒçŠ¶æ€ - å¢å¼ºç‰ˆï¼Œæ”¯æŒå®æ—¶çŠ¶æ€å’Œè¯¦ç»†ä¿¡æ¯"""
        try:
            # ä¼˜å…ˆä»äº‘ç«¯è·å–æœ€æ–°çŠ¶æ€
            cloud_status = self._get_cloud_training_status(task_id)
            
            if cloud_status:
                # æ›´æ–°æœ¬åœ°ç¼“å­˜
                if task_id in self.training_tasks:
                    with self.task_locks[task_id]:
                        self.training_tasks[task_id].update(cloud_status)
                
                # æ·»åŠ è¾¹ç¼˜ç«¯ç‰¹æœ‰çš„ä¿¡æ¯
                cloud_status.update({
                    'source': 'cloud',
                    'last_update': time.time(),
                    'edge_cached': task_id in self.training_tasks
                })
                
                return cloud_status
            
            # äº‘ç«¯ä¸å¯è¾¾æ—¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜çŠ¶æ€
            if task_id not in self.training_tasks:
                return {
                    'status': 'error',
                    'message': 'ä»»åŠ¡ä¸å­˜åœ¨ä¸”æ— æ³•è¿æ¥äº‘ç«¯',
                    'source': 'cache',
                    'task_id': task_id
                }
            
            with self.task_locks[task_id]:
                cached_status = self.training_tasks[task_id].copy()
                cached_status.update({
                    'source': 'cache',
                    'last_update': time.time(),
                    'cloud_available': False,
                    'warning': 'ä½¿ç”¨ç¼“å­˜çŠ¶æ€ï¼Œäº‘ç«¯è¿æ¥ä¸å¯ç”¨'
                })
                return cached_status
                
        except Exception as e:
            self.logger.error(f"è·å–è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}',
                'task_id': task_id,
                'source': 'error'
            }
    
    def _get_cloud_training_status(self, task_id):
        """ä»äº‘ç«¯è·å–çœŸå®çš„è®­ç»ƒçŠ¶æ€ - å¢å¼ºç‰ˆ"""
        try:
            # ä½¿ç”¨æ­£ç¡®çš„å¼‚å¸¸æ£€æµ‹è®­ç»ƒçŠ¶æ€APIç«¯ç‚¹
            url = f"{self._get_cloud_url()}/api/anomaly_detection/training_status/{task_id}"
            response = requests.get(
                url, 
                timeout=10,
                headers={'User-Agent': 'EdgeTrainingService/1.0'}
            )
            
            if response.status_code == 200:
                cloud_response = response.json()
                if cloud_response.get('success') and 'task' in cloud_response:
                    cloud_task = cloud_response['task']
                    
                    # è§£æäº‘ç«¯çŠ¶æ€
                    status = cloud_task.get('status', 'unknown')
                    progress = cloud_task.get('progress', 0)
                    message = cloud_task.get('message', '')
                    
                    # æå–è®­ç»ƒæŒ‡æ ‡
                    current_epoch = cloud_task.get('current_epoch', 0)
                    completed_epochs = cloud_task.get('completed_epochs', 0)
                    total_epochs = cloud_task.get('config', {}).get('epochs', 50)
                    
                    # ç›´æ¥ä»äº‘ç«¯è·å–æŸå¤±å€¼
                    loss_value = cloud_task.get('loss') or cloud_task.get('train_loss')
                    val_loss = cloud_task.get('val_loss')
                    
                    # ç¡®ä¿æŸå¤±å€¼æ˜¯æœ‰æ•ˆçš„æ•°å­—
                    if loss_value is not None:
                        try:
                            loss_value = float(loss_value)
                        except (ValueError, TypeError):
                            loss_value = None
                    
                    if val_loss is not None:
                        try:
                            val_loss = float(val_loss)
                        except (ValueError, TypeError):
                            val_loss = None
                    
                    # å¦‚æœè®­ç»ƒå·²å®Œæˆï¼Œä½¿ç”¨completed_epochsä½œä¸ºå½“å‰epoch
                    if status in ['completed', 'threshold_completed'] and completed_epochs > 0:
                        current_epoch = completed_epochs
                    
                    # æ™ºèƒ½è§£æè®­ç»ƒæ¶ˆæ¯ï¼ˆä½œä¸ºfallbackï¼‰
                    if message and 'Epoch' in message and current_epoch == 0:
                        import re
                        # åŒ¹é…epochä¿¡æ¯
                        epoch_match = re.search(r'Epoch (\d+)/(\d+)', message)
                        if epoch_match:
                            current_epoch = int(epoch_match.group(1))
                            total_epochs = int(epoch_match.group(2))
                        
                        # åŒ¹é…æŸå¤±å€¼
                        train_loss_match = re.search(r'Train Loss: ([\d.]+)', message)
                        if train_loss_match:
                            loss_value = float(train_loss_match.group(1))
                        
                        val_loss_match = re.search(r'Val Loss: ([\d.]+)', message)
                        if val_loss_match:
                            val_loss = float(val_loss_match.group(1))
                    else:
                        # ä»æ¶ˆæ¯ä¸­è§£ææŸå¤±å€¼ï¼ˆä¸ä¾èµ–epochè§£æï¼‰
                        if message:
                            import re
                            train_loss_match = re.search(r'Train Loss: ([\d.]+)', message)
                            if train_loss_match:
                                loss_value = float(train_loss_match.group(1))
                            
                            val_loss_match = re.search(r'Val Loss: ([\d.]+)', message)
                            if val_loss_match:
                                val_loss = float(val_loss_match.group(1))
                    
                    # è®¡ç®—è®­ç»ƒè¿›åº¦ç™¾åˆ†æ¯”
                    if total_epochs > 0:
                        epoch_progress = (current_epoch / total_epochs) * 100
                        progress = max(progress, epoch_progress)
                    
                    # è·å–æ¨¡å‹ä¿¡æ¯
                    model_path = cloud_task.get('model_save_path')
                    config = cloud_task.get('config', {})
                    
                    # æ„å»ºè¯¦ç»†çŠ¶æ€å“åº”
                    detailed_status = {
                        'task_id': task_id,
                        'cloud_task_id': cloud_task.get('id'),
                        'status': 'training' if status == 'running' else status,
                        'epoch': current_epoch,
                        'total_epochs': total_epochs,
                        'progress': round(progress, 2),
                        'message': message,
                        'loss': loss_value,
                        'val_loss': val_loss,
                        'logs': cloud_task.get('logs', []) + ([message] if message and message not in cloud_task.get('logs', []) else []),
                        'created_at': cloud_task.get('created_at'),
                        'updated_at': cloud_task.get('updated_at'),
                        'model_path': model_path,
                        'threshold_value': cloud_task.get('threshold_value'),
                        'threshold_path': cloud_task.get('threshold_path'),
                        'threshold_metadata': cloud_task.get('threshold_metadata'),
                        'scaler_path': cloud_task.get('scaler_path'),
                        'config': config,
                        'performance': {
                            'train_loss': loss_value,
                            'val_loss': val_loss,
                            'epoch_progress': f"{current_epoch}/{total_epochs}",
                            'completion_rate': f"{progress:.1f}%"
                        }
                    }
                    
                    # å¦‚æœè®­ç»ƒå®Œæˆï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                    if status in ['completed', 'finished']:
                        detailed_status.update({
                            'completed_at': cloud_task.get('updated_at'),
                            'final_loss': loss_value,
                            'final_val_loss': val_loss,
                            'model_ready': bool(model_path),
                            'download_url': f"/api/models/{os.path.basename(model_path)}/download" if model_path else None
                        })
                    
                    self.logger.debug(f"äº‘ç«¯çŠ¶æ€æ›´æ–° - ä»»åŠ¡ {task_id}: Epoch {current_epoch}/{total_epochs}, è¿›åº¦: {progress:.1f}%")
                    return detailed_status
                    
            elif response.status_code == 404:
                self.logger.warning(f"äº‘ç«¯ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return {
                    'status': 'not_found',
                    'message': 'äº‘ç«¯ä»»åŠ¡ä¸å­˜åœ¨',
                    'task_id': task_id
                }
                
        except requests.exceptions.ConnectionError:
            self.logger.warning(f"äº‘ç«¯è¿æ¥å¤±è´¥: {self._get_cloud_url()}")
        except requests.exceptions.Timeout:
            self.logger.warning("äº‘ç«¯è¯·æ±‚è¶…æ—¶")
        except Exception as e:
            self.logger.error(f"è·å–äº‘ç«¯çŠ¶æ€å¼‚å¸¸: {e}")
        
        return None
    
    def download_model(self, task_id, local_path=None):
        """ä¸‹è½½è®­ç»ƒå®Œæˆçš„æ¨¡å‹"""
        try:
            # è·å–ä»»åŠ¡çŠ¶æ€
            status = self.get_training_status(task_id)
            if not status or status.get('status') != 'completed':
                return {
                    'success': False,
                    'error': 'ä»»åŠ¡æœªå®Œæˆæˆ–ä¸å­˜åœ¨'
                }
            
            model_path = status.get('model_path')
            if not model_path:
                return {
                    'success': False,
                    'error': 'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨'
                }
            
            # ä»äº‘ç«¯è·å–æ¨¡å‹ä¿¡æ¯ï¼Œç¡®å®šæ¨¡å‹ç±»å‹
            try:
                info_response = requests.get(f"{self._get_cloud_url()}/api/anomaly_detection/models/{task_id}/info", timeout=10)
                model_type_dir = 'lstm_prediction'  # é»˜è®¤å€¼
                
                if info_response.status_code == 200:
                    info_data = info_response.json()
                    if info_data.get('success') and 'model_info' in info_data:
                        cloud_model_type = info_data['model_info'].get('model_type', '')
                        # å°†äº‘ç«¯æ¨¡å‹ç±»å‹æ˜ å°„åˆ°æœ¬åœ°ç›®å½•å
                        if cloud_model_type == 'lstm_predictor':
                            model_type_dir = 'lstm_prediction'
                        elif cloud_model_type == 'lstm_autoencoder':
                            model_type_dir = 'lstm_autoencoder'
                        elif cloud_model_type == 'cnn_1d_autoencoder':
                            model_type_dir = 'cnn_1d_autoencoder'
                        else:
                            # å¦‚æœäº‘ç«¯è¿”å›çš„æ˜¯ç›®å½•åæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            model_type_dir = cloud_model_type if cloud_model_type else 'lstm_prediction'
            except Exception as e:
                # å¦‚æœè·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥ï¼Œå›é€€åˆ°ä»statusä¸­è·å–
                self.logger.warning(f"æ— æ³•ä»äº‘ç«¯è·å–æ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                config = status.get('config', {})
                model_type = config.get('model_type', 'lstm_predictor')
                if model_type == 'lstm_autoencoder':
                    model_type_dir = 'lstm_autoencoder'
                elif model_type == 'cnn_1d_autoencoder':
                    model_type_dir = 'cnn_1d_autoencoder'
                else:
                    model_type_dir = 'lstm_prediction'
            
            # ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆä½¿ç”¨download_packageæ¥å£ï¼Œä¸‹è½½å®Œæ•´æ¨¡å‹åŒ…ï¼‰
            download_url = f"{self._get_cloud_url()}/api/anomaly_detection/models/{task_id}/download_package"
            
            response = requests.get(download_url, stream=True, timeout=60)
            if response.status_code == 200:
                # ç¡®å®šæœ¬åœ°ä¿å­˜ç›®å½•
                if local_path is None:
                    # è·å–æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆé¿å…åœ¨åº”ç”¨ä¸Šä¸‹æ–‡å¤–è®¿é—®ï¼‰
                    try:
                        model_folder = current_app.config.get('MODEL_FOLDER', './models')
                    except RuntimeError:
                        # ä¸åœ¨åº”ç”¨ä¸Šä¸‹æ–‡ä¸­ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
                        edge_root = Path(__file__).resolve().parents[4]
                        model_folder = edge_root / 'models'
                    local_path = Path(model_folder) / 'anomaly_detection' / model_type_dir / task_id
                    local_path.mkdir(parents=True, exist_ok=True)
                else:
                    local_path = Path(local_path)
                    local_path.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜ZIPæ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                import tempfile
                import zipfile
                with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as tmp_file:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            tmp_file.write(chunk)
                    tmp_zip_path = tmp_file.name
                
                # è§£å‹ZIPæ–‡ä»¶åˆ°æœ¬åœ°ç›®æ ‡ç›®å½•
                extracted_files = []
                with zipfile.ZipFile(tmp_zip_path, 'r') as zip_file:
                    zip_file.extractall(local_path)
                    extracted_files = zip_file.namelist()
                
                # æ¸…ç†ä¸´æ—¶ZIPæ–‡ä»¶
                os.unlink(tmp_zip_path)
                
                self.logger.info(f"æ¨¡å‹ä¸‹è½½æˆåŠŸ: {local_path} ({len(extracted_files)} ä¸ªæ–‡ä»¶)")
                return {
                    'success': True,
                    'local_path': str(local_path),
                    'files_count': len(extracted_files),
                    'extracted_files': extracted_files
                }
            else:
                return {
                    'success': False,
                    'error': f'ä¸‹è½½å¤±è´¥: HTTP {response.status_code}'
                }
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return {
                'success': False,
                'error': f'ä¸‹è½½å¤±è´¥: {str(e)}'
            }
    
    def get_task_logs(self, task_id, lines=50):
        """è·å–ä»»åŠ¡è¯¦ç»†æ—¥å¿—"""
        try:
            response = requests.get(
                f"{self._get_cloud_url()}/api/training/{task_id}/logs",
                params={'lines': lines},
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    return {
                        'success': True,
                        'logs': result.get('logs', []),
                        'total_lines': len(result.get('logs', []))
                    }
            
            # å›é€€åˆ°æœ¬åœ°æ—¥å¿—
            if task_id in self.training_tasks:
                local_logs = self.training_tasks[task_id].get('logs', [])
                return {
                    'success': True,
                    'logs': local_logs[-lines:] if len(local_logs) > lines else local_logs,
                    'total_lines': len(local_logs),
                    'source': 'local_cache'
                }
            
            return {
                'success': False,
                'error': 'æ— æ³•è·å–æ—¥å¿—'
            }
            
        except Exception as e:
            self.logger.error(f"è·å–æ—¥å¿—å¤±è´¥: {e}")
            return {
                'success': False,
                'error': f'è·å–æ—¥å¿—å¤±è´¥: {str(e)}'
            }
    
    def cleanup_old_files(self, max_age_hours=24):
        """æ¸…ç†è¿‡æœŸçš„ä¸Šä¼ æ–‡ä»¶"""
        try:
            current_time = time.time()
            cleanup_count = 0
            
            for filename, file_info in list(self.uploaded_files.items()):
                age_hours = (current_time - file_info['upload_time']) / 3600
                if age_hours > max_age_hours:
                    try:
                        if os.path.exists(file_info['path']):
                            os.remove(file_info['path'])
                        del self.uploaded_files[filename]
                        cleanup_count += 1
                        self.logger.info(f"æ¸…ç†è¿‡æœŸæ–‡ä»¶: {filename}")
                    except Exception as e:
                        self.logger.error(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
            
            if cleanup_count > 0:
                self.logger.info(f"æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {cleanup_count} ä¸ªæ–‡ä»¶")
            
            return {
                'success': True,
                'cleaned_count': cleanup_count,
                'remaining_count': len(self.uploaded_files)
            }
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶æ¸…ç†å¼‚å¸¸: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    def pause_training(self, task_id):
        """æš‚åœè®­ç»ƒ - å¢å¼ºç‰ˆ"""
        try:
            if task_id in self.training_tasks:
                with self.task_locks[task_id]:
                    self.training_tasks[task_id]['paused'] = True
                    self.training_tasks[task_id]['logs'].append(f'[{time.strftime("%H:%M:%S")}] è®­ç»ƒæš‚åœè¯·æ±‚å·²å‘é€')
                
                # å‘äº‘ç«¯å‘é€æš‚åœè¯·æ±‚
                task = self.training_tasks[task_id]
                cloud_task_id = task.get('cloud_task_id')
                
                if cloud_task_id:
                    try:
                        response = requests.post(
                            f"{self._get_cloud_url()}/api/training/{cloud_task_id}/cancel",
                            timeout=10,
                            headers={'User-Agent': 'EdgeTrainingService/1.0'}
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            if result.get('success'):
                                self.logger.info(f"äº‘ç«¯è®­ç»ƒæš‚åœæˆåŠŸ: {task_id}")
                                return {
                                    'status': 'success', 
                                    'message': 'è®­ç»ƒå·²æš‚åœ',
                                    'cloud_confirmed': True
                                }
                        
                        # äº‘ç«¯æš‚åœå¤±è´¥ï¼Œä½†æœ¬åœ°æ ‡è®°æˆåŠŸ
                        self.logger.warning(f"äº‘ç«¯æš‚åœå“åº”å¼‚å¸¸: {response.status_code}")
                        return {
                            'status': 'partial_success',
                            'message': 'æœ¬åœ°æ ‡è®°ä¸ºæš‚åœï¼Œä½†äº‘ç«¯ç¡®è®¤å¤±è´¥',
                            'cloud_confirmed': False
                        }
                        
                    except Exception as e:
                        self.logger.error(f"äº‘ç«¯æš‚åœè¯·æ±‚å¤±è´¥: {e}")
                        return {
                            'status': 'partial_success',
                            'message': f'æœ¬åœ°æ ‡è®°ä¸ºæš‚åœï¼Œäº‘ç«¯è¯·æ±‚å¤±è´¥: {str(e)}',
                            'cloud_confirmed': False
                        }
                else:
                    return {
                        'status': 'success', 
                        'message': 'è®­ç»ƒå·²æš‚åœï¼ˆæœ¬åœ°ï¼‰',
                        'cloud_confirmed': False
                    }
            else:
                return {'status': 'error', 'message': 'è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨'}
                
        except Exception as e:
            self.logger.error(f"æš‚åœè®­ç»ƒå¤±è´¥: {e}")
            return {'status': 'error', 'message': f'æš‚åœå¤±è´¥: {str(e)}'}

    def stop_training(self, task_id):
        """åœæ­¢è®­ç»ƒ - å¢å¼ºç‰ˆ"""
        try:
            if task_id in self.training_tasks:
                with self.task_locks[task_id]:
                    self.training_tasks[task_id]['status'] = 'stopped'
                    self.training_tasks[task_id]['logs'].append(f'[{time.strftime("%H:%M:%S")}] è®­ç»ƒåœæ­¢è¯·æ±‚å·²å‘é€')
                
                # å‘äº‘ç«¯å‘é€åœæ­¢è¯·æ±‚
                task = self.training_tasks[task_id]
                cloud_task_id = task.get('cloud_task_id')
                
                if cloud_task_id:
                    try:
                        response = requests.post(
                            f"{self._get_cloud_url()}/api/training/{cloud_task_id}/cancel",
                            timeout=10,
                            headers={'User-Agent': 'EdgeTrainingService/1.0'}
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            if result.get('success'):
                                self.logger.info(f"äº‘ç«¯è®­ç»ƒåœæ­¢æˆåŠŸ: {task_id}")
                                
                                # æ¸…ç†æœ¬åœ°æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                                self._cleanup_task_files(task_id)
                                
                                return {
                                    'status': 'success', 
                                    'message': 'è®­ç»ƒå·²åœæ­¢',
                                    'cloud_confirmed': True
                                }
                        
                        self.logger.warning(f"äº‘ç«¯åœæ­¢å“åº”å¼‚å¸¸: {response.status_code}")
                        return {
                            'status': 'partial_success',
                            'message': 'æœ¬åœ°æ ‡è®°ä¸ºåœæ­¢ï¼Œä½†äº‘ç«¯ç¡®è®¤å¤±è´¥',
                            'cloud_confirmed': False
                        }
                        
                    except Exception as e:
                        self.logger.error(f"äº‘ç«¯åœæ­¢è¯·æ±‚å¤±è´¥: {e}")
                        return {
                            'status': 'partial_success',
                            'message': f'æœ¬åœ°æ ‡è®°ä¸ºåœæ­¢ï¼Œäº‘ç«¯è¯·æ±‚å¤±è´¥: {str(e)}',
                            'cloud_confirmed': False
                        }
                else:
                    self._cleanup_task_files(task_id)
                    return {
                        'status': 'success', 
                        'message': 'è®­ç»ƒå·²åœæ­¢ï¼ˆæœ¬åœ°ï¼‰',
                        'cloud_confirmed': False
                    }
            else:
                return {'status': 'error', 'message': 'è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨'}
                
        except Exception as e:
            self.logger.error(f"åœæ­¢è®­ç»ƒå¤±è´¥: {e}")
            return {'status': 'error', 'message': f'åœæ­¢å¤±è´¥: {str(e)}'}
    
    def _cleanup_task_files(self, task_id):
        """æ¸…ç†ä»»åŠ¡ç›¸å…³çš„ä¸´æ—¶æ–‡ä»¶"""
        try:
            if task_id in self.training_tasks:
                task = self.training_tasks[task_id]
                config = task.get('config', {})
                
                # æ¸…ç†ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶
                for key in ['dataset_file', 'train_file', 'val_file', 'test_file']:
                    filename = config.get(key)
                    if filename and filename in self.uploaded_files:
                        file_info = self.uploaded_files[filename]
                        try:
                            if os.path.exists(file_info['path']):
                                os.remove(file_info['path'])
                            del self.uploaded_files[filename]
                            self.logger.info(f"æ¸…ç†ä»»åŠ¡æ–‡ä»¶: {filename}")
                        except Exception as e:
                            self.logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                            
        except Exception as e:
            self.logger.error(f"æ¸…ç†ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_training_summary(self):
        """è·å–è®­ç»ƒä»»åŠ¡æ‘˜è¦"""
        try:
            summary = {
                'total_tasks': len(self.training_tasks),
                'active_tasks': 0,
                'completed_tasks': 0,
                'failed_tasks': 0,
                'paused_tasks': 0,
                'uploaded_files': len(self.uploaded_files),
                'cloud_url': self._get_cloud_url(),
                'monitoring_active': self.monitoring_active
            }
            
            for task in self.training_tasks.values():
                status = task.get('status', 'unknown')
                if status in ['running', 'training']:
                    summary['active_tasks'] += 1
                elif status in ['completed', 'finished']:
                    summary['completed_tasks'] += 1
                elif status in ['failed', 'error']:
                    summary['failed_tasks'] += 1
                elif task.get('paused'):
                    summary['paused_tasks'] += 1
            
            # è®¡ç®—å­˜å‚¨ä½¿ç”¨æƒ…å†µ
            total_storage = sum(file_info['size'] for file_info in self.uploaded_files.values())
            summary['storage_used_mb'] = round(total_storage / (1024 * 1024), 2)
            
            return summary
            
        except Exception as e:
            self.logger.error(f"è·å–è®­ç»ƒæ‘˜è¦å¤±è´¥: {e}")
            return {
                'error': str(e),
                'total_tasks': 0,
                'cloud_url': self.cloud_url
            }
    
    def calculate_threshold(self, task_id, threshold_params=None):
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
        try:
            if threshold_params is None:
                threshold_params = {}
            
            response = requests.post(
                f"{self._get_cloud_url()}/api/anomaly_detection/calculate_threshold/{task_id}",
                json=threshold_params,
                headers={'Content-Type': 'application/json'}
            )
            if response.status_code == 200:
                return response.json()
            else:
                error_data = response.json() if response.content else {'error': 'æœªçŸ¥é”™è¯¯'}
                return {
                    'success': False, 
                    'error': error_data.get('error', f'HTTP {response.status_code}')
                }
        except requests.exceptions.RequestException as e:
            return {'success': False, 'error': f'ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}'}

    def _upload_data_to_cloud(self, config):
        """ä¸Šä¼ è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®åˆ°äº‘ç«¯çš„è®­ç»ƒæ•°æ®ç›®å½•"""
        print("="*50)
        print("ğŸ”¥ _upload_data_to_cloud å‡½æ•°è¢«è°ƒç”¨äº†ï¼")
        try:
            dataset_mode = config.get('dataset_mode', 'processed_file')
            self.logger.info(f"å¼€å§‹ä¸Šä¼ æ•°æ®åˆ°äº‘ç«¯ ({dataset_mode}æ¨¡å¼)")
            
            dataset_mode = config.get('dataset_mode', 'processed_file')
            
            # å·¥å†µç­›é€‰æ¨¡å¼ï¼šä¸Šä¼ å¤šä¸ªæ–‡ä»¶
            if dataset_mode == 'condition_filtered':
                train_files = config.get('train_files', [])
                test_files = config.get('test_files', [])
                
                if not train_files:
                    self.logger.error("å·¥å†µç­›é€‰æ¨¡å¼æ²¡æœ‰æŒ‡å®šè®­ç»ƒæ–‡ä»¶")
                    return False
                
                edge_root = Path(__file__).resolve().parents[4]
                labeled_dir = edge_root / 'data' / 'labeled' / 'AnomalyDetection'
                data_training_dir = edge_root / 'data' / 'training' / 'AnomalyDetection'
                data_training_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¸Šä¼ æ‰€æœ‰è®­ç»ƒæ–‡ä»¶
                uploaded_train_files = []
                for filename in train_files:
                    upload_success = self._upload_single_file_to_cloud(
                        filename, labeled_dir, data_training_dir, config, None, 'train'
                    )
                    if upload_success:
                        uploaded_train_files.append(filename)
                    else:
                        self.logger.warning(f"è®­ç»ƒæ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename}")
                
                if not uploaded_train_files:
                    self.logger.error("æ‰€æœ‰è®­ç»ƒæ–‡ä»¶ä¸Šä¼ å¤±è´¥")
                    return False
                
                # ä¸Šä¼ æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
                uploaded_test_files = []
                for filename in test_files:
                    upload_success = self._upload_single_file_to_cloud(
                        filename, labeled_dir, data_training_dir, config, None, 'test'
                    )
                    if upload_success:
                        uploaded_test_files.append(filename)
                    else:
                        self.logger.warning(f"æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {filename}")
                
                # æ›´æ–°configä¸­çš„æ–‡ä»¶åˆ—è¡¨
                config['train_files'] = uploaded_train_files
                config['test_files'] = uploaded_test_files
                
                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºä¸Šä¼ åçš„æ–‡ä»¶åˆ—è¡¨
                self.logger.info(f"Edgeç«¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {uploaded_train_files}")
                self.logger.info(f"Edgeç«¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {uploaded_test_files}")
                print(f"ğŸ” Edgeç«¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ - è®­ç»ƒæ–‡ä»¶åˆ—è¡¨: {uploaded_train_files}")
                print(f"ğŸ” Edgeç«¯æ–‡ä»¶ä¸Šä¼ å®Œæˆ - æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {uploaded_test_files}")
                
                return True
            
            # åŸæœ‰æ¨¡å¼ï¼šå•ä¸ªæ–‡ä»¶ä¸Šä¼ 
            dataset_file = config.get('dataset_file')
            test_file = config.get('test_file')  # æµ‹è¯•é›†æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            
            print(f"ğŸ”¥ dataset_file = {dataset_file}")
            print(f"ğŸ”¥ test_file = {test_file}")
            self.logger.info(f"ä»configä¸­è·å–çš„dataset_file: {dataset_file}")
            self.logger.info(f"ä»configä¸­è·å–çš„test_file: {test_file}")
            
            if not dataset_file:
                print("ğŸ”¥ æ²¡æœ‰æŒ‡å®šæ•°æ®æ–‡ä»¶ - è¿™æ˜¯é”™è¯¯æ¥æºï¼")
                self.logger.error("æ²¡æœ‰æŒ‡å®šæ•°æ®æ–‡ä»¶")
                return False

            # è·å–è·¯å¾„
            edge_root = Path(__file__).resolve().parents[4]  # ä» trainer.py åˆ° edge ç›®å½•
            data_processed_dir = edge_root / 'data' / 'processed' / 'AnomalyDetection'
            data_training_dir = edge_root / 'data' / 'training' / 'AnomalyDetection'
            data_training_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸Šä¼ è®­ç»ƒæ•°æ®æ–‡ä»¶
            self.logger.info(f"å‡†å¤‡å¤„ç†è®­ç»ƒæ–‡ä»¶: {dataset_file}")
            upload_success = self._upload_single_file_to_cloud(
                dataset_file, data_processed_dir, data_training_dir, config, 'dataset_file'
            )
            if not upload_success:
                return False
            
            # ä¸Šä¼ æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
            if test_file:
                self.logger.info(f"å‡†å¤‡å¤„ç†æµ‹è¯•æ–‡ä»¶: {test_file}")
                test_upload_success = self._upload_single_file_to_cloud(
                    test_file, data_processed_dir, data_training_dir, config, 'test_file'
                )
                if not test_upload_success:
                    self.logger.warning(f"æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œä½†è®­ç»ƒå¯ä»¥ç»§ç»­: {test_file}")
                    # æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å¤±è´¥ä¸å½±å“è®­ç»ƒï¼Œåªæ˜¯ä¸ä¼šæœ‰è¯„ä¼°ç»“æœ
            
            return True
                
        except Exception as e:
            self.logger.error(f"æ•°æ®ä¸Šä¼ å¼‚å¸¸: {e}")
            import traceback
            self.logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return False
    
    def _upload_single_file_to_cloud(self, filename, source_dir, training_dir, config, config_key=None, file_type='train'):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°äº‘ç«¯
        
        Args:
            filename: æ–‡ä»¶å
            source_dir: æºç›®å½•ï¼ˆedge/data/processed/AnomalyDetection æˆ– edge/data/labeled/AnomalyDetectionï¼‰
            training_dir: è®­ç»ƒç›®å½•ï¼ˆedge/data/training/AnomalyDetectionï¼‰
            config: é…ç½®å­—å…¸ï¼Œç”¨äºæ›´æ–°äº‘ç«¯æ–‡ä»¶å
            config_key: é…ç½®ä¸­çš„é”®åï¼ˆdataset_file æˆ– test_fileï¼‰ï¼Œå·¥å†µç­›é€‰æ¨¡å¼æ—¶ä¸ºNone
            file_type: æ–‡ä»¶ç±»å‹ï¼ˆ'train' æˆ– 'test'ï¼‰ï¼Œç”¨äºå·¥å†µç­›é€‰æ¨¡å¼
        
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            import shutil
            
            # ç¬¬ä¸€æ­¥ï¼šä»processedç›®å½•å¤åˆ¶åˆ°edgeçš„trainingç›®å½•
            processed_file_path = source_dir / filename
            self.logger.info(f"æºæ–‡ä»¶è·¯å¾„: {processed_file_path}")
            
            if not processed_file_path.exists():
                self.logger.error(f"é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {processed_file_path}")
                return False
            
            # å¤åˆ¶æ–‡ä»¶åˆ°trainingç›®å½•
            training_file_path = training_dir / filename
            shutil.copy2(processed_file_path, training_file_path)
            self.logger.info(f"æ–‡ä»¶å·²å¤åˆ¶åˆ°trainingç›®å½•: {training_file_path}")
            
            # ç¬¬äºŒæ­¥ï¼šä»edge/data/training/AnomalyDetectionä¸Šä¼ åˆ°cloud/data/ad
            file_size = training_file_path.stat().st_size
            self.logger.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # ä¸Šä¼ åˆ°äº‘ç«¯è®­ç»ƒæ•°æ®ç›®å½•
            upload_url = f"{self._get_cloud_url()}/api/anomaly_detection/upload_data"
            self.logger.info(f"ä¸Šä¼ åˆ°äº‘ç«¯è®­ç»ƒæ•°æ®ç›®å½•: {upload_url}")
            
            # è·å–task_idï¼ˆå¦‚æœå·²åˆ›å»ºä»»åŠ¡ï¼‰
            task_id = config.get('task_id', '')
            
            with open(training_file_path, 'rb') as f:
                files = {
                    'file': (filename, f, 'text/csv')
                }
                data = {}
                if task_id:
                    data['task_id'] = task_id
                
                self.logger.info(f"å¼€å§‹ä¸Šä¼  {filename} åˆ°äº‘ç«¯è®­ç»ƒæ•°æ®ç›®å½•... [task_id: {task_id or 'N/A'}]")
                response = requests.post(
                    upload_url,
                    files=files,
                    data=data,  # ä¼ é€’task_id
                    timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´ç”¨äºå¤§æ–‡ä»¶ä¸Šä¼ 
                    headers={'User-Agent': 'EdgeTrainingService/1.0'}
                )
                
                self.logger.info(f"äº‘ç«¯å“åº”çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    saved_filename = result.get('saved_filename', filename)
                    self.logger.debug(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}")
                    # æ›´æ–°é…ç½®ä¸­çš„æ–‡ä»¶åä¸ºäº‘ç«¯ä¿å­˜çš„æ–‡ä»¶åï¼ˆå¦‚æœæä¾›äº†config_keyï¼‰
                    if config_key:
                        config[config_key] = saved_filename
                    
                    # ä¸Šä¼ å¯¹åº”çš„å…ƒæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    # å…ƒæ•°æ®æ–‡ä»¶åœ¨ meta ç›®å½•ï¼Œä¸åœ¨ source_dir
                    meta_filename = filename.replace('.csv', '.json')
                    edge_root = Path(__file__).resolve().parents[4]
                    meta_dir = edge_root / 'data' / 'meta' / 'AnomalyDetection'
                    meta_file_path = meta_dir / meta_filename
                    
                    if meta_file_path.exists():
                        try:
                            with open(meta_file_path, 'rb') as meta_f:
                                meta_data = {}
                                if task_id:
                                    meta_data['task_id'] = task_id
                                meta_response = requests.post(
                                    self._get_cloud_url() + '/api/anomaly_detection/upload_data',
                                    files={'file': (meta_filename, meta_f, 'application/json')},
                                    data=meta_data,
                                    timeout=300
                                )
                                if meta_response.status_code == 200:
                                    self.logger.debug(f"å…ƒæ•°æ®æ–‡ä»¶ {meta_filename} ä¸Šä¼ æˆåŠŸ")
                                else:
                                    self.logger.warning(f"å…ƒæ•°æ®æ–‡ä»¶ {meta_filename} ä¸Šä¼ å¤±è´¥: HTTP {meta_response.status_code}")
                        except Exception as e:
                            self.logger.warning(f"ä¸Šä¼ å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥ {meta_filename}: {e}")
                    
                    return True
                else:
                    self.logger.error(f"äº‘ç«¯ {config_key} ä¸Šä¼ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                self.logger.error(f"ä¸Šä¼ è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                self.logger.error(f"é”™è¯¯è¯¦æƒ…: {response.text}")
                return False
                
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ {filename} ä¸Šä¼ å¼‚å¸¸: {e}")
            return False
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        try:
            self.stop_monitoring()
        except:
            pass
