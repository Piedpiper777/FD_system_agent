"""
å¼‚å¸¸æ£€æµ‹æ¨ç†æœåŠ¡å®ç°
æ¨¡å‹ç±»ä»æœ¬åœ° core æ¨¡å—å¯¼å…¥ï¼ˆä» Cloud ç«¯å¤åˆ¶ï¼‰
"""

import logging
import numpy as np
import pandas as pd
import mindspore as ms
from typing import Dict, Any, Optional, Union, Tuple, List
from pathlib import Path
import pickle
import json
from datetime import datetime

# ä»æœ¬åœ° core æ¨¡å—å¯¼å…¥æ¨¡å‹ç±»ï¼ˆä» Cloud ç«¯å¤åˆ¶ï¼‰
from ..core.lstm_predictor.model_builder import LSTMPredictor
from ..core.lstm_autoencoder.model_builder import LSTMAutoencoder
from ..core.cnn_1d_autoencoder.model_builder import CNN1DAutoencoder


class LocalAnomalyDetector:
    """
    æœ¬åœ°å¼‚å¸¸æ£€æµ‹å™¨

    è´Ÿè´£ï¼š
    - åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé˜ˆå€¼
    - é¢„å¤„ç†è¾“å…¥æ•°æ®
    - æ‰§è¡Œæ‰¹é‡å¼‚å¸¸æ£€æµ‹
    - è¿”å›æ£€æµ‹ç»“æœ
    """

    def __init__(self, model_path: Union[str, Path],
                 threshold_path: Union[str, Path],
                 scaler_path: Union[str, Path],
                 sequence_length: int = 50,
                 model_type: str = 'lstm_predictor'):
        """
        åˆå§‹åŒ–æœ¬åœ°å¼‚å¸¸æ£€æµ‹å™¨

        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            threshold_path: é˜ˆå€¼æ–‡ä»¶è·¯å¾„
            scaler_path: æ ‡å‡†åŒ–å™¨æ–‡ä»¶è·¯å¾„
            sequence_length: åºåˆ—é•¿åº¦
            model_type: æ¨¡å‹ç±»å‹ ('lstm_predictor', 'lstm_autoencoder', 'cnn_1d_autoencoder')
        """
        # è®¾ç½®MindSporeä¸Šä¸‹æ–‡
        ms.set_context(mode=ms.GRAPH_MODE)
        ms.set_device('CPU')

        self.sequence_length = sequence_length
        self.model_type = model_type
        self.model = None
        self.threshold_value = None
        self.scaler = None
        self._scaler_feature_names = None
        self._scaler_feature_count = None

        # æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(__name__)

        # åŠ è½½ç»„ä»¶
        self._load_model(model_path)
        self._load_threshold(threshold_path)
        self._load_scaler(scaler_path)

        print(f"âœ… æœ¬åœ°å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  - æ¨¡å‹: {model_path}")
        print(f"  - æ¨¡å‹ç±»å‹: {model_type}")
        print(f"  - é˜ˆå€¼: {threshold_path}")
        print(f"  - æ ‡å‡†åŒ–å™¨: {scaler_path}")
        print(f"  - åºåˆ—é•¿åº¦: {sequence_length}")

    def _load_model(self, model_path: Union[str, Path]):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_path = Path(model_path)

            # ä»é…ç½®æ–‡ä»¶ä¸­è·å–æ¨¡å‹å‚æ•°
            config_path = model_path.parent / 'config.json'
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                # è·å–æ¨¡å‹ç±»å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„model_typeï¼Œå¦åˆ™ä»configè¯»å–ï¼‰
                model_type = self.model_type or config.get('model_type', 'lstm_predictor')
                self.model_type = model_type

                # é‡å»ºæ¨¡å‹æ¶æ„ï¼ˆä¼˜å…ˆä½¿ç”¨configä¸­çš„sequence_lengthï¼Œç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                sequence_length = config.get('sequence_length', self.sequence_length)
                # å¦‚æœconfigä¸­æœ‰sequence_lengthï¼Œæ›´æ–°self.sequence_lengthä»¥ç¡®ä¿ä¸€è‡´æ€§
                if 'sequence_length' in config:
                    self.sequence_length = sequence_length
                feature_dim = config.get('feature_dim', 1)  # é»˜è®¤1ä¸ªç‰¹å¾
                input_shape = (sequence_length, feature_dim)

                if model_type == 'lstm_predictor':
                    # LSTM Predictoræ¨¡å‹
                    hidden_units = config.get('hidden_units', 128)
                    num_layers = config.get('num_layers', 2)
                    dropout = config.get('dropout', 0.1)
                    activation = config.get('activation', 'tanh')

                    self.model = LSTMPredictor(
                        input_shape=input_shape,
                        hidden_units=hidden_units,
                        num_layers=num_layers,
                        dropout=dropout,
                        activation=activation
                    )
                elif model_type == 'lstm_autoencoder':
                    # LSTM Autoencoderæ¨¡å‹
                    hidden_units = config.get('hidden_units', 128)
                    num_layers = config.get('num_layers', 2)
                    bottleneck_size = config.get('bottleneck_size', config.get('bottleneck_dim', 64))
                    dropout = config.get('dropout', 0.1)
                    activation = config.get('activation', 'tanh')

                    self.model = LSTMAutoencoder(
                        input_shape=input_shape,
                        hidden_units=hidden_units,
                        bottleneck_size=bottleneck_size,
                        num_layers=num_layers,
                        dropout=dropout,
                        activation=activation
                    )
                elif model_type == 'cnn_1d_autoencoder':
                    # 1D CNN Autoencoderæ¨¡å‹
                    num_filters = config.get('num_filters', 64)
                    kernel_size = config.get('kernel_size', 3)
                    bottleneck_size = config.get('bottleneck_size', config.get('bottleneck_dim', 64))
                    num_conv_layers = config.get('num_conv_layers', config.get('num_layers', 3))
                    dropout = config.get('dropout', 0.1)
                    activation = config.get('activation', 'relu')

                    self.model = CNN1DAutoencoder(
                        input_shape=input_shape,
                        num_filters=num_filters,
                        kernel_size=kernel_size,
                        bottleneck_size=bottleneck_size,
                        num_conv_layers=num_conv_layers,
                        dropout=dropout,
                        activation=activation
                    )
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

                # åŠ è½½æ¨¡å‹æƒé‡
                if model_path.exists():
                    ms.load_checkpoint(str(model_path), self.model)
                    print(f"ğŸ“‚ æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ: {model_path} (ç±»å‹: {model_type})")
                else:
                    print(f"âš ï¸ æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            else:
                print(f"âš ï¸ æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model = None

    def _load_threshold(self, threshold_path: Union[str, Path]):
        """åŠ è½½é˜ˆå€¼"""
        try:
            with open(threshold_path, 'r', encoding='utf-8') as f:
                threshold_data = json.load(f)
                self.threshold_value = threshold_data.get(
                    'threshold_value',
                    threshold_data.get('threshold', 0.5)
                )
            print(f"ğŸ“‚ é˜ˆå€¼åŠ è½½æˆåŠŸ: {self.threshold_value}")
        except Exception as e:
            print(f"âš ï¸ é˜ˆå€¼åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5: {e}")
            self.threshold_value = 0.5

    def _load_scaler(self, scaler_path: Union[str, Path]):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        try:
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
                self._scaler_feature_names = getattr(self.scaler, 'feature_names_in_', None)
                self._scaler_feature_count = getattr(self.scaler, 'n_features_in_', None)
            print(f"ğŸ“‚ æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
            self.scaler = None
            self._scaler_feature_names = None
            self._scaler_feature_count = None

    def _align_features_with_scaler(self, feature_df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿è¾“å…¥ç‰¹å¾ä¸è®­ç»ƒé˜¶æ®µçš„æ ‡å‡†åŒ–å™¨ç»´åº¦ä¸€è‡´"""
        if self.scaler is None:
            return feature_df

        aligned_df = feature_df.copy()

        if self._scaler_feature_names is not None and len(self._scaler_feature_names) > 0:
            # æ ¹æ®ç‰¹å¾åå¯¹é½ï¼Œç¼ºå¤±çš„åˆ—ç”¨0å¡«å……ï¼Œå¤šä½™çš„åˆ—ä¸¢å¼ƒ
            for name in self._scaler_feature_names:
                if name not in aligned_df.columns:
                    aligned_df[name] = 0.0
            aligned_df = aligned_df[list(self._scaler_feature_names)]
            return aligned_df

        if self._scaler_feature_count is not None:
            current_count = aligned_df.shape[1]
            if current_count < self._scaler_feature_count:
                missing = self._scaler_feature_count - current_count
                for idx in range(missing):
                    aligned_df[f'feature_pad_{idx+1}'] = 0.0
            elif current_count > self._scaler_feature_count:
                aligned_df = aligned_df.iloc[:, :self._scaler_feature_count]

        return aligned_df

    def preprocess_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œåˆ›å»ºåºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼

        Args:
            data: è¾“å…¥æ•°æ® DataFrame

        Returns:
            å…ƒç»„ (sequences, targets)
            - sequences: è¾“å…¥åºåˆ— (n_sequences, sequence_length, n_features)
            - targets: ç›®æ ‡å€¼
              - å¯¹äºpredictoræ¨¡å‹: (n_sequences, n_features) - ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼
              - å¯¹äºautoencoderæ¨¡å‹: (n_sequences, sequence_length, n_features) - é‡æ„ç›®æ ‡ï¼ˆä¸è¾“å…¥ç›¸åŒï¼‰
        """
        # ç§»é™¤æ—¶é—´æˆ³åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'timestamp' in data.columns:
            feature_data = data.drop('timestamp', axis=1)
        else:
            feature_data = data

        # å¯¹é½ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶çš„æ ‡å‡†åŒ–å™¨
        feature_data = self._align_features_with_scaler(feature_data)

        # æ ‡å‡†åŒ–
        if self.scaler is not None:
            feature_array = self.scaler.transform(feature_data.values)
        else:
            feature_array = feature_data.values

        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºä¸åŒçš„åºåˆ—å’Œç›®æ ‡
        is_autoencoder = self.model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']
        
        sequences = []
        targets = []
        
        if is_autoencoder:
            # è‡ªç¼–ç å™¨ï¼šè¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯åºåˆ—
            for i in range(len(feature_array) - self.sequence_length + 1):
                sequence = feature_array[i:i + self.sequence_length]
                sequences.append(sequence)
                targets.append(sequence)  # ç›®æ ‡ä¸è¾“å…¥ç›¸åŒ
        else:
            # é¢„æµ‹å™¨ï¼šè¾“å…¥æ˜¯åºåˆ—ï¼Œè¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
            for i in range(len(feature_array) - self.sequence_length):
                sequence = feature_array[i:i + self.sequence_length]
                sequences.append(sequence)
                target = feature_array[i + self.sequence_length]
                targets.append(target)

        if not sequences:
            min_samples = self.sequence_length if is_autoencoder else self.sequence_length + 1
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ {min_samples} ä¸ªæ ·æœ¬")

        return np.array(sequences), np.array(targets)

    def detect_anomalies(self, sequences: np.ndarray, actual_targets: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        æ‰§è¡Œå¼‚å¸¸æ£€æµ‹

        æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼š
        - Predictoræ¨¡å‹ï¼šæ ¹æ®å‰sequence_lengthä¸ªæ—¶é—´ç‚¹é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œè®¡ç®—é¢„æµ‹è¯¯å·®
        - Autoencoderæ¨¡å‹ï¼šé‡æ„è¾“å…¥åºåˆ—ï¼Œè®¡ç®—é‡æ„è¯¯å·®

        Args:
            sequences: è¾“å…¥åºåˆ— (n_sequences, sequence_length, n_features)
            actual_targets: å®é™…ç›®æ ‡å€¼
              - Predictor: (n_sequences, n_features)
              - Autoencoder: (n_sequences, sequence_length, n_features)

        Returns:
            å…ƒç»„ (predictions, anomaly_scores, anomaly_flags)
        """
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ‰§è¡Œæ¨ç†")

            # è®¾ç½®æ¨ç†æ¨¡å¼
            self.model.set_train(False)

            n_sequences = len(sequences)
            predictions = []

            # æ‰¹é‡æ¨ç†
            batch_size = 32
            for i in range(0, n_sequences, batch_size):
                batch_sequences = sequences[i:i + batch_size]
                batch_tensor = ms.Tensor(batch_sequences.astype(np.float32))

                # æ¨¡å‹é¢„æµ‹
                batch_predictions = self.model(batch_tensor)
                predictions.extend(batch_predictions.asnumpy())

            predictions = np.array(predictions)

            # æ ¹æ®æ¨¡å‹ç±»å‹è®¡ç®—å¼‚å¸¸åˆ†æ•°
            is_autoencoder = self.model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']
            
            if is_autoencoder:
                # è‡ªç¼–ç å™¨ï¼šè®¡ç®—é‡æ„è¯¯å·®ï¼ˆRMSEï¼‰
                # predictionså’Œactual_targetséƒ½æ˜¯ (n_sequences, sequence_length, n_features)
                residuals = actual_targets - predictions
                # å¯¹æ¯ä¸ªåºåˆ—è®¡ç®—RMSE
                anomaly_scores = np.sqrt(np.mean(residuals ** 2, axis=(1, 2)))
            else:
                # é¢„æµ‹å™¨ï¼šè®¡ç®—é¢„æµ‹è¯¯å·®ï¼ˆL2èŒƒæ•°ï¼‰
                # predictionså’Œactual_targetséƒ½æ˜¯ (n_sequences, n_features)
                anomaly_scores = np.linalg.norm(actual_targets - predictions, axis=1)

            # æ ¹æ®é˜ˆå€¼åˆ¤æ–­å¼‚å¸¸
            anomaly_flags = anomaly_scores > self.threshold_value

            return predictions, anomaly_scores, anomaly_flags

        except Exception as e:
            self.logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            raise

    def run_inference(self, data_path: Union[str, Path],
                     batch_size: int = 32) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´æ¨ç†æµç¨‹

        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            æ¨ç†ç»“æœå­—å…¸
        """
        try:
            # è¯»å–æ•°æ®
            data = pd.read_csv(data_path)
            total_samples = len(data)

            # é¢„å¤„ç†æ•°æ®ï¼šè·å–åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
            sequences, targets = self.preprocess_data(data)
            n_sequences = len(sequences)

            # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
            predictions, anomaly_scores, anomaly_flags = self.detect_anomalies(sequences, targets)

            # ç»Ÿè®¡ç»“æœ
            anomalies_detected = int(np.sum(anomaly_flags))
            anomaly_percentage = (anomalies_detected / n_sequences) * 100 if n_sequences > 0 else 0

            # ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆå¯¹åº”é¢„æµ‹çš„æ—¶é—´ç‚¹ï¼‰
            # å¯¹äºautoencoderæ¨¡å‹ï¼Œåºåˆ—ä»ç´¢å¼•0å¼€å§‹ï¼Œæ¯ä¸ªåºåˆ—å¯¹åº”å…¶æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
            # å¯¹äºpredictoræ¨¡å‹ï¼Œåºåˆ—ä»ç´¢å¼•0å¼€å§‹ï¼Œæ¯ä¸ªåºåˆ—å¯¹åº”ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
            if 'timestamp' in data.columns:
                if self.model_type in ['lstm_autoencoder', 'cnn_1d_autoencoder']:
                    # Autoencoder: æ¯ä¸ªåºåˆ—å¯¹åº”å…¶æœ€åä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆç´¢å¼• sequence_length - 1, sequence_length, ..., len-1ï¼‰
                    start_idx = self.sequence_length - 1
                else:
                    # Predictor: æ¯ä¸ªåºåˆ—å¯¹åº”ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆç´¢å¼• sequence_length, sequence_length+1, ..., len-1ï¼‰
                    start_idx = self.sequence_length
                
                # ç¡®ä¿ç´¢å¼•ä¸è¶…å‡ºèŒƒå›´
                if start_idx + n_sequences <= len(data):
                    timestamps = data['timestamp'].iloc[start_idx:start_idx + n_sequences].tolist()
                else:
                    # å¦‚æœæ•°æ®ä¸å¤Ÿï¼Œåªå–èƒ½å–åˆ°çš„éƒ¨åˆ†
                    available_len = len(data) - start_idx
                    timestamps = data['timestamp'].iloc[start_idx:].tolist() if available_len > 0 else []
                    # å¦‚æœæ—¶é—´æˆ³æ•°é‡ä¸å¤Ÿï¼Œç”¨æœ€åä¸€ä¸ªæ—¶é—´æˆ³å¡«å……
                    if len(timestamps) < n_sequences and len(timestamps) > 0:
                        last_timestamp = timestamps[-1]
                        timestamps.extend([last_timestamp] * (n_sequences - len(timestamps)))
            else:
                # ç”Ÿæˆæ¨¡æ‹Ÿæ—¶é—´æˆ³
                timestamps = pd.date_range(
                    start='2024-01-01 00:00:00',
                    periods=n_sequences,
                    freq='1H'
                ).strftime('%Y-%m-%d %H:%M:%S').tolist()

            # æ„å»ºç»“æœ
            result = {
                'inference_id': f"inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'total_samples': total_samples,
                'sequences_count': n_sequences,
                'anomalies_detected': anomalies_detected,
                'anomaly_percentage': round(anomaly_percentage, 2),
                'threshold_value': self.threshold_value,
                'timestamps': timestamps,
                'anomaly_scores': anomaly_scores.tolist(),
                'anomaly_flags': anomaly_flags.tolist(),
                'predictions': predictions.tolist(),
                'actual_targets': targets.tolist(),
                'statistics': {
                    'mean_score': float(np.mean(anomaly_scores)),
                    'std_score': float(np.std(anomaly_scores)),
                    'min_score': float(np.min(anomaly_scores)),
                    'max_score': float(np.max(anomaly_scores)),
                    'median_score': float(np.median(anomaly_scores))
                },
                'processing_location': 'edge',
                'created_at': datetime.now().isoformat()
            }

            return result

        except Exception as e:
            self.logger.error(f"æ¨ç†æ‰§è¡Œå¤±è´¥: {e}")
            raise