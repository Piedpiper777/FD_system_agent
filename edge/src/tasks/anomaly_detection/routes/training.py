"""
å¼‚å¸¸æ£€æµ‹è®­ç»ƒè·¯ç”±
"""

import json
import os
from datetime import datetime
from pathlib import Path
from flask import Blueprint, request, jsonify, render_template
from ..services.trainer import AnomalyDetectionTrainer
from ....utils.parameter_validator import validate_training_config

ad_training_bp = Blueprint('ad_training', __name__, url_prefix='/anomaly_detection')

# å»¶è¿Ÿåˆå§‹åŒ–è®­ç»ƒæœåŠ¡
_trainer = None

def get_trainer():
    """è·å–è®­ç»ƒæœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _trainer
    if _trainer is None:
        _trainer = AnomalyDetectionTrainer()
    return _trainer


@ad_training_bp.route('/train', methods=['GET'])
def train_page():
    """å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒé¡µé¢"""
    return render_template('anomaly_detection/train.html')


def _normalize_training_payload(payload: dict) -> dict:
    """å°†å‰ç«¯ä¼ å…¥çš„åµŒå¥—é…ç½®å±•å¼€å¹¶è½¬æ¢ä¸ºè®­ç»ƒå™¨éœ€è¦çš„å­—æ®µ"""
    print("ğŸ”¥ğŸ”¥ _normalize_training_payloadè¢«è°ƒç”¨")
    print(f"ğŸ”¥ åŸå§‹payload: {payload}")
    
    if not isinstance(payload, dict):
        return {}

    merged: dict = {}

    section_keys = ('model_config', 'training_config', 'dataset_config')
    for section_key in section_keys:
        section = payload.get(section_key)
        if isinstance(section, dict):
            merged.update(section)

    for key, value in payload.items():
        if key in section_keys:
            continue
        if key not in merged:
            merged[key] = value

    print(f"ğŸ”¥ åˆå¹¶åçš„merged (åœ¨setdefaultä¹‹å‰): {merged}")

    if isinstance(merged.get('bidirectional'), str):
        merged['bidirectional'] = merged['bidirectional'].strip().lower() in {'true', '1', 'yes', 'y'}

    int_fields = ['epochs', 'batch_size', 'sequence_length', 'input_dim', 'hidden_units', 'num_layers', 'prediction_horizon', 'num_filters', 'kernel_size', 'bottleneck_size', 'num_conv_layers', 'stride']
    float_fields = ['learning_rate', 'weight_decay', 'dropout', 'train_ratio', 'val_ratio', 'test_ratio', 'val_ratio_from_train', 'validation_split']

    for field in int_fields:
        if field in merged and merged[field] not in (None, ''):
            try:
                merged[field] = int(merged[field])
            except (TypeError, ValueError):
                pass

    for field in float_fields:
        if field in merged and merged[field] not in (None, ''):
            try:
                merged[field] = float(merged[field])
            except (TypeError, ValueError):
                pass

    merged.setdefault('module', 'anomaly_detection')
    merged.setdefault('model_type', 'lstm_predictor')
    # æ³¨é‡Šæ‰å¼ºåˆ¶è®¾ç½®dataset_modeä¸º'one'ï¼Œè®©å‰ç«¯çš„'processed_file'æ¨¡å¼ä¿æŒ
    # merged.setdefault('dataset_mode', 'one')

    # å¯¹äºcondition_filteredæ¨¡å¼ï¼Œæ ¹æ®validation_splitè®¡ç®—train_ratioå’Œval_ratio
    # è¿™æ ·éªŒè¯å™¨å°±èƒ½æ‰¾åˆ°è¿™äº›å¿…å¡«å­—æ®µ
    dataset_mode = merged.get('dataset_mode', 'processed_file')
    if dataset_mode == 'condition_filtered':
        validation_split = merged.get('validation_split', 0.2)
        if validation_split not in (None, ''):
            try:
                validation_split = float(validation_split)
                merged['train_ratio'] = 1.0 - validation_split
                merged['val_ratio'] = validation_split
            except (TypeError, ValueError):
                pass
    elif dataset_mode == 'processed_file':
        # processed_fileæ¨¡å¼ä¹Ÿéœ€è¦è®¡ç®—train_ratioå’Œval_ratio
        validation_split = merged.get('validation_split', 0.2)
        if validation_split not in (None, ''):
            try:
                validation_split = float(validation_split)
                merged['train_ratio'] = 1.0 - validation_split
                merged['val_ratio'] = validation_split
            except (TypeError, ValueError):
                pass

    print(f"ğŸ”¥ æœ€ç»ˆmerged (åœ¨setdefaultä¹‹å): {merged}")

    if not merged.get('output_path'):
        merged['output_path'] = f"models/{merged['model_type']}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"

    print(f"ğŸ”¥ è¿”å›çš„merged: {merged}")
    return merged


@ad_training_bp.route('/api/train', methods=['POST'])
def train_model():
    """å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒAPI"""
    print("ğŸš€ /api/trainè·¯ç”±è¢«è°ƒç”¨")
    try:
        model_config = request.get_json()
        print(f"ğŸš€ åŸå§‹request.get_json(): {model_config}")
        if not model_config:
            return jsonify({
                'status': 'error',
                'message': 'æ— æ•ˆçš„é…ç½®æ•°æ®',
                'validation': {
                    'is_valid': False,
                    'errors': ['è¯·æ±‚ä¸­æ²¡æœ‰é…ç½®æ•°æ®'],
                    'warnings': [],
                    'suggestions': []
                }
            }), 400

        print("ğŸš€ å³å°†è°ƒç”¨_normalize_training_payload")
        normalized_payload = _normalize_training_payload(model_config)
        print(f"ğŸš€ _normalize_training_payloadè¿”å›: {normalized_payload}")

        # è¿›è¡Œå‚æ•°éªŒè¯
        validation_result = validate_training_config(normalized_payload)

        if not validation_result['is_valid']:
            print('å‚æ•°éªŒè¯å¤±è´¥ï¼Œvalidation_result =', validation_result)
            return jsonify({
                'status': 'validation_error',
                'message': 'å‚æ•°éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥å‚æ•°',
                'validation': validation_result
            }), 422

        trainer = get_trainer()
        result = trainer.train(normalized_payload)

        if validation_result['warnings'] or validation_result['suggestions']:
            result['validation'] = validation_result

        return jsonify(result)

    except ValueError as ve:
        return jsonify({
            'status': 'error',
            'message': f'å‚æ•°é”™è¯¯: {str(ve)}',
            'validation': {
                'is_valid': False,
                'errors': [str(ve)],
                'warnings': [],
                'suggestions': []
            }
        }), 400

    except Exception as e:
        print(f"Anomaly detection training error: {e}")
        return jsonify({
            'status': 'error',
            'message': f'è®­ç»ƒå¤±è´¥: {str(e)}',
            'validation': {
                'is_valid': False,
                'errors': [f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'],
                'warnings': [],
                'suggestions': ['è¯·æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—æˆ–è”ç³»ç®¡ç†å‘˜']
            }
        }), 500


@ad_training_bp.route('/training_progress', methods=['GET'])
def training_progress_page():
    """è®­ç»ƒè¿›åº¦ç›‘æ§é¡µé¢"""
    return render_template('anomaly_detection/training_progress.html')


@ad_training_bp.route('/api/training_status/<task_id>', methods=['GET'])
def get_training_status(task_id):
    """è·å–è®­ç»ƒçŠ¶æ€API"""
    try:
        trainer = get_trainer()
        status = trainer.get_training_status(task_id)
        return jsonify(status)
    except Exception as e:
        print(f"Get training status error: {e}")
        return jsonify({
            'status': 'error',
            'message': f'è·å–è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/pause_training/<task_id>', methods=['POST'])
def pause_training(task_id):
    """æš‚åœè®­ç»ƒAPI"""
    try:
        trainer = get_trainer()
        result = trainer.pause_training(task_id)
        return jsonify(result)
    except Exception as e:
        print(f"Pause training error: {e}")
        return jsonify({
            'status': 'error',
            'message': f'æš‚åœè®­ç»ƒå¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/stop_training/<task_id>', methods=['POST'])
def stop_training(task_id):
    """åœæ­¢è®­ç»ƒAPI"""
    try:
        trainer = get_trainer()
        result = trainer.stop_training(task_id)
        return jsonify(result)
    except Exception as e:
        print(f"Stop training error: {e}")
        return jsonify({
            'status': 'error',
            'message': f'åœæ­¢è®­ç»ƒå¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/validate', methods=['POST'])
def validate_parameters():
    """å‚æ•°éªŒè¯APIï¼ˆç‹¬ç«‹éªŒè¯æ¥å£ï¼‰"""
    try:
        # è·å–JSONé…ç½®
        config = request.get_json()
        if not config:
            return jsonify({
                'is_valid': False,
                'errors': ['è¯·æ±‚ä¸­æ²¡æœ‰é…ç½®æ•°æ®'],
                'warnings': [],
                'suggestions': []
            }), 400

        # è¿›è¡Œå‚æ•°éªŒè¯
        validation_result = validate_training_config(config)
        return jsonify(validation_result)

    except Exception as e:
        print(f"Parameter validation error: {e}")
        return jsonify({
            'is_valid': False,
            'errors': [f'éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}'],
            'warnings': [],
            'suggestions': []
        }), 500


@ad_training_bp.route('/api/calculate_threshold/<task_id>', methods=['POST'])
def calculate_threshold_proxy(task_id):
    """é˜ˆå€¼è®¡ç®—APIä»£ç†ï¼ˆè½¬å‘åˆ°äº‘ç«¯ï¼‰"""
    try:
        # è·å–å‰ç«¯å‘é€çš„é˜ˆå€¼å‚æ•°
        threshold_params = request.get_json() or {}
        trainer = get_trainer()
        result = trainer.calculate_threshold(task_id, threshold_params)
        return jsonify(result)
    except Exception as e:
        print(f"Calculate threshold error: {e}")
        return jsonify({
            'success': False,
            'error': f'é˜ˆå€¼è®¡ç®—å¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/processed_data', methods=['GET'])
def get_processed_data_files():
    """è·å–å·²æ ‡æ³¨çš„æ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼ˆä»labeledç›®å½•ï¼‰"""
    try:
        # æ•°æ®æ–‡ä»¶ç›®å½• - ä½¿ç”¨labeledç›®å½•
        edge_root = Path(__file__).resolve().parents[4]  # ä» training.py åˆ° edge ç›®å½•
        labeled_dir = edge_root / 'data' / 'labeled' / 'AnomalyDetection'
        
        if not labeled_dir.exists():
            return jsonify({
                'success': True,
                'files': [],
                'message': 'æ ‡æ³¨æ•°æ®ç›®å½•ä¸å­˜åœ¨ (edge/data/labeled/AnomalyDetection)'
            })
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        files = []
        for filename in os.listdir(labeled_dir):
            if filename.endswith('.csv'):
                file_path = labeled_dir / filename
                file_stat = file_path.stat()
                
                # è§£ææ–‡ä»¶åè·å–ä¿¡æ¯
                file_info = {
                    'filename': filename,
                    'display_name': filename,
                    'size': file_stat.st_size,
                    'modified_time': datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # å°è¯•ä»å…ƒæ•°æ®æ–‡ä»¶è·å–æ ‡ç­¾ä¿¡æ¯
                meta_file_path = edge_root / 'data' / 'meta' / 'AnomalyDetection' / (filename.replace('.csv', '.json'))
                if meta_file_path.exists():
                    try:
                        with open(meta_file_path, 'r', encoding='utf-8') as f:
                            meta_data = json.load(f)
                            file_info['display_name'] = meta_data.get('display_name', filename)
                            tags_label = meta_data.get('tags_label', [])
                            if tags_label and len(tags_label) > 0:
                                # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šdictæ ¼å¼ {'value': 'æ­£å¸¸'} æˆ–ç›´æ¥æ˜¯å­—ç¬¦ä¸²
                                first_label = tags_label[0]
                                if isinstance(first_label, dict):
                                    file_info['label'] = first_label.get('value', '')
                                else:
                                    file_info['label'] = str(first_label)
                            
                            # è¯»å–å·¥å†µä¿¡æ¯
                            tags_condition = meta_data.get('tags_condition', [])
                            if tags_condition:
                                file_info['conditions'] = {}
                                for cond in tags_condition:
                                    if isinstance(cond, dict) and 'key' in cond and 'value' in cond:
                                        file_info['conditions'][cond['key']] = cond['value']
                    except Exception:
                        pass
                
                files.append(file_info)
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        files.sort(key=lambda x: x['modified_time'], reverse=True)
        
        return jsonify({
            'success': True,
            'files': files,
            'total': len(files)
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/condition_keys', methods=['GET'])
def get_condition_keys():
    """è·å–æ‰€æœ‰å·¥å†µkeyåˆ—è¡¨"""
    try:
        edge_root = Path(__file__).resolve().parents[4]
        meta_dir = edge_root / 'data' / 'meta' / 'AnomalyDetection'
        
        print(f"ğŸ” æŸ¥æ‰¾å…ƒæ•°æ®ç›®å½•: {meta_dir}")
        print(f"ğŸ” ç›®å½•æ˜¯å¦å­˜åœ¨: {meta_dir.exists()}")
        
        if not meta_dir.exists():
            print(f"âš ï¸ å…ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {meta_dir}")
            return jsonify({
                'success': True,
                'keys': []
            })
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å·¥å†µkey
        condition_keys = set()
        file_count = 0
        
        for meta_file in meta_dir.glob('*.json'):
            file_count += 1
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                    tags_condition = meta_data.get('tags_condition', [])
                    print(f"ğŸ“„ æ–‡ä»¶ {meta_file.name}: {len(tags_condition)} ä¸ªå·¥å†µ")
                    for cond in tags_condition:
                        if isinstance(cond, dict) and 'key' in cond:
                            condition_keys.add(cond['key'])
                            print(f"  - æ‰¾åˆ°å·¥å†µkey: {cond['key']}")
            except Exception as e:
                print(f"âŒ è¯»å–å…ƒæ–‡ä»¶å¤±è´¥ {meta_file}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"âœ… å…±å¤„ç† {file_count} ä¸ªå…ƒæ–‡ä»¶ï¼Œæ‰¾åˆ° {len(condition_keys)} ä¸ªå”¯ä¸€çš„å·¥å†µkey: {sorted(list(condition_keys))}")
        
        return jsonify({
            'success': True,
            'keys': sorted(list(condition_keys))
        })
        
    except Exception as e:
        print(f"âŒ è·å–å·¥å†µkeyåˆ—è¡¨å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'è·å–å·¥å†µkeyåˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/condition_values', methods=['GET'])
def get_condition_values():
    """è·å–æŒ‡å®šå·¥å†µkeyçš„æ‰€æœ‰valueåˆ—è¡¨"""
    try:
        key = request.args.get('key', '').strip()
        if not key:
            return jsonify({
                'success': False,
                'error': 'è¯·æŒ‡å®šå·¥å†µkey'
            }), 400
        
        edge_root = Path(__file__).resolve().parents[4]
        meta_dir = edge_root / 'data' / 'meta' / 'AnomalyDetection'
        
        print(f"ğŸ” æŸ¥æ‰¾å·¥å†µå€¼: key={key}, ç›®å½•={meta_dir}")
        
        if not meta_dir.exists():
            print(f"âš ï¸ å…ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {meta_dir}")
            return jsonify({
                'success': True,
                'values': []
            })
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„valueï¼ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¿æŒä¸€è‡´æ€§ï¼‰
        condition_values = set()
        
        for meta_file in meta_dir.glob('*.json'):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                    tags_condition = meta_data.get('tags_condition', [])
                    for cond in tags_condition:
                        if isinstance(cond, dict) and cond.get('key') == key and 'value' in cond:
                            # å°†valueè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿ç±»å‹ä¸€è‡´
                            value = str(cond['value'])
                            condition_values.add(value)
            except Exception as e:
                print(f"âŒ è¯»å–å…ƒæ–‡ä»¶å¤±è´¥ {meta_file}: {e}")
                continue
        
        sorted_values = sorted(list(condition_values), key=lambda x: (float(x) if x.replace('.', '').replace('-', '').isdigit() else float('inf'), x))
        print(f"âœ… æ‰¾åˆ° {len(sorted_values)} ä¸ªå€¼: {sorted_values}")
        
        return jsonify({
            'success': True,
            'key': key,
            'values': sorted_values
        })
        
    except Exception as e:
        print(f"âŒ è·å–å·¥å†µvalueåˆ—è¡¨å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'è·å–å·¥å†µvalueåˆ—è¡¨å¤±è´¥: {str(e)}'
        }), 500


@ad_training_bp.route('/api/filter_files', methods=['POST'])
def filter_files():
    """æ ¹æ®å·¥å†µæ¡ä»¶ç­›é€‰æ–‡ä»¶"""
    try:
        data = request.get_json()
        conditions = data.get('conditions', {})  # {key: [value1, value2, ...]}
        file_type = data.get('file_type', 'train')  # 'train' æˆ– 'test'
        
        edge_root = Path(__file__).resolve().parents[4]
        meta_dir = edge_root / 'data' / 'meta' / 'AnomalyDetection'
        labeled_dir = edge_root / 'data' / 'labeled' / 'AnomalyDetection'
        
        if not meta_dir.exists() or not labeled_dir.exists():
            return jsonify({
                'success': True,
                'files': []
            })
        
        matched_files = []
        
        # éå†æ‰€æœ‰å…ƒæ–‡ä»¶
        for meta_file in meta_dir.glob('*.json'):
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                
                # æ£€æŸ¥æ ‡ç­¾
                tags_label = meta_data.get('tags_label', [])
                if not tags_label:
                    continue
                
                # è·å–ç¬¬ä¸€ä¸ªæ ‡ç­¾çš„å€¼
                first_label_value = tags_label[0].get('value', '').strip()
                
                # æ£€æŸ¥å¯¹åº”çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå…ˆè·å–æ–‡ä»¶åï¼‰
                data_filename = meta_file.stem + '.csv'
                
                # æ ¹æ®file_typeç­›é€‰æ ‡ç­¾
                if file_type == 'train':
                    # è®­ç»ƒé›†ï¼šåªé€‰æ‹©æ ‡ç­¾ä¸º"æ­£å¸¸"çš„æ–‡ä»¶
                    if first_label_value != 'æ­£å¸¸':
                        continue
                elif file_type == 'test':
                    # æµ‹è¯•é›†ï¼šé€‰æ‹©æ ‡ç­¾ä¸º"æ­£å¸¸"æˆ–"å¼‚å¸¸"çš„æ–‡ä»¶
                    if first_label_value not in ['æ­£å¸¸', 'å¼‚å¸¸']:
                        continue
                
                # æ£€æŸ¥å·¥å†µæ¡ä»¶
                tags_condition = meta_data.get('tags_condition', [])
                condition_dict = {}
                for cond in tags_condition:
                    if isinstance(cond, dict) and 'key' in cond and 'value' in cond:
                        condition_dict[cond['key']] = cond['value']
                
                # éªŒè¯æ˜¯å¦æ»¡è¶³æ‰€æœ‰æ¡ä»¶
                satisfies_all = True
                for key, required_values in conditions.items():
                    if not required_values:  # å¦‚æœæ²¡æœ‰é€‰æ‹©ä»»ä½•valueï¼Œè·³è¿‡è¿™ä¸ªkey
                        continue
                    file_value = condition_dict.get(key)
                    if file_value is None or file_value not in required_values:
                        satisfies_all = False
                        break
                
                if not satisfies_all:
                    continue
                data_file_path = labeled_dir / data_filename
                
                if not data_file_path.exists():
                    continue
                
                # æ„å»ºæ–‡ä»¶ä¿¡æ¯
                file_stat = data_file_path.stat()
                file_info = {
                    'filename': data_filename,
                    'display_name': meta_data.get('display_name', data_filename),
                    'label': first_label_value,
                    'conditions': condition_dict,
                    'size': file_stat.st_size,
                    'modified_time': datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'meta_file': meta_file.name
                }
                
                matched_files.append(file_info)
                
            except Exception as e:
                print(f"å¤„ç†å…ƒæ–‡ä»¶å¤±è´¥ {meta_file}: {e}")
                continue
        
        # æŒ‰æ–‡ä»¶åæ’åº
        matched_files.sort(key=lambda x: x['filename'])
        
        return jsonify({
            'success': True,
            'files': matched_files,
            'total': len(matched_files)
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'ç­›é€‰æ–‡ä»¶å¤±è´¥: {str(e)}'
        }), 500
